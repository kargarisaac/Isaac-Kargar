{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd6c6e5c",
   "metadata": {},
   "source": [
    "# \"Machine Learning Experiment Tracking Using MLFlow\"\n",
    "> \"Experiment tracking in machine learning model development.\"\n",
    "\n",
    "- toc: True\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [mlops]\n",
    "- image: images/some_folder/your_image.png\n",
    "- hide: false\n",
    "- search_exclude: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9eeb1",
   "metadata": {},
   "source": [
    "In this series of blog posts, I decided to explain the whole process of creating a machine learning service from development to deployment and monitoring. This is the final project of the amazing [MLOps zoomcamp course](https://www.youtube.com/playlist?list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK). The goal in this blog posts is to train a simple sentiment analysis model and deploy it on Google Cloud and use MLOps as much as I can. The service gets a review about clothing on E-commerce and predicts whether the custoomer will recommed it to her friends or not. You can find the dataset [here](https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews).\n",
    "\n",
    "We do not focus that much on the model development here. I use [Kaggle](https://www.kaggle.com/code/granjithkumar/nlp-with-women-clothing-reviews) notebook as the reference and will try to do experiment tracking using MLFlow. You can check the main notebook for further data preprocessing and visualizations.\n",
    "\n",
    "So, what is experiment tracking? Experiment tracking is the process of keeping track of all the relevant information from an ML experiment, which includes:\n",
    "\n",
    "- source code\n",
    "- environment\n",
    "- data\n",
    "- model\n",
    "- hyperparameters\n",
    "- metrics\n",
    "- ...\n",
    "\n",
    "And why experiment tracking is important?\n",
    "\n",
    "- reproducibilty\n",
    "- organization\n",
    "- optimization\n",
    "\n",
    "MLflow is an open source platform for managing the end-to-end machine learning lifecycle, and we will use it here. It tackles four primary functions:[[source](https://www.mlflow.org/docs/latest/index.html)]\n",
    "\n",
    "- Tracking experiments to record and compare parameters and results (MLflow Tracking). The MLflow Tracking component is an API and UI for logging parameters, code versions, metrics, and output files when running your machine learning code and for later visualizing the results\n",
    "\n",
    "- Packaging ML code in a reusable, reproducible form in order to share with other data scientists or transfer to production (MLflow Projects). \n",
    "\n",
    "- Managing and deploying models from a variety of ML libraries to a variety of model serving and inference platforms (MLflow Models).\n",
    "\n",
    "- Providing a central model store to collaboratively manage the full lifecycle of an MLflow Model, including model versioning, stage transitions, and annotations (MLflow Model Registry).\n",
    "\n",
    "You can learn more about MLFlow by reading their documentations and also watching the videos from the MLOps zoomcamp course. \n",
    "\n",
    "\n",
    "> youtube: https://youtu.be/MiA7LQin9c8\n",
    "\n",
    "\n",
    "In this post, we train three different ML models for the sentiment analysis task: Bag of Words, TF-IDF, and a simple neural network model.\n",
    "\n",
    "First you need to install MLFlow using pip. You can check more [here](https://www.mlflow.org/docs/latest/quickstart.html).\n",
    "\n",
    "You can then run the MLFlow UI using:\n",
    "\n",
    "```bash\n",
    "mlflow ui --backend-store-uri sqlite:///mlflow.db\n",
    "```\n",
    "\n",
    "You can then see the UI in this address: `http://127.0.0.1:5000/`\n",
    "\n",
    "> youtube: https://youtu.be/cESCQE9J3ZE\n",
    "\n",
    "It is also possible to run MLFlow on Cloud to store models and metadata there. I wrote a blog post about how to set up MLFlow on GCP. You can read it [here](https://kargarisaac.github.io/blog/mlops/jupyter/2022/06/15/MLFlow-on-GCP.html).\n",
    "\n",
    "For hyperparameter optimization you can use [`hyperopt`](http://hyperopt.github.io/hyperopt/) library. Currently three algorithms are implemented in hyperopt:\n",
    "\n",
    "- Random Search\n",
    "- Tree of Parzen Estimators (TPE)\n",
    "- Adaptive TPE\n",
    "\n",
    "Hyperopt has been designed to accommodate Bayesian optimization algorithms based on Gaussian processes and regression trees, but these are not currently implemented.\n",
    "\n",
    "We don't have that much parameters here, so we don't use `hyperopt` but you can check it out. \n",
    "Let's start with the BoW model:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d273f",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "os.environ[\"NLTK_DATA\"] = \"./corpora\"\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"nlp-bow\")\n",
    "\n",
    "## data loading\n",
    "data = pd.read_csv('Womens Clothing E-Commerce Reviews.csv',index_col =[0])\n",
    "\n",
    "## preprocess text\n",
    "data = data[~data['Review Text'].isnull()]  #Dropping columns which don't have any review\n",
    "X = data[['Review Text']]\n",
    "X.index = np.arange(len(X))\n",
    "\n",
    "y = data['Recommended IND']\n",
    "\n",
    "corpus =[]\n",
    "for i in range(len(X)):\n",
    "    review = re.sub('[^a-zA-z]',' ',X['Review Text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]\n",
    "    review =' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "cv  = CV(max_features = 3000,ngram_range=(1,1))\n",
    "X_cv = cv.fit_transform(corpus).toarray()\n",
    "y = y.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cv, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    mlflow.set_tag(\"developer\", \"Isaac\")\n",
    "    mlflow.set_tag(\"algorithm\", \"BernoulliNB\")\n",
    "    mlflow.log_param(\"train-data\", \"Womens Clothing E-Commerce Reviews\")\n",
    "    \n",
    "    alpha = 1\n",
    "    mlflow.log_param(\"alpha\", alpha)\n",
    "\n",
    "    classifier = BernoulliNB(alpha = alpha)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "\n",
    "    print(\"accuracy on test data:\", acc)\n",
    "\n",
    "    model_name = \"model_bow.bin\"\n",
    "    with open(\"models/\" + model_name, 'wb') as fout:\n",
    "        pickle.dump((cv, classifier), fout)\n",
    "\n",
    "    mlflow.log_artifact(local_path=\"models/\" + model_name, artifact_path=\"models_pickle\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62796b27",
   "metadata": {},
   "source": [
    "You can see how easy it is to use mlflow to keep track of the ML model development process.\n",
    "\n",
    "Let's do it for TF-IDf too:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "# nltk.download('stopwords')\n",
    "os.environ[\"NLTK_DATA\"] = \"./corpora\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TV\n",
    "import pickle\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"nlp-tfidf\")\n",
    "\n",
    "## data loading\n",
    "data = pd.read_csv('Womens Clothing E-Commerce Reviews.csv',index_col =[0])\n",
    "\n",
    "## preprocess text\n",
    "data = data[~data['Review Text'].isnull()]  #Dropping columns which don't have any review\n",
    "X = data[['Review Text']]\n",
    "X.index = np.arange(len(X))\n",
    "\n",
    "y = data['Recommended IND']\n",
    "\n",
    "corpus =[]\n",
    "for i in range(len(X)):\n",
    "    review = re.sub('[^a-zA-z]',' ',X['Review Text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]\n",
    "    review =' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "tv  = TV(ngram_range =(1,1),max_features = 3000)\n",
    "X_tv = tv.fit_transform(corpus).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tv, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    mlflow.set_tag(\"developer\", \"Isaac\")\n",
    "    mlflow.set_tag(\"algorithm\", \"MultinomialNB\")\n",
    "    mlflow.log_param(\"train-data\", \"Womens Clothing E-Commerce Reviews\")\n",
    "\n",
    "    alpha = 1\n",
    "    mlflow.log_param(\"alpha\", alpha)\n",
    "\n",
    "    classifier = MultinomialNB(alpha = alpha)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "\n",
    "    print(\"accuracy on test data:\", acc)\n",
    "\n",
    "    model_name = \"model_tfidf.bin\"\n",
    "    with open(\"models/\" + model_name, 'wb') as fout:\n",
    "        pickle.dump((tv, classifier), fout)\n",
    "\n",
    "    mlflow.log_artifact(local_path=\"models/\" + model_name, artifact_path=\"models_pickle\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a58352",
   "metadata": {},
   "source": [
    "And finally, the deep learning model:\n",
    "\n",
    "```python\n",
    "# ref: https://www.kaggle.com/code/granjithkumar/nlp-with-women-clothing-reviews/data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "# nltk.download('stopwords')\n",
    "os.environ[\"NLTK_DATA\"] = \"./corpora\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import pickle\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"nlp-tfidf\")\n",
    "\n",
    "## data loading\n",
    "data = pd.read_csv('Womens Clothing E-Commerce Reviews.csv',index_col =[0])\n",
    "\n",
    "## preprocess text\n",
    "data = data[~data['Review Text'].isnull()]  #Dropping columns which don't have any review\n",
    "X = data[['Review Text']]\n",
    "X.index = np.arange(len(X))\n",
    "\n",
    "y = data['Recommended IND']\n",
    "\n",
    "corpus =[]\n",
    "for i in range(len(X)):\n",
    "    review = re.sub('[^a-zA-z]',' ',X['Review Text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]\n",
    "    review =' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "## tokenization and dataset creation\n",
    "tokenizer = Tokenizer(num_words = 3000)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    ## model definition\n",
    "    embedding_dim = 32\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(3000, embedding_dim),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(6, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    ## training\n",
    "    num_epochs = 50\n",
    "    batch_size = 32\n",
    "    callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=0,\n",
    "        patience=2,\n",
    "        verbose=0,\n",
    "        mode=\"auto\",\n",
    "        baseline=None,\n",
    "        restore_best_weights=False,\n",
    "    )\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    mlflow.set_tag(\"developer\", \"Isaac\")\n",
    "    mlflow.set_tag(\"algorithm\", \"Deep Learning\")\n",
    "    mlflow.log_param(\"train-data\", \"Womens Clothing E-Commerce Reviews\")\n",
    "    mlflow.log_param(\"embedding-dim\", embedding_dim)\n",
    "\n",
    "    print(\"Fit model on training data\")\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        callbacks=callback,\n",
    "        # We pass some validation for\n",
    "        # monitoring validation loss and metrics\n",
    "        # at the end of each epoch\n",
    "        validation_data=(X_test, y_test),\n",
    "    )\n",
    "\n",
    "    ## save model and tokenizer\n",
    "    mlflow.keras.log_model(model, 'models/model_dl')\n",
    "\n",
    "    with open('models/tf_tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    mlflow.log_artifact(local_path=\"models/tf_tokenizer.pickle\", artifact_path=\"tokenizer_pickle\")\n",
    "\n",
    "    # Evaluate the model on the test data using `evaluate`\n",
    "    print(\"Evaluate on test data\")\n",
    "    results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "    mlflow.log_metric(\"loss\", results[0])\n",
    "    mlflow.log_metric(\"accuracy\", results[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac21f0",
   "metadata": {},
   "source": [
    "As you can see, we can use `autolog()` for TensorFlow and Scikit Learn to log parameters (automatically) or we can also log whatever we want manually. Finally you will see something like below after training different models with different hyperparameters:\n",
    "\n",
    "![](images/experiment-tracking-mlflow/1.png)\n",
    "<!-- *[source](https://github.com/evidentlyai/evidently)* -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc631efc",
   "metadata": {},
   "source": [
    "You can then easily go to each one of the experiments and see more details for each run and compare them. You can select one model based on different parameters like training time, accuracy, etc.. \n",
    "\n",
    "Note that we can log a model using one of the following two ways:\n",
    "\n",
    "1. save the model based on the framework, and then use `mlflow.log_artifact(local_path=<local path to saved model>, artifact_path=<name of the folder you want the model to be saved in mlruns>)`. We saved the tokenizer using this method in the deep learning version above.\n",
    "\n",
    "2. second, you can use `mlflow.<framework>.log_model(...)`. We saved the Keras model using this method in the deep learning version above.\n",
    "\n",
    "Let's take a look at what is saved for one of the runs. For example one of the deep learning runs:\n",
    "\n",
    "![](images/experiment-tracking-mlflow/3.png)\n",
    "<!-- *[source](https://github.com/evidentlyai/evidently)* -->\n",
    "\n",
    "As you see, the model, tokenizer, all the information about the required python packages, and many more metadata is saved. Each run has a unique `run id`, which we will use it later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b09fd",
   "metadata": {},
   "source": [
    "Now let's see how we can manage the trained models, called Model Management.\n",
    "\n",
    "![](images/experiment-tracking-mlflow/2.png)\n",
    "*[source](https://neptune.ai/experiment-tracking)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdce82e",
   "metadata": {},
   "source": [
    "As you see above in the codes, we save models for each run. we should have some kind of model versioning and then deployment. Let's see how we can use MLFlow to do model management."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
