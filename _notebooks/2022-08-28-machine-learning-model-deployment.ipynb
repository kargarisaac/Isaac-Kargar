{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa2a4035",
   "metadata": {},
   "source": [
    "# \"MLOps project - part 3a: Machine Learning Model Deployment\" \n",
    "> \"Deploying machine learning models in production.\"\n",
    "\n",
    "- toc: True\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [mlops]\n",
    "- image: images/some_folder/your_image.png\n",
    "- hide: false\n",
    "- search_exclude: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d74f68",
   "metadata": {},
   "source": [
    "So far in this series of blog posts, we saw how to do experiment tracking and creating a machine learning pipeline for model training. But what should we do now with the trained mode?! That's right. We have to deploy the model into production. so people can use it and make inference. \n",
    "\n",
    "In this blog post we will see what is machine learning model deployment and what are the options that can help us to do it. In the next post, we will deploy our own trained model for customer sentiment analysis in production.\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "\n",
    "# Machine Learning Model Deployment\n",
    "\n",
    "There are multiple options for model deployment. First, we need to ask if we want the predictions to be done immediately or if they can wait for an hour, a day, etc. \n",
    "\n",
    "- In case we can wait a bit, we can go for **batch or offline** deployment. In this case, the model doen't need to be running all the time and we can call it with a batch of data regularly in a time interval.\n",
    "- In the other case, we need the model predictions as soon as possible and the model should be running all the time. This is called **online** deployment. Online deployment has multiple variants as well:\n",
    "    - *Web service*: In this case, we deploy our model as a web service and we can send HTTPS requests and get the prediction from that.\n",
    "    - *Streaming*: In this case, there is a stream of events and model is listening to events and reacting to them. \n",
    "    \n",
    "![](images/model-deployment/1.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec5b1cf",
   "metadata": {},
   "source": [
    "## Batch or Offline Mode\n",
    "\n",
    "In this case, we need to apply the model to a batch of data in a time interval. It can be every 10 minutes, every half an hour, every day, every week, etc. \n",
    "\n",
    "Usually we have a database with data in it and a job which has the model to pull the data from database and apply the model to the data. It can then save the results of the prediction into another database, so other jobs can use the data for other purposes like a report or dashboard.\n",
    "\n",
    "\n",
    "![](images/model-deployment/2.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccfe0a",
   "metadata": {},
   "source": [
    "## Online Deployment \n",
    "\n",
    "### Web Service\n",
    "\n",
    "The other common way of deploying models is Web Service which is in the category of Online deployment. In this case you have a web service which has the machine learning model. This service needs to be up and running all the time. It is also possible to use serverless microservices like a service deployed using Cloud Run. There would be a very small delay which needs to be taken into consideration and if it is not acceptable, you need to go for more real-time architectures. This case is more like a one-to-one relationship between client and server.\n",
    "\n",
    "![](images/model-deployment/3.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c4f54",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "In this case, we have producers and consumers. Producers will push some events into an event stream and multiple services or consumers would read from the stream and react to the events. This more like a one-to-many or many-to-many relationship between producer(s) and consumers. For example, the producer can be a user with an app which interacts with the backend and produces some events. Then this events will go to an stream event and multiple services can do different jobs on those events. The difference with the web service option is that there is no explicit connection between the procuser and consumers here. The producer just pushes an event and some services will process it. The result of these consumers may go to another event stream to be used by some other consumers and services. There is no limit there.\n",
    "\n",
    "\n",
    "![](images/model-deployment/4.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b8fd7",
   "metadata": {},
   "source": [
    "Check the following video to learn more about different deployment options:\n",
    "\n",
    "> youtube: https://youtu.be/JMGe4yIoBRA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153c1380",
   "metadata": {},
   "source": [
    "# Model Deployment Tools\n",
    "\n",
    "There are many ways to deploy your model into production. [This blog post](https://getindata.com/blog/machine-learning-model-serving-tools-comaprison-kserve-seldon-core-bentoml/) does a nice comparison of three popular tools: [Seldon Core](https://www.seldon.io/solutions/open-source-projects/core), [KServe](https://kserve.github.io/website/0.9/), and [BentoML](https://www.bentoml.com/). \n",
    "\n",
    "The comparison is focused on 9 main areas of model serving tools:\n",
    "\n",
    "- ability to serve models from standard frameworks, including Scikit-Learn, PyTorch, Tensorflow and XGBoost\n",
    "- ability to serve custom models / models from niche frameworks\n",
    "- ability to pre-process/post-process data\n",
    "- impact on the development workflow and existing codebase\n",
    "- availability of the documentation\n",
    "- DevOps operability\n",
    "- autoscaling capabilities\n",
    "- available interfaces for obtaining predictions\n",
    "- infrastructure management\n",
    "\n",
    "> **KServe**: KServe (previously, before the 0.7 version was named KFServing) is an open-source, Kubernetes-based tool providing custom abstraction (Kubernetes Custom Resource Definition) to define Machine Learning model serving capabilities. It’s main focus is to hide the underlying complexity of such deployments so that it’s users only need to  focus on the ML-related parts. It supports many advanced features such as autoscaling, scaling-to-zero, canary deployments, automatic request batching as well as many popular ML frameworks out-of-the-box.\n",
    "\n",
    "> **Seldon Core**: Seldon Core is an open source tool developed by Seldon Technologies Ltd, as a building block of the larger (paid) Seldon Deploy solution. It’s similar to KServe in terms of the approach - it provides high level Kubernetes CRD and supports canary deployments, A/B testing as well as Multi-Armed-Bandit deployments. \n",
    "\n",
    "> **BentoML**: BentoML is a Python framework for wrapping the machine learning models into deployable services. It provides a simple object-oriented interface for packaging ML models and creating HTTP(s) services for them. BentoML offers in-depth integration with popular ML frameworks, so that all of the complexity related to packaging the models and their dependencies is hidden. BentoML-packaged models can be deployed in many runtimes, which include plain Kubernetes Clusters, Seldon Core, KServe, Knative as well as cloud-managed, serverless solutions like AWS Lambda, Azure Functions or Google Cloud Run.\n",
    "\n",
    "It's really informative and I highly recommend check it out. \n",
    "\n",
    "In this blog post, I show how to deploy our own example, customer sentiment analysis, as a web service. I will do it in three ways: Google Cloud Run, Vertex-AI, and KServe with ZenML.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cd1456",
   "metadata": {},
   "source": [
    "## Cloud Run\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d8f4c",
   "metadata": {},
   "source": [
    "In the previous posts, we trained the model on Vertex AI on GCP and used MLflow for experiment tracking. MLflow will save model artifacts on GCS. Here we just use those artifacts in our endpoint for prediction. I use a solution here to download the model based on experiment ID and run ID and pass these values as environment variable in the docker file. If you use Cloud Run for deployment, you can also set these variables using cloud commands. Check [here](https://cloud.google.com/run/docs/configuring/environment-variables#console) for more information.\n",
    "\n",
    "We develop a single endpoint for prediction using FastAPI. To access GCS, one way is to use service account. You can create the service account from Google Console and give it the required access to storage. I give it storage admin role here, but in production, you can give more limited access. Then you need to generate the JSON key file and same it with the name of `key.json` it in root folder. Be carefull to add this file to your `.gitignore` file as you don't want to push it into Github.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca753edc",
   "metadata": {},
   "source": [
    "Here is the code:\n",
    "\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import mlflow\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "experiment_id = str(os.getenv(\"EXPERIMENT_ID\"))\n",
    "run_id = str(os.getenv(\"RUN_ID\"))\n",
    "\n",
    "path_on_gcs = experiment_id + \"/\" + run_id\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# fastapi app\n",
    "app = FastAPI(\n",
    "    title=\"Fashion Sentment Analysis REST API\",\n",
    "    description=\"This API analyses the review for a fashion product.\",\n",
    "    version=\"0.0.1\",\n",
    "    contact={\n",
    "        \"name\": \"Isaac Kargar\",\n",
    "        \"email\": \"Isaac@email.com\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(review):\n",
    "    review_processed = review.lower()\n",
    "    review_processed = review_processed.split()\n",
    "    ps = PorterStemmer()\n",
    "    review_processed =[ps.stem(i) for i in review_processed if not i in set(stopwords.words('english'))]\n",
    "    review_processed =' '.join(review_processed)\n",
    "    return review_processed\n",
    "\n",
    "def download_mlflow_artifacts():\n",
    "    # create storage client\n",
    "    storage_client = storage.Client.from_service_account_json('key.json')\n",
    "    # storage_client = storage.Client()\n",
    "    # get bucket with name\n",
    "    bucket = storage_client.get_bucket('mlflow')\n",
    "    # get bucket data as blob\n",
    "    blobs = bucket.list_blobs(prefix=path_on_gcs) \n",
    "\n",
    "    for blob in blobs:\n",
    "        blob_name = blob.name \n",
    "        file_path = \"models/\" + blob_name\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        if os.path.isdir(folder_path) == False:\n",
    "            os.makedirs(folder_path)\n",
    "        \n",
    "        blob.download_to_filename(file_path)\n",
    "    \n",
    "    print(\"Artifacts downloaded.\\n\\n\")\n",
    "\n",
    "def load_model_tokenizer():\n",
    "    artifact_folder = f\"models/{path_on_gcs}/artifacts\"\n",
    "    model = mlflow.pyfunc.load_model(f\"{artifact_folder}/models/model_dl\")\n",
    "\n",
    "    with open(f\"{artifact_folder}/tokenizer_pickle/tf_tokenizer.pickle\", 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    print(\"Model and tokenizer loaded.\\n\\n\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Customer review sentiment analysis\"}\n",
    "\n",
    "\n",
    "@app.get(\"/predict\")\n",
    "async def get_predict(review: str,):\n",
    "    \"\"\"\n",
    "    Reads the list of sensors from the database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        review_processed = preprocess(review)\n",
    "\n",
    "        if len(review_processed) == 0:\n",
    "            return {\"message\": \"Please enter a valid review - It seems there is no valubale review in the text.\"}\n",
    "        \n",
    "        download_mlflow_artifacts()\n",
    "        model, tokenizer = load_model_tokenizer()\n",
    "        review_processed = tokenizer.texts_to_sequences(review_processed)\n",
    "        review_processed = pad_sequences(review_processed, padding='post').T\n",
    "        prediction = model.predict(review_processed)\n",
    "        return {\n",
    "            \"prediction\": str(prediction[0][0])\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e0314",
   "metadata": {},
   "source": [
    "We then need to create a `Dockerfile`. I use `pipenv` in this project to be able to have isolated virtual environment for each project. You can learn more about it [here](https://pipenv.pypa.io/en/latest/). Our Dockerfile would be as follows:\n",
    "\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "RUN pip install -U pip\n",
    "RUN pip install pipenv \n",
    "\n",
    "EXPOSE 8080\n",
    "ENV EXPERIMENT_ID 2\n",
    "ENV RUN_ID 7e6fc298380e484ba7cac0ab029ba510\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY . .\n",
    "\n",
    "RUN pipenv install --system --deploy\n",
    "\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762bbde5",
   "metadata": {},
   "source": [
    "We use `uvicorn` web server for FastAPI here. You can also see that I set `EXPERIMENT_ID` and `RUN_ID` as environment variables to be used in my python code.\n",
    "\n",
    "Then we can build the image and test locally first. Follow the steps below to do so:\n",
    "\n",
    "- Build the image\n",
    "```bash\n",
    "docker build -t sentiment:v1 .\n",
    "```\n",
    "\n",
    "- Run the econtainer\n",
    "```bash\n",
    "docker run -it --rm -p 8080:8080 sentiment:v1\n",
    "```\n",
    "\n",
    "- Then visit http://localhost:8080/docs to view your fastapi app. You will see the following page which you can play with the API interactively:\n",
    "\n",
    "![](images/model-deployment/5.jpg)\n",
    "\n",
    "\n",
    "- You can use this test sentence to test the API: `Like it, but don't love it.`\n",
    "\n",
    "\n",
    "If everything works fine, you can deploy the app on Google Cloud using the following commands:\n",
    "\n",
    "- Set project ID variable:\n",
    "\n",
    "```bash\n",
    "export PROJECT_ID=<your gcp project id>\n",
    "```\n",
    "\n",
    "- Build the image:\n",
    "```bash\n",
    "gcloud builds submit --tag gcr.io/$PROJECT_ID/sentiment\n",
    "```\n",
    "\n",
    "- Deploy on cloud run:\n",
    "```bash\n",
    "gcloud run deploy sentiment --image gcr.io/$PROJECT_ID/sentiment --platform managed --allow-unauthenticated\n",
    "```\n",
    "\n",
    "You then should be able to use the endpoint link and see the same page as before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434192a9",
   "metadata": {},
   "source": [
    "We also developed a simple UI using streamlit package to get user review about a product and analyze it. The code for UI is as follows:\n",
    "\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "import requests\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"Fashion Sentiment Analysis\",\n",
    "    page_icon=\"📈\",\n",
    "    layout=\"wide\",\n",
    ")\n",
    "\n",
    "st.header(\"Fashion Sentiment Analysis UI\")\n",
    "\n",
    "st.image(\"https://krm-stc-ms.azureedge.net/-/media/Images/Ecco/Products/MENS/BROWN/ECCO-SOFT-7-M/470364-02053-main.webp?Crop=1&Size=ProductDetailsMedium1x\")\n",
    "\n",
    "review_text = st.text_input('User review about the product:', 'Input your review here')\n",
    "\n",
    "if st.button(\"Analyze\"):\n",
    "    endpoint = \"<url for the deployed model on cloud run>\"\n",
    "    url = f'{endpoint}/predict?review={review_text}'\n",
    "    \n",
    "    prediction = requests.get(url).json()\n",
    "    st.write(\"Prediction:\", prediction)\n",
    "```\n",
    "\n",
    "The Docker file would be as follows:\n",
    "\n",
    "\n",
    "```Dockerfile\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "You can again dockerize this and deploy it on Cloud Run as another microservice by following the steps below:\n",
    "\n",
    "\n",
    "- Set project ID variable:\n",
    "```bash\n",
    "export PROJECT_ID=$(gcloud config get-value core/project)\n",
    "```\n",
    "\n",
    "- build the image:\n",
    "```bash\n",
    "gcloud builds submit --tag gcr.io/$PROJECT_ID/sentiment_ui\n",
    "```\n",
    "\n",
    "- deploy on cloud run:\n",
    "```bash\n",
    "gcloud run deploy sentiment_ui --image gcr.io/$PROJECT_ID/sentiment_ui --platform managed --allow-unauthenticated\n",
    "```\n",
    "\n",
    "\n",
    "By following the url for the `sentiment_ui` endpoint, you would see a UI wo as follows:\n",
    "\n",
    "\n",
    "image source: https://be.ecco.com/nl-BE/product/5643183598/ECCO-SOFT-7-M?dfw_tracker=25318-5643183603&gclid=CjwKCAjw9suYBhBIEiwA7iMhNIahBjBQOZsqjVd9PhdZIbIviG_z_5BTMoPgxxhSqSlX48jdDu7qfBoCJpwQAvD_BwE&gclsrc=aw.ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dcb07d",
   "metadata": {},
   "source": [
    "## Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757ca80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406799f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a8a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e13b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9760e78f",
   "metadata": {},
   "source": [
    "# KServe with ZenML\n",
    "\n",
    "> [KServe](https://github.com/kserve/kserve) provides a Kubernetes Custom Resource Definition for serving machine learning (ML) models on arbitrary frameworks. It aims to solve production model serving use cases by providing performant, high abstraction interfaces for common ML frameworks like Tensorflow, XGBoost, ScikitLearn, PyTorch, and ONNX. It encapsulates the complexity of autoscaling, networking, health checking, and server configuration to bring cutting edge serving features like GPU Autoscaling, Scale to Zero, and Canary Rollouts to your ML deployments. It enables a simple, pluggable, and complete story for Production ML Serving including prediction, pre-processing, post-processing and explainability. KServe is being used across various organizations.\n",
    "\n",
    "\n",
    "![](images/model-deployment/kserve.png)\n",
    "*[source](https://github.com/kserve/kserve)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcb0b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://blog.zenml.io/deploy-with-kserve/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4640ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85e6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
