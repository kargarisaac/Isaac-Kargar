{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a4530e7",
   "metadata": {},
   "source": [
    "# \"Event Driven App using Kafka and Python\"\n",
    "> \"An event-driven application using Kafka.\"\n",
    "\n",
    "- toc: True\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [mlops, devops]\n",
    "- image: images/some_folder/your_image.png\n",
    "- hide: false\n",
    "- search_exclude: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fdcebe",
   "metadata": {},
   "source": [
    "In this blog ppst ww will design and implement an event-driven application using Kafka in Python. We take an example of furniture ordering from somewhere like Ikea in this post. This is just a simple design that we do and is not similar to what is happening in real worl at Ikea.\n",
    "We do this on our local machine but for production environments you can transfer it to a cloud provider like AWS, GCP, or Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec0a22",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![](images/kafka/furniture-ordering-kafka.jpg)\n",
    "<!-- *[source](https://docs.zenml.io/getting-started/introduction)* -->\n",
    "\n",
    "Let's see what do we have in the above architecture:\n",
    "\n",
    "- **Frontend**: it can be a mobile or web app that user can order the item. When the user selects and orders the furnitures using the app, the frontend is going to call the orders backend endpoint. We will write a simple streamlit app for the frontend.\n",
    "\n",
    "- **Orders Backend**: it takes in an order from the frontend with all the data related to that order and then writes to a kafka topic called `order_details`. The topic `order_details` is gonna encapsulate all the information related to one individual order. This would be a simple python file. You can deploy this backend service and also the next ones as some microservices on a Cloud using, for example, cloud run on GCP or Lambda on AWS.\n",
    "\n",
    "- **Transactions Backend**: it subscribes to the `order_details` kafka topic so that whenever someone writes to the topic, the transactions backend is going to read the message and process it in real time. we assume that the transactions backend is going to do some credit card processing and some other checks to make sure that the order is confirmed. Once the order is confirmed, it's going to write back to another kafka topic called `order_confirmed`. The point of the order confirmed topic is to encapsulate all data related to an order that has been confirmed.\n",
    "\n",
    "- **Email Backend**: it subscribes to the `order_confirmed` topic and sends a confirmation email to the user when the order is confirmed. It can also send a message to a topic like `order_email_sent`.\n",
    "\n",
    "- **Analytics Backend**: it subscribes to the `order_confirmed` topic and performs some analytics on it. For example, it can aggregate the total number of orders in that day and the total number of revenue coming from different orders. We can then send the analytics result on the topic `analytics_result`.\n",
    "\n",
    "- **Dashboard**: we can also have a service to get some data from different topics and send them to a dashboard for visualization. Here, we just use one service for both of them in python for simplicity, but you can separate them easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71fed5",
   "metadata": {},
   "source": [
    "If you are new to kafka, I highly recommed week 6 of the [data engineering zoomcamp](https://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb) by Datatalks club. You can also check [these](https://github.com/ziritrion/dataeng-zoomcamp/blob/main/notes/6_streaming.md) notes. Then you can come back here and continue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fa347a",
   "metadata": {},
   "source": [
    "We will need the following packages in this blog post:\n",
    "    \n",
    "```\n",
    "kafka-python\n",
    "flask\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "You can also find the repo for this blog post [here]()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e6516c",
   "metadata": {},
   "source": [
    "To run kafka locally, I use the following compose file with a kafka cluster with one broker and also one zookeeper and some other kafka components like control-centro for UI, schema-registry, etc.:\n",
    "\n",
    "```bash\n",
    "## docker-compose-kafka.yml\n",
    "\n",
    "version: \"3\"\n",
    "\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:5.4.0\n",
    "    hostname: zookeeper\n",
    "    container_name: zookeeper\n",
    "    ports:\n",
    "      - \"2181:2181\"\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "\n",
    "  broker:\n",
    "    image: confluentinc/cp-server:5.4.0\n",
    "    hostname: broker\n",
    "    container_name: broker\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    ports:\n",
    "      - \"9092:9092\"\n",
    "      - \"29093:29093\"\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: \"zookeeper:2181\"\n",
    "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092\n",
    "      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n",
    "      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1\n",
    "      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:29092\n",
    "      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181\n",
    "      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1\n",
    "      CONFLUENT_METRICS_ENABLE: \"true\"\n",
    "      CONFLUENT_SUPPORT_CUSTOMER_ID: \"anonymous\"\n",
    "\n",
    "  kafka-tools:\n",
    "    image: confluentinc/cp-kafka:5.4.0\n",
    "    hostname: kafka-tools\n",
    "    container_name: kafka-tools\n",
    "    command: [\"tail\", \"-f\", \"/dev/null\"]\n",
    "    network_mode: \"host\"\n",
    "\n",
    "  schema-registry:\n",
    "    image: confluentinc/cp-schema-registry:5.4.0\n",
    "    hostname: schema-registry\n",
    "    container_name: schema-registry\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "      - broker\n",
    "    ports:\n",
    "      - \"8081:8081\"\n",
    "    environment:\n",
    "      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n",
    "      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: \"zookeeper:2181\"\n",
    "\n",
    "  control-center:\n",
    "    image: confluentinc/cp-enterprise-control-center:5.4.0\n",
    "    hostname: control-center\n",
    "    container_name: control-center\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "      - broker\n",
    "      - schema-registry\n",
    "    ports:\n",
    "      - \"9021:9021\"\n",
    "    environment:\n",
    "      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'broker:29092'\n",
    "      CONTROL_CENTER_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n",
    "      CONTROL_CENTER_SCHEMA_REGISTRY_URL: \"http://schema-registry:8081\"\n",
    "      CONTROL_CENTER_REPLICATION_FACTOR: 1\n",
    "      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1\n",
    "      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1\n",
    "      CONFLUENT_METRICS_TOPIC_REPLICATION: 1\n",
    "      PORT: 9021\n",
    "```\n",
    "\n",
    "I also like kafka-ui and Conduktor. The control-center UI just shows the messages that are sent when the UI page for the topic is opened. I don't like this and use kafka-ui for the visualization. For kafka-ui, you can add the following codes to your compose file instead of the control-center part:\n",
    "\n",
    "```bash\n",
    "  kafka-ui:\n",
    "    image: provectuslabs/kafka-ui\n",
    "    container_name: kafka-ui\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "    restart: always\n",
    "    environment:\n",
    "      - KAFKA_CLUSTERS_0_NAME=local\n",
    "      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=broker:29092\n",
    "      - KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper:2181\n",
    "      - KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL=PLAINTEXT\n",
    "      - KAFKA_CLUSTERS_0_SCHEMAREGISTRY=http://schema-registry:8081\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af66e3",
   "metadata": {},
   "source": [
    "We then need to run `docker-compose -f docker-compose-kafka.yml up -d` to run kafka with all the components. Note that kafka should be running when we want to test the frontend and backend services which need to send or receive data from kafka topics.\n",
    "\n",
    "We can test if everything is up and running by `docker-compose -f docker-compose-kafka.yml ps` command:\n",
    "\n",
    "```\n",
    "NAME                COMMAND                  SERVICE             STATUS              PORTS\n",
    "broker              \"/etc/confluent/dock…\"   broker              running             0.0.0.0:9092->9092/tcp, 0.0.0.0:29093->29093/tcp\n",
    "kafka-tools         \"tail -f /dev/null\"      kafka-tools         running             \n",
    "kafka-ui            \"/bin/sh -c 'java $J…\"   kafka-ui            running             0.0.0.0:8080->8080/tcp\n",
    "schema-registry     \"/etc/confluent/dock…\"   schema-registry     running             0.0.0.0:8081->8081/tcp\n",
    "zookeeper           \"/etc/confluent/dock…\"   zookeeper           running             2888/tcp, 0.0.0.0:2181->2181/tcp, 3888/tcp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b662f42e",
   "metadata": {},
   "source": [
    "In addition to the above docker-compose file for kafka, we will have another one for our microservices too. You can add services one by one to the docker-compose file and test them. \n",
    "\n",
    "Note that to be able to connect your producers and consumers to the kafka broker:\n",
    "- if you use the same network (puting microservice on the same docker-compose file as kafka or useing a separate docker-compose file and set the network as the kafka network), you can use `broker:29092`.\n",
    "- if you run your service locally on the same machine without dockerizing it, you can use `localhost:9092` in your code.\n",
    "- if you want to run kafka on a machine and your services on another machine, you need to use `<kafka machine ip>:29093` in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851c546",
   "metadata": {},
   "source": [
    "Let's start with the backend services. we will add the fronend later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a6c2e",
   "metadata": {},
   "source": [
    "# Orders Backend\n",
    "\n",
    "Now, let's go for the orders backend service. We will dockerize the app and put it in a separate docker-compose file called `docker-compose-services.yml` and set the network the same as the kafka network. The `orders_backend.py` is a flask application as follows:\n",
    "\n",
    "```python\n",
    "# orders_backend.py\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "from flask import Flask, jsonify, request\n",
    "\n",
    "ORDER_KAFKA_TOPIC = 'order_details'\n",
    "# KAFKA_SERVER_ADDRESS = 'localhost:9092'\n",
    "KAFKA_SERVER_ADDRESS = 'broker:29092'\n",
    "# KAFKA_SERVER_ADDRESS = '47.93.191.241:29093`\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "## from inside docker compose network - when add the service to compose file -> orders_backend:v1\n",
    "producer = KafkaProducer(bootstrap_servers=[KAFKA_SERVER_ADDRESS], security_protocol=\"PLAINTEXT\",\n",
    "                              value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
    "\n",
    "# post endpoint to get user id , order id, user email, and order details\n",
    "@app.route('/order', methods=['POST'])\n",
    "def order():\n",
    "    user_id = request.json['user_id']\n",
    "    order_id = request.json['order_id']\n",
    "    user_email = request.json['user_email']\n",
    "    order_details = request.json['order_details']\n",
    "    order = {}\n",
    "    order['user_id'] = user_id\n",
    "    order['order_id'] = order_id\n",
    "    order['user_email'] = user_email\n",
    "    order['order_details'] = order_details\n",
    "    order['time'] = time.time()\n",
    "    producer.send(ORDER_KAFKA_TOPIC, order)\n",
    "    print(\"Sent order details {} to kafka topic: {}\".format(order, ORDER_KAFKA_TOPIC))\n",
    "    return jsonify(order)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=\"0.0.0.0\", port=5002, debug=True)\n",
    "```\n",
    "\n",
    "It has a post endpoint to get an order and publish it on a kafka topic named `order_details`. \n",
    "\n",
    "You can then easily dockerize this service. Here is the Dockerfile:\n",
    "\n",
    "```bash\n",
    "FROM python:3.9.7-slim\n",
    "\n",
    "RUN pip install -U pip\n",
    "RUN pip install pipenv \n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]\n",
    "\n",
    "RUN pipenv install --system --deploy\n",
    "\n",
    "COPY [ \"orders_backend.py\", \"./\" ]\n",
    "\n",
    "EXPOSE 5002\n",
    "\n",
    "ENTRYPOINT [\"python\", \"orders_backend.py\"]\n",
    "```\n",
    "\n",
    "We can then build the image using:\n",
    "\n",
    "```bash\n",
    "docker build -t orders_backend:v1 .\n",
    "```\n",
    "\n",
    "The docker compose file would be as follows:\n",
    "\n",
    "```bash\n",
    "# docker-compose-services.yml\n",
    "\n",
    "version: \"1\"\n",
    "\n",
    "services:\n",
    "  orders_backend:\n",
    "    restart: always\n",
    "    image: orders_backend:v1\n",
    "    ports:\n",
    "      - \"5002:5002\"\n",
    "    networks:\n",
    "      - ikea-ordering-kafka_default\n",
    "\n",
    "networks:\n",
    "  ikea-ordering-kafka_default:\n",
    "    external: true\n",
    "```\n",
    "\n",
    "We can use postman to test it:\n",
    "\n",
    "![](images/kafka/postman.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d4ef3",
   "metadata": {},
   "source": [
    "We can also see the messages on the topic in the UI:\n",
    "\n",
    "![](images/kafka/kafka-ui1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f26fb7",
   "metadata": {},
   "source": [
    "# Transactions Backend\n",
    "\n",
    "This service is a simple service to listen to the `order_details` topic, do some processing on the data, and send the confirmed message to the `order_confirmed` topic:\n",
    "\n",
    "```python\n",
    "# transactions_backend.py\n",
    "\n",
    "import json\n",
    "import time\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "\n",
    "OERDER_KAFKA_TOPIC = 'order_details'\n",
    "ORDER_CONFIRMED_KAFKA_TOPIC = 'order_confirmed'\n",
    "# KAFKA_SERVER_ADDRESS = 'localhost:9092'\n",
    "KAFKA_SERVER_ADDRESS = 'broker:29092'\n",
    "# KAFKA_SERVER_ADDRESS = '47.93.191.241:29093`\n",
    "\n",
    "consumer = KafkaConsumer(OERDER_KAFKA_TOPIC, bootstrap_servers=[KAFKA_SERVER_ADDRESS], security_protocol=\"PLAINTEXT\",\n",
    "                            value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n",
    "producer = KafkaProducer(bootstrap_servers=[KAFKA_SERVER_ADDRESS], security_protocol=\"PLAINTEXT\",\n",
    "                            value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
    "\n",
    "while True:\n",
    "    for message in consumer:\n",
    "        print(\"Received order details: {}\".format(message.value))\n",
    "        user_id = message.value['user_id']\n",
    "        order_id = message.value['order_id']\n",
    "        user_email = message.value['user_email']\n",
    "        order_details = message.value['order_details']\n",
    "        time = message.value['time']\n",
    "        ## do some suff on the order and check the confirmation\n",
    "        order_confirmed = {}\n",
    "        order_confirmed['user_id'] = user_id\n",
    "        order_confirmed['order_id'] = order_id\n",
    "        order_confirmed['user_email'] = user_email\n",
    "        order_confirmed['order_details'] = order_details\n",
    "        order_confirmed['time'] = time\n",
    "        order_confirmed['status'] = 'confirmed'\n",
    "        producer.send(ORDER_CONFIRMED_KAFKA_TOPIC, order_confirmed)\n",
    "        print(\"Sent order details {} to kafka topic: {}\".format(order_confirmed, ORDER_CONFIRMED_KAFKA_TOPIC))\n",
    "```\n",
    "\n",
    "The dockerfile is as follows:\n",
    "\n",
    "```bash\n",
    "FROM python:3.9.7-slim\n",
    "\n",
    "RUN pip install -U pip\n",
    "RUN pip install pipenv \n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]\n",
    "\n",
    "RUN pipenv install --system --deploy\n",
    "\n",
    "COPY [ \"transactions_backend.py\", \"./\" ]\n",
    "\n",
    "ENTRYPOINT [\"python\", \"transactions_backend.py\"]\n",
    "```\n",
    "\n",
    "You can build this image and update the `docker-compose-services.yml` file:\n",
    "\n",
    "```bash\n",
    "version: \"1\"\n",
    "\n",
    "services:\n",
    "  orders_backend:\n",
    "    restart: always\n",
    "    image: orders_backend:v1\n",
    "    ports:\n",
    "      - \"5002:5002\"\n",
    "    networks:\n",
    "      - ikea-ordering-kafka_default\n",
    "  \n",
    "  transactions_backend:\n",
    "    restart: always\n",
    "    image: transactions_backend:v1\n",
    "    ports:\n",
    "      - \"5003:5003\"\n",
    "    networks:\n",
    "      - ikea-ordering-kafka_default\n",
    "\n",
    "networks:\n",
    "  ikea-ordering-kafka_default:\n",
    "    external: true\n",
    "```\n",
    "\n",
    "Then again by testing the services with postman, we can see the messages coming to the topic:\n",
    "\n",
    "![](images/kafka/kafka-ui2.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f1bf0",
   "metadata": {},
   "source": [
    "# Email Backend\n",
    "\n",
    "The code for this service is as follows:\n",
    "\n",
    "```python\n",
    "# email_backend.py\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "# from flask import Flask, jsonify, request\n",
    "\n",
    "ORDER_CONFIRMED_KAFKA_TOPIC = 'order_confirmed'\n",
    "EMAIL_SENT_KAFKA_TOPIC = 'order_email_sent'\n",
    "# KAFKA_SERVER_ADDRESS = 'localhost:9092'\n",
    "KAFKA_SERVER_ADDRESS = 'broker:29092'\n",
    "# KAFKA_SERVER_ADDRESS = '47.93.191.241:29093`\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=[KAFKA_SERVER_ADDRESS], security_protocol=\"PLAINTEXT\",\n",
    "                            value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
    "\n",
    "consumer = KafkaConsumer(ORDER_CONFIRMED_KAFKA_TOPIC, bootstrap_servers=[KAFKA_SERVER_ADDRESS], security_protocol=\"PLAINTEXT\",\n",
    "                            value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n",
    "\n",
    "\n",
    "def send_email(user_id, order_id, user_email, order_details, time, status):\n",
    "    print(\"Sending email to user: {} with order details: {}\".format(user_email, order_details))\n",
    "    # send email to user\n",
    "    # ...\n",
    "    # ...\n",
    "    # ...\n",
    "    # ...   \n",
    "    return True\n",
    "\n",
    "\n",
    "while True:\n",
    "    for message in consumer:\n",
    "        # read data from consumer and call the send_email() function\n",
    "        print(\"Received order details: {}\".format(message.value))\n",
    "        user_id = message.value['user_id']\n",
    "        order_id = message.value['order_id']\n",
    "        user_email = message.value['user_email']\n",
    "        order_details = message.value['order_details']\n",
    "        time = message.value['time']\n",
    "        status = message.value['status']\n",
    "        email_send_status = send_email(user_id, order_id, user_email, order_details, time, status)\n",
    "        email_sent = {}\n",
    "        email_sent['user_id'] = user_id\n",
    "        email_sent['order_id'] = order_id\n",
    "        email_sent['user_email'] = user_email\n",
    "        email_sent['order_details'] = order_details\n",
    "        email_sent['time'] = time\n",
    "        email_sent['status'] = email_send_status\n",
    "        producer.send(EMAIL_SENT_KAFKA_TOPIC, email_sent)\n",
    "        print(\"Sent email details {} to kafka topic: {}\".format(email_sent, EMAIL_SENT_KAFKA_TOPIC))\n",
    "\n",
    "```\n",
    "\n",
    "The docker file also similar to previous ones with small modifications for the python file name. Then you can build the image and update the docker-compose file and run it. After sending some new messages via postman, we can see the messages on the topic in the UI:\n",
    "\n",
    "![](images/kafka/kafka-ui3.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2086a150",
   "metadata": {},
   "source": [
    "# Analytics Backend\n",
    "\n",
    "The code for this service is as follows to get the confirmed order and calculate total number of orders and total revenue:\n",
    "\n",
    "```python\n",
    "# analytics_backend.py\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "\n",
    "ORDER_CONFIRMED_KAFKA_TOPIC = 'order_confirmed'\n",
    "ANALYTICS_KAFKA_TOPIC = 'analytics_result'\n",
    "# KAFKA_SERVER_ADDRESS = 'localhost:9092'\n",
    "KAFKA_SERVER_ADDRESS = 'broker:29092'\n",
    "# KAFKA_SERVER_ADDRESS = '47.93.191.241:29093`\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=[KAFKA_SERVER_ADDRESS], security_protocol=\"PLAINTEXT\",\n",
    "                            value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
    "\n",
    "consumer = KafkaConsumer(ORDER_CONFIRMED_KAFKA_TOPIC, bootstrap_servers=[KAFKA_SERVER_ADDRESS], security_protocol=\"PLAINTEXT\",\n",
    "                            value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n",
    "\n",
    "total_revenue = 0\n",
    "total_orders_count = 0\n",
    "\n",
    "while True:\n",
    "    for message in consumer:\n",
    "        # read data from consumer and do some analytics on it\n",
    "        print(\"Received order details: {}\".format(message.value))\n",
    "        order_details = message.value['order_details']\n",
    "        total_revenue += int(order_details['price'])\n",
    "        total_orders_count += 1\n",
    "        analytics = {}\n",
    "        analytics['total_revenue'] = total_revenue\n",
    "        analytics['total_orders_count'] = total_orders_count\n",
    "        producer.send(ANALYTICS_KAFKA_TOPIC, analytics)\n",
    "        print(\"Sent analytics details {} to kafka topic: {}\".format(analytics, ANALYTICS_KAFKA_TOPIC))\n",
    "```\n",
    "\n",
    "The docker file is again similar to previous ones with a small modification. Then build the image and update the compose file, and finally run it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7a965b",
   "metadata": {},
   "source": [
    "We can send some new messages via postman and see the messages on the topic in the UI:\n",
    "\n",
    "![](images/kafka/kafka-ui4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5d124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40416547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7531891f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5a5bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a6c298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "840eee5b",
   "metadata": {},
   "source": [
    "# Frontend App\n",
    "First, let's get started with the frontend:\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3417d95",
   "metadata": {},
   "source": [
    "ref: https://www.youtube.com/watch?v=qi7uR3ItaOY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f62a59e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191737a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
