{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the icml 2020 tutorial on bayesian deep learning. \n",
    "We'll start with a pattern recognition problem where we have airline passenger numbers indexed by time we'll consider three different modeling choices:\n",
    "\n",
    "1- a linear function\n",
    "\n",
    "2- a cubic polynomial and choice\n",
    "\n",
    "3- a ten thousandth order polynomial\n",
    "\n",
    "<img src=\"images/bayesdl/bayesdlicml2020-002.png\">\n",
    "\n",
    "which choice you would make in order to provide a good description of the data? most people go with choices one or two. In this tutorial we'll argue for choice three, because the real world is a complicated place and there'll be some setting of the coefficients in choice three the $w_j$s which provide a better description of reality than could be possibly managed by choices one or two which are just special cases of choice three.\n",
    "\n",
    "In practice we're often making something like choice three and using neural nets that have tens of millions of parameters to follow problems with just tens of thousands of data points and finding we often get very good generalization. In part two of this talk we'll actually consider models with an infinite number of parameters that at the same time have very simple inductive dot biases and provide great generalization even on problems with a small number of data points. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin understanding the motivation for choice three. It's helpful to think about modeling from a function space perspective let's consider choice one, the linear function.\n",
    "\n",
    "<img src=\"images/bayesdl/bayesdlicml2020-003.png\">\n",
    "\n",
    "$f(x)=w_0 + w_1 x$ and we'll just put a standard normal distribution over $w_0$ and $w_1$ and this will induce a distribution over functions which we can visualize by sampling from this distribution over the parameters and looking at the different straight lines with different slopes and intercepts that we get. We'll get different $w_0$ and $w_1$s. Different straight lines the gray shade here shows a 95 credible set containing 95 of these functions and the solid blue curve here shows the expectation of this distribution over functions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/bayesdl/bayesdlicml2020-004.png\">\n",
    "\n",
    "In this diagram we have a conceptualization of all possible data sets on the horizontal axis and the marginal likelihood or evidence on the vertical axis which is the\n",
    "probability that we would generate a\n",
    "data set\n",
    "if we were to randomly sample from the\n",
    "parameters of our model\n",
    "the model which we were considering on\n",
    "the last slide can just generate\n",
    "straight lines with different slopes and\n",
    "intercepts not very many data sets\n",
    "but because the marginal likelihood is a\n",
    "proper normalizable probability density\n",
    "it's going to give a lot of mass to\n",
    "those data sets we could alternatively\n",
    "consider a model like a multi-layer\n",
    "perceptron which might have a lot of\n",
    "hidden units and layers and we have a\n",
    "broad distribution over the parameters\n",
    "and it can generate a wide range of\n",
    "different data sets\n",
    "but it won't give any one of those data\n",
    "sets very much probability\n",
    "we could alternatively consider a third\n",
    "type of model like a convolutional\n",
    "neural net which is very flexible it can\n",
    "describe a wide array of different data\n",
    "sets\n",
    "but at the same time it has very\n",
    "particular inductive biases like\n",
    "translation equivariance which says that\n",
    "if we translate an image we don't want\n",
    "to change the class label\n",
    "and this means that these types of\n",
    "models are going to give a reasonable\n",
    "amount of mass also to structured image\n",
    "data sets and provide\n",
    "good generalization on those problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "in order to construct models with good\n",
    "generalization we'll argue that we want\n",
    "large support the support being which\n",
    "solutions are a priority possible\n",
    "and reasonably calibrated inductive\n",
    "biases the inductive biases being the\n",
    "distribution of support which solutions\n",
    "are a priori likely\n",
    "so we can consider the support to be the\n",
    "flexibility and we want a lot of\n",
    "flexibility\n",
    "at the same time we should be careful\n",
    "not to conflate flexibility and\n",
    "complexity\n",
    "in fact in part two as we've mentioned\n",
    "we'll be considering models with\n",
    "infinitely many parameters that are\n",
    "extraordinarily flexible\n",
    "and at the same time provide very good\n",
    "generalization even given very small\n",
    "data sets\n",
    "by the same token we should not treat\n",
    "parameter counting as a proxy for\n",
    "complexity\n",
    "in this figure on the left panel we have\n",
    "the same diagram we had on the previous\n",
    "slide in the other panels\n",
    "we see what happens as these different\n",
    "models are exposed to a given data set\n",
    "in green here we see in the second panel\n",
    "the model with\n",
    "large support is able to contain a good\n",
    "ground truth description of reality\n",
    "but it has well-calibrated inductive\n",
    "biases so it efficiently collapses down\n",
    "onto that good description of reality\n",
    "in the next panel in blue we can hardly\n",
    "describe\n",
    "many data sets at all just straight\n",
    "lines with different slopes and\n",
    "intercepts and so\n",
    "while the model becomes quickly\n",
    "constrained by the available data it\n",
    "becomes erroneously constrained and it\n",
    "starts to collapse down onto a bad\n",
    "solution\n",
    "and in the last panel we have a flexible\n",
    "model which cap\n",
    "casts wide enough a net to provide a\n",
    "good description of reality\n",
    "but it it doesn't have it spreads its\n",
    "support too thinly to have good\n",
    "inductive biases and reasonable\n",
    "contraction\n",
    "around that good description of reality\n",
    "in this tutorial we'll argue that the\n",
    "key distinguishing property of a\n",
    "bayesian approach\n",
    "is marginalization rather than\n",
    "optimization\n",
    "that is instead of use a single setting\n",
    "of parameters w\n",
    "we want to use all possible settings of\n",
    "parameters and weight them by their\n",
    "posterior probabilities in what's called\n",
    "a bayesian model average\n",
    "and we'll argue that this bayesian model\n",
    "average will be especially relevant\n",
    "in deep learning\n",
    "i first became interested in bayesian\n",
    "deep learning\n",
    "after listening to a talk about\n",
    "optimization which is perhaps a bit\n",
    "ironic given what i said\n",
    "on the previous slide we might\n",
    "characterize bayesian methods as trying\n",
    "to avoid optimization at all costs\n",
    "don't just bet everything on a single\n",
    "setting of parameters use all possible\n",
    "settings of parameters\n",
    "the argument was being made that mini\n",
    "batch std would converge to flat regions\n",
    "of the loss which would provide\n",
    "better generalization in deep learning\n",
    "than full batch gradient methods\n",
    "in this diagram we have a\n",
    "conceptualization of parameters on the\n",
    "horizontal axis\n",
    "and the value of the loss on the\n",
    "vertical axis training loss in black\n",
    "testing loss in red we can see that a\n",
    "flat solution and train\n",
    "has reasonably low loss and test whereas\n",
    "a sharp solution has pretty high loss\n",
    "after there's this horizontal shift\n",
    "between the training and the test loss\n",
    "which will typically happen because our\n",
    "model won't be\n",
    "completely determined by a finite sample\n",
    "when we evaluate the loss on different\n",
    "sets of points even if they're drawn\n",
    "from the same distribution\n",
    "we should get a different optimal\n",
    "setting of parameters and the shape of\n",
    "the losses should be relatively similar\n",
    "because the training loss is still not a\n",
    "horrible proxy for generalization\n",
    "now if this argument were true and it\n",
    "meant something\n",
    "then to mean something the different\n",
    "parameters in the flat region would have\n",
    "to correspond to different functions\n",
    "which provide\n",
    "compelling and complementary\n",
    "explanations for the data otherwise we\n",
    "could just\n",
    "contrive flatness to re-parametrization\n",
    "and it wouldn't really\n",
    "mean anything then this was an\n",
    "extraordinary argument for following a\n",
    "bayesian approach and doing\n",
    "marginalization or integration basically\n",
    "integrating a flipped version\n",
    "of this curve where we want to consider\n",
    "all of those good solutions and weight\n",
    "them by their posterior probabilities\n",
    "in a sense it might just be a bit\n",
    "arbitrary if everything on just one good\n",
    "solution\n",
    "we know that there are many\n",
    "and so there are many reasons to be\n",
    "excited about bayesian deep learning\n",
    "neural nets can represent a variety of\n",
    "complementary explanations for the data\n",
    "and we'll be seeing this particularly in\n",
    "part three\n",
    "and this will lead to better uncertainty\n",
    "representation which is crucial for\n",
    "decision making\n",
    "we could think from a practical\n",
    "perspective if our model could never\n",
    "influence\n",
    "a decision conceivably then it might not\n",
    "have much of a practical impact but it\n",
    "will also have a big effect\n",
    "on the accuracy of our point predictions\n",
    "which is\n",
    "perhaps an under appreciated aspect of\n",
    "the benefits of bayesian marginalization\n",
    "in deep learning in particular because\n",
    "there are all these different\n",
    "complementary\n",
    "solutions we can form a rich ensemble\n",
    "of high performing and diverse solutions\n",
    "and by doing that we'll often get\n",
    "much better accuracy if we can do this\n",
    "marginalization effectively\n",
    "bayesian neural nets were also a gold\n",
    "standard for a wide variety of problems\n",
    "in the second wave of neural nets\n",
    "led in many ways by bradford neil's\n",
    "hamiltonian monte carlo approaches\n",
    "which don't scale to modern\n",
    "architectures but we know that\n",
    "in a sense there's treasure buried in\n",
    "some direction and we just need to\n",
    "build the right tools as a community to\n",
    "extract that treasure\n",
    "as we started to see neural nets are\n",
    "also much less mysterious\n",
    "when viewed through the lens of\n",
    "probability theory\n",
    "over parametrization double descent\n",
    "model construction\n",
    "and many other properties like being\n",
    "able to fit random labels\n",
    "become very understandable when we think\n",
    "about things from a probabilistic\n",
    "perspective\n",
    "why not these models can be\n",
    "computationally intractable and can\n",
    "involve a lot of moving parts design\n",
    "decisions approximate inference\n",
    "procedures and so on but they don't have\n",
    "to and in the last year there's been\n",
    "really extraordinary empirical progress\n",
    "for bayesian deep learning\n",
    "where we now have several methods often\n",
    "providing better practical results than\n",
    "classical training\n",
    "without significant overhead on quite a\n",
    "wide variety of problems\n",
    "this tutorial will have four parts this\n",
    "first part is about foundations of\n",
    "bayesian\n",
    "machine learning particularly with\n",
    "respect to\n",
    "how bayesian methods could impact deep\n",
    "learning in particular\n",
    "in part two we'll be considering a\n",
    "function space perspective of machine\n",
    "learning\n",
    "in part three we'll consider several\n",
    "practical methods for\n",
    "modern bayesian deep learning the goal\n",
    "of this part isn't just to\n",
    "enumerate all the state-of-the-art\n",
    "methods but rather to\n",
    "exemplify many of the foundational\n",
    "concepts that we introduced in other\n",
    "parts of the tutorial\n",
    "with several modern approaches and in\n",
    "part four we'll be considering\n",
    "bayesian model construction and\n",
    "generalization including deep ensembles\n",
    "and their connection with bayesian\n",
    "marginalization and how we can build on\n",
    "those connections for the multi-swag\n",
    "approach which\n",
    "marginalizes within multiple basins of\n",
    "attraction\n",
    "tempering prior specification rethinking\n",
    "generalization double descend with that\n",
    "trade-offs and a variety of other topics\n",
    "now add a brief disclaimer which is\n",
    "similar to a nice disclaimer i saw in a\n",
    "nurip's tutorial\n",
    "on deep learning with bayesian\n",
    "principles this tutorial\n",
    "is not meant to be a review of all\n",
    "things bayesian deep learning that may\n",
    "have actually been possible three years\n",
    "ago but i'm excited to say it's not\n",
    "anymore we're having\n",
    "workshops now on busy and deep learning\n",
    "with hundreds of paper submissions\n",
    "rather this tutorial is meant to provide\n",
    "a complimentary perspective which is\n",
    "largely based in my own experiences and\n",
    "expertise\n",
    "a decent portion will be based on my own\n",
    "work in a sense it's\n",
    "what i would tell myself if i could\n",
    "build a time machine and\n",
    "go back in time that said if you feel i\n",
    "should have included something\n",
    "please send me an email and i'll try to\n",
    "include it next time\n",
    "so let's go back to this airline\n",
    "passenger number example\n",
    "and really forget everything that we\n",
    "know about\n",
    "statistics and machine learning and you\n",
    "know think about the foundations\n",
    "and use this example also to set up a\n",
    "lot of notation that we'll use\n",
    "throughout the tutorial\n",
    "so we have these n training points or\n",
    "targets observations\n",
    "y and they're indexed by x's x one up to\n",
    "x n\n",
    "generally the x's could be like time\n",
    "spatial locations images\n",
    "and we want to make a prediction at some\n",
    "arbitrary test input\n",
    "x star in this case could be like the\n",
    "airline passenger number is in 1961.\n",
    "now just pause the video for a moment\n",
    "and think about a step-by-step procedure\n",
    "that you might might have followed just\n",
    "knowing what you knew in high school\n",
    "in order to solve this problem\n",
    "if it were me in high school i would\n",
    "start by thinking about the functional\n",
    "forms that i'm familiar with like\n",
    "science cosines exponentials\n",
    "polynomials and then i would create a\n",
    "functional form that\n",
    "i thought would be a reasonable\n",
    "description of of what i'm seeing\n",
    "and it might have some free parameters\n",
    "and i would specify an error function\n",
    "which could be like the square distance\n",
    "between the outputs of\n",
    "of my function and and the training\n",
    "observations\n",
    "and i would minimize that error function\n",
    "with respect to my parameters to learn\n",
    "those parameters\n",
    "but this approach would involve a lot of\n",
    "ad hoc design decisions like y squared\n",
    "error and not\n",
    "absolute error for instance\n",
    "we can instead follow a probabilistic\n",
    "approach where we\n",
    "suppose that our observations are drawn\n",
    "from a noise-free function\n",
    "f x w plus for example additive gaussian\n",
    "noise with noise variance sigma squared\n",
    "and we can then use this observation\n",
    "model to form\n",
    "a likelihood and then we can maximize\n",
    "that likelihood with respect to our\n",
    "parameters and learn those parameters\n",
    "and then use our\n",
    "conditional predictive distribution\n",
    "given those parameters to make our\n",
    "predictions\n",
    "we can see by taking logs of the\n",
    "likelihood that if we follow this\n",
    "approach\n",
    "we'll get exactly the same point\n",
    "predictions as we had using the approach\n",
    "on the previous slide where we just\n",
    "specified the squared error function\n",
    "however in this approach the design\n",
    "decisions are a bit more interpretable\n",
    "we probably have some intuitions for\n",
    "example about\n",
    "whether we want to use gaussian noise\n",
    "perhaps\n",
    "if we thought there were outliers in our\n",
    "problem we might use a heavy-tailed\n",
    "noise model like a laplace distribution\n",
    "and that would lead to an absolute value\n",
    "error function\n",
    "so we can make different design\n",
    "decisions here and derive different loss\n",
    "functions\n",
    "if we believe our model f x w to some\n",
    "extent we can also get\n",
    "an estimate of the noise variance in the\n",
    "data\n",
    "now remembering what we know about\n",
    "statistics\n",
    "you may be familiar with the idea that\n",
    "either of those approaches could lead to\n",
    "what's called overfitting\n",
    "where we get very low training loss but\n",
    "we get\n",
    "very bad testing error\n",
    "in order to combat overfitting it's\n",
    "quite popular to introduce what's called\n",
    "a regularizer where we add\n",
    "some kind of complexity penalty like we\n",
    "want to penalize the the magnitude of\n",
    "the weights in our model\n",
    "but this also involves all sorts of\n",
    "heuristic design decisions like\n",
    "how do we know whether we want large\n",
    "weights or small weights it would\n",
    "totally depend on the parametrization of\n",
    "our model\n",
    "what is complexity how much should we\n",
    "penalize it we could use some kind of\n",
    "lambda parameter maybe it determines\n",
    "your cross validation but\n",
    "what would be our validation sets and if\n",
    "we had several\n",
    "lambda parameters then we'd have a\n",
    "cursive dimensionality in estimating\n",
    "those parameters\n",
    "we can gain some interpretability by\n",
    "thinking about maximizing a log\n",
    "posterior which would equal a log\n",
    "likelihood plus a log\n",
    "prior and the log prior could be\n",
    "interpreted as a regularizer\n",
    "but this really isn't a bayesian\n",
    "approach\n",
    "there isn't much you need to know in\n",
    "order to use a bayesian approach for\n",
    "your own research\n",
    "we have bayes rule here which is often\n",
    "expressed as a posterior being\n",
    "proportional to a likelihood times of\n",
    "prior\n",
    "the normalization constant is the\n",
    "marginal likelihood what we were\n",
    "considering on the vertical axis of that\n",
    "plot\n",
    "we had all possible data sets on the\n",
    "horizontal axis\n",
    "the sum rule says the marginal\n",
    "distribution of p of x is equal to the\n",
    "sum\n",
    "over the joint distribution of p of x\n",
    "and y summing out y\n",
    "the product rule says that the joint\n",
    "distribution over x and y\n",
    "is equal to the conditional distribution\n",
    "of p of x given y times p of y\n",
    "or the conditional distribution of y\n",
    "given x times p of x and we can derive\n",
    "the\n",
    "phase rule from the product rule\n",
    "now ultimately we want to compute the\n",
    "unconditional predictive distribution p\n",
    "of y\n",
    "given our data bulb y but not given\n",
    "parameters\n",
    "and so the sum and product rules give us\n",
    "this integral in equation\n",
    "11. the integral of p of y given the\n",
    "parameters times the posterior over\n",
    "those parameters given the data y\n",
    "and so this is called marginalization\n",
    "because we see w\n",
    "doesn't appear on the left side it does\n",
    "on the right\n",
    "in words this integral is saying let's\n",
    "not just use one setting of parameters\n",
    "let's use all possible settings of\n",
    "parameters weighted by their posterior\n",
    "probabilities\n",
    "and this isn't a controversial\n",
    "expression it's a direct consequence of\n",
    "the sum and product rules of probability\n",
    "this model average represents what's\n",
    "called epistemic uncertainty over which\n",
    "function\n",
    "fits the data there are many different\n",
    "functions corresponding to different\n",
    "settings of the parameters\n",
    "and we're not sure given a finite sample\n",
    "which is the right description of the\n",
    "data\n",
    "by representing episomic uncertainty we\n",
    "can\n",
    "we can have some robustness against\n",
    "against overfitting\n",
    "we can also view classical training as a\n",
    "special case of this approach where we\n",
    "have an approximate posterior q of w\n",
    "given our data y\n",
    "equal to just a point mass a delta\n",
    "function centered on the map the\n",
    "regularized maximum likelihood\n",
    "solution of parameters we can see that\n",
    "if we substitute this in\n",
    "we're just going to get our conditional\n",
    "predictive distribution given those\n",
    "maximum likelihood or map parameters\n",
    "we can also see then that bayesian and\n",
    "classical approaches will be\n",
    "similar when the posterior is highly\n",
    "concentrated around a setting of\n",
    "parameters which of course is exactly\n",
    "not the case\n",
    "in deep learning where we have neural\n",
    "nets that are\n",
    "very diffuse in their posteriors and\n",
    "also the posteriors capture a variety of\n",
    "different models corresponding to\n",
    "complementary explanations\n",
    "of the data\n",
    "so we're going to especially want to do\n",
    "this integral in deep learning\n",
    "we can also see that we can probably do\n",
    "a lot better than classical training in\n",
    "terms of estimating this integral\n",
    "by using some fairly simple posteriors\n",
    "which\n",
    "might not be good descriptions of the\n",
    "exact posterior but are still a lot\n",
    "better than a point mass\n",
    "so we can definitely improve our\n",
    "estimates without needing to have\n",
    "you know an exact approximation of this\n",
    "integral or exact\n",
    "representation of this integral\n",
    "now there are fundamental differences\n",
    "between bayesian model averaging and\n",
    "some types of model combination in\n",
    "particular the bayesian model average is\n",
    "meant to represent\n",
    "a statistical inability to distinguish\n",
    "between hypotheses given limited\n",
    "information but the assumption\n",
    "is that one of those hypotheses one\n",
    "setting of those parameters is the\n",
    "correct setting of parameters\n",
    "and as we get more and more data our\n",
    "posterior over our hypotheses our\n",
    "parameters\n",
    "will collapse onto a particular setting\n",
    "and we'll recover the maximum likelihood\n",
    "solution\n",
    "this is different than some approaches\n",
    "to ensembling and model combination\n",
    "which work by enriching the hypothesis\n",
    "space and assuming for example the\n",
    "combination models might be a correct\n",
    "description\n",
    "of reality\n",
    "now let's exemplify some of these ideas\n",
    "with a few applications\n",
    "suppose we flip a bias coin with the\n",
    "probability lambda\n",
    "of landing tails i'd like you to pause\n",
    "the video\n",
    "and answer these three questions one\n",
    "what is the likelihood of a set of data\n",
    "y1 up to yn so we're just doing\n",
    "m flips maybe we see m tails\n",
    "two what is the maximum likelihood\n",
    "solution for lambda\n",
    "three suppose the first flip is tails\n",
    "what is the probability that the next\n",
    "flip will be tails\n",
    "using our maximum likelihood estimate\n",
    "for lambda\n",
    "you can assume m tails and n flips\n",
    "so the likelihood of our data is just a\n",
    "product of bernoulli distributions two\n",
    "possible outcomes here we have y\n",
    "equals one if y i is tails and y equals\n",
    "zero if y is\n",
    "heads if we don't care about ordering\n",
    "and we observe m tails then our\n",
    "likelihood is\n",
    "a binomial distribution\n",
    "m tails here probability of getting\n",
    "tails is lambda\n",
    "and we can easily maximize this\n",
    "likelihood you could try taking\n",
    "logs of this expression then derivatives\n",
    "with respect to lambda setting those\n",
    "derivatives to equal to zero and so on\n",
    "we'll get the solution that the maximum\n",
    "likelihood setting of lambda is m over n\n",
    "where we have m tails and total flips\n",
    "which in a sense\n",
    "is kind of intuitive but on the other\n",
    "hand is kind of problematic\n",
    "why do you think this estimate might be\n",
    "problematic and in considering this\n",
    "question\n",
    "think about the third part of the\n",
    "problem what's the probability\n",
    "that we would get tails on the next flip\n",
    "assuming we've done one flip and we've\n",
    "just observed tails\n",
    "using this estimator pause the video and\n",
    "think about the problem for a moment\n",
    "so if we substitute in m equals one n\n",
    "equals one\n",
    "we're saying there's a one hundred\n",
    "percent chance that the next flip was\n",
    "tails\n",
    "do you believe that of course not and\n",
    "when we arrive at a clearly unbelievable\n",
    "prediction\n",
    "it's usually because some part of our\n",
    "model modeling procedure\n",
    "has not honestly represented our beliefs\n",
    "let's think about a bayesian approach\n",
    "this problem if we choose a prior p of\n",
    "lambda\n",
    "proportional to lambda to the alpha one\n",
    "minus lambda to the beta\n",
    "then the posterior after we multiply the\n",
    "likelihood against the prior\n",
    "will have the same functional form as\n",
    "the priors this is called a conjugate\n",
    "prior\n",
    "a beta distribution has this functional\n",
    "form\n",
    "the gamma functions here for\n",
    "normalization\n",
    "we can analytically compute the moments\n",
    "of the beta distribution\n",
    "here we have visualizations of the beta\n",
    "distribution corresponding to different\n",
    "settings of its parameters a\n",
    "and b we can see in the top right panel\n",
    "if we if we want to express the belief\n",
    "that we don't know what the bias is then\n",
    "we can use a uniform distribution\n",
    "setting a and b equal to one and this\n",
    "means lambda\n",
    "is equally probable for any value\n",
    "between zero and one a priori\n",
    "and so we can express you know\n",
    "even the idea that we really just don't\n",
    "know using a prior distribution we don't\n",
    "need to have\n",
    "an informative prior however we might\n",
    "want to consider\n",
    "a prior that says well we think that the\n",
    "bias is probably close to a half but\n",
    "we're not going to say it's\n",
    "definitely a half just choose whichever\n",
    "prior\n",
    "is an honest reflection of your beliefs\n",
    "even if your belief is\n",
    "i don't know so we can\n",
    "multiply our prior with our likelihood\n",
    "to get our unnormalized posterior\n",
    "this is a beta distribution we can\n",
    "compute its moments and\n",
    "use the posterior expectation over\n",
    "lambda for our predictions\n",
    "we can see in equation 27 it's m plus a\n",
    "over\n",
    "n plus a plus b now let's consider\n",
    "a few questions it's good to do some\n",
    "sanity checks here\n",
    "what's the probability the next flip is\n",
    "tails let's suppose that\n",
    "a and b are one we can see that it's not\n",
    "going to be\n",
    "a hundred percent if m is one and n is\n",
    "one so that's good\n",
    "what happens when we make a and b really\n",
    "large well the prior starts to dominate\n",
    "that gives us a strong prior\n",
    "if we make the data really large then\n",
    "both n and m will be large\n",
    "and m over n will dominate in this\n",
    "estimate it will recover the maximum\n",
    "likelihood solution\n",
    "which is what we want that's a good\n",
    "sanity check\n",
    "and now i'd like you to consider this\n",
    "fourth question which is conceptually\n",
    "very important\n",
    "does the map estimate what we get when\n",
    "we\n",
    "take the r max of the log posterior over\n",
    "lambda\n",
    "which is equal to the arg max of the log\n",
    "likelihood plus the log prior\n",
    "with a uniform prior over lambda\n",
    "give the same answer as bayesian\n",
    "marginalization\n",
    "to find the probability that the next\n",
    "flip is tails\n",
    "pause the video and think about this\n",
    "question for a minute\n",
    "so if we have a uniform prior over\n",
    "lambda\n",
    "then log p of lambda won't affect our\n",
    "optimization in equation 31 and we'll\n",
    "just get the maximum likelihood solution\n",
    "and we just saw that when we do\n",
    "marginalization we get a different\n",
    "answer than the maximum likelihood\n",
    "solution even with a uniform prior\n",
    "so i can't emphasize enough that we\n",
    "should not interpret\n",
    "bayesian methods as regularizers in\n",
    "optimization\n",
    "there is a conceptually very important\n",
    "difference\n",
    "between marginalization and regularized\n",
    "maximum likelihood\n",
    "optimization and that difference will be\n",
    "practically\n",
    "crucial when we're thinking about\n",
    "bayesian methods\n",
    "in deep learning\n",
    "let's consider one more example suppose\n",
    "we have observations y1 up to yn\n",
    "drawn from an unknown density p of y\n",
    "we'll start by specifying an observation\n",
    "model we'll suppose that the points are\n",
    "drawn from a mixture of gaussians and in\n",
    "order to estimate this unknown density\n",
    "we'll\n",
    "learn the parameters of this mixture of\n",
    "two gaussians\n",
    "parameters here are the weights the\n",
    "means and the variances\n",
    "so we can use the observation model to\n",
    "form a likelihood and i've just written\n",
    "it down here\n",
    "and i'd like you to pause again and\n",
    "think about\n",
    "choosing a setting of parameters which\n",
    "will provide\n",
    "a lot of likelihood without you know\n",
    "having to take derivatives and stuff\n",
    "like that you can just kind of look at\n",
    "this expression\n",
    "and play with a few settings and find\n",
    "something and just think about the\n",
    "the means and the variances don't worry\n",
    "about the weights\n",
    "so if we make for example the mean of\n",
    "the first component equal to one of the\n",
    "data points\n",
    "then this x will disappear it'll just be\n",
    "1 and we get this normalization constant\n",
    "w1 over square root 2 pi sigma 1 squared\n",
    "and then we can make sigma 1 the\n",
    "variance or the standard deviation very\n",
    "small for the first component\n",
    "and then this term will blow up and the\n",
    "other term we can use to assign density\n",
    "to all the points so we're not\n",
    "multiplying again zeros and the\n",
    "likelihood goes to infinity\n",
    "now do we believe this solution of\n",
    "course not we typically wouldn't believe\n",
    "our data are comprised of point masses\n",
    "and when we reach an unbelievable\n",
    "solution it's typically because we\n",
    "haven't\n",
    "fully represented our beliefs in our\n",
    "modeling procedure\n",
    "we could introduce a regularizer or a\n",
    "prior which would go to zero faster than\n",
    "the likelihood goes to infinity\n",
    "as the variance parameters go to zero\n",
    "but we might want to include the point\n",
    "mass solution\n",
    "as long as it's one of an uncountably\n",
    "infinite number of solutions\n",
    "which we can do through full bayesian\n",
    "marginalization in which case we can use\n",
    "extremely flexible models\n",
    "even infinite mixtures of gaussians\n",
    "corresponding to dirichlet processed\n",
    "mixture models\n",
    "and achieve good generalization even\n",
    "with a small number of points\n",
    "now ultimately as we've been saying we\n",
    "wish to compute a bayesian model average\n",
    "corresponding to our unconditional\n",
    "predictive distribution\n",
    "p of y given data rather than our\n",
    "conditional predictive distribution\n",
    "p of y given parameters w this\n",
    "unconditional predictive distribution is\n",
    "equal to the integral\n",
    "of our conditional predictive\n",
    "distribution times our posterior\n",
    "p of w given data this is just an\n",
    "expression of the sum and product rules\n",
    "of probability\n",
    "rather than use a single setting of\n",
    "parameters we want to use all possible\n",
    "settings of parameters\n",
    "weighted by their posterior\n",
    "probabilities which is going to be\n",
    "especially impactful in deep learning\n",
    "where we have\n",
    "highly diffused posteriors containing\n",
    "different settings of parameters that\n",
    "correspond to a variety of compelling\n",
    "and different solutions to a given\n",
    "problem\n",
    "for most models including bayesian\n",
    "neural nets this integral is not\n",
    "analytic\n",
    "it's common to use what's called a\n",
    "simple monte carlo approximation\n",
    "where we take an average of the\n",
    "conditional predictive distributions\n",
    "for different settings of parameters\n",
    "sampled from an approximate posterior q\n",
    "of w given data\n",
    "we find these samples typically through\n",
    "one of two approaches\n",
    "deterministic methods approximate the\n",
    "posterior distribution with some\n",
    "convenient distribution q\n",
    "although the integral can't be computed\n",
    "in closed form\n",
    "typically we can represent the\n",
    "unnormalized posterior analytically\n",
    "it's just the likelihood times the prior\n",
    "q our approximate posterior is chosen\n",
    "for convenience often so that it's easy\n",
    "to sample from\n",
    "like a gaussian distribution in which\n",
    "case its parameters would be its mean\n",
    "vector and its covariance matrix\n",
    "which we choose typically to make q\n",
    "close to p\n",
    "in some sense for example variational\n",
    "methods find these parameters by\n",
    "minimizing the kl divergence between\n",
    "q and p as we mentioned earlier\n",
    "classical training is a special case of\n",
    "approximate inference\n",
    "where our approximate posterior is just\n",
    "a point mass centered at the maximum\n",
    "likelihood\n",
    "or regularized maximum likelihood map\n",
    "setting of parameters\n",
    "the laplace approximation is another\n",
    "popular deterministic method which we'll\n",
    "discuss further in part three\n",
    "expectation propagation is another\n",
    "popular approach and there are several\n",
    "others\n",
    "we could alternatively consider markov\n",
    "chain monte carlo which forms a markov\n",
    "chain of approximate but asymptotically\n",
    "exact samples from our posterior\n",
    "metropolis hastings is a popular mcmc\n",
    "approach\n",
    "hamiltonian monte carlo uses gradient\n",
    "information and was very\n",
    "successfully developed by radford neal\n",
    "in the mid-90s for bayesian neural nets\n",
    "recently stochastic gradient mcmc\n",
    "methods have been very\n",
    "up-and-coming and exciting approaches in\n",
    "bayesian deep learning because they\n",
    "algorithmically resemble sgd which means\n",
    "they can be applied in\n",
    "a wide variety of variety of\n",
    "applications where you might otherwise\n",
    "use classical training but often with\n",
    "better results stochastic gradient\n",
    "longitude dynamics and\n",
    "sarcastic radiant hamiltonian monte\n",
    "carlo our stochastic gradient mcmc\n",
    "approaches\n",
    "we'll discuss in part three later in\n",
    "part four\n",
    "we'll also argue that we may sometimes\n",
    "want to avoid the simple monte carlo\n",
    "perspective in equation 33 really what\n",
    "we're most interested in\n",
    "ultimately is estimating this\n",
    "unconditional predictive distribution in\n",
    "equation 32\n",
    "under computational constraints and from\n",
    "this perspective it's helpful to think\n",
    "of estimating that integral as an\n",
    "active learning problem under\n",
    "constraints in which case the deep\n",
    "ensembles method can be very\n",
    "compelling as an approximate bayesian\n",
    "method which we'll discuss\n",
    "in part four\n",
    "that's the end of part one in part two\n",
    "we'll be considering a function space\n",
    "perspective\n",
    "hi i'm andrew wilson and welcome back to\n",
    "part two of the icml 2020 tutorial\n",
    "on bayesian deep learning in this part\n",
    "we'll be considering a function space\n",
    "perspective\n",
    "of machine learning\n",
    "we'll cover gaussian processes infinite\n",
    "neural nets\n",
    "how training a neural net is like\n",
    "learning a kernel bayesian\n",
    "non-parametric deep learning\n",
    "and several other topics\n",
    "from the function space perspective the\n",
    "parameters in isolation\n",
    "are entirely divorced from the\n",
    "statistical properties of a model\n",
    "in part one we considered how in\n",
    "regularization whether or not we want\n",
    "large weights or small weights entirely\n",
    "depends\n",
    "on the parametrization of the function\n",
    "that we're using\n",
    "yet we focus most of our modeling\n",
    "efforts on learning parameters w\n",
    "what we really care about are how those\n",
    "parameters w combine with the functional\n",
    "form\n",
    "f x w ideally we want to perform\n",
    "inference\n",
    "directly in function space\n",
    "let's return to the example that we\n",
    "considered in part one\n",
    "where we had a distribution over\n",
    "functions induced by a distribution over\n",
    "parameters in a linear model we'll\n",
    "consider f of x w\n",
    "equals w naught plus w one x and we'll\n",
    "place a standard normal distribution\n",
    "over w naught and w one we can sample\n",
    "from that distribution\n",
    "to get different values of w naught and\n",
    "w1\n",
    "corresponding to different straight\n",
    "lines with different slopes and\n",
    "intercepts\n",
    "the gray shade here corresponds to a 95\n",
    "credible set\n",
    "95 of these functions are in that shade\n",
    "and the solid blue line corresponds to\n",
    "our expectation\n",
    "of this induced distribution over\n",
    "functions\n",
    "we'll now consider the more general\n",
    "model class where we have an inner\n",
    "product of a vector of weights w\n",
    "with a vector of basis functions phi\n",
    "the entries of phi for example could be\n",
    "one x x squared x cubed if we're\n",
    "considering a polynomial basis we could\n",
    "alternatively consider a fourier basis\n",
    "this model class is quite general and\n",
    "we'll place a gaussian distribution\n",
    "over w with mean zero for convenience\n",
    "and covariance matrix sigma w\n",
    "now i'd like you to pause the video for\n",
    "a second and see if you can derive the\n",
    "moments of the induced distribution\n",
    "over functions given by equations 38 and\n",
    "39\n",
    "now all of the randomness in this model\n",
    "is coming from the distribution over\n",
    "parameters w\n",
    "so we can pull w out of we can pull phi\n",
    "out of the expectation it's just\n",
    "deterministic\n",
    "and we can evaluate the expectation over\n",
    "w\n",
    "and by definition in this case it's just\n",
    "zero so the mean of this induced\n",
    "distribution over functions is zero\n",
    "now f of x i and f of x j\n",
    "are two different random variables\n",
    "corresponding to querying\n",
    "our random function at two different\n",
    "input points\n",
    "x i and x j so the inputs could be time\n",
    "spatial locations whatever is\n",
    "indexing our random function\n",
    "we can use the definition of covariance\n",
    "and derive the covariance function\n",
    "also known as the kernel function in\n",
    "this case as the inner product\n",
    "of phi of x i with phi of x j\n",
    "under sigma w now i'd like you to\n",
    "consider whether there are any\n",
    "higher moments in this induced\n",
    "distribution over functions\n",
    "in this case there are no higher moments\n",
    "because we just have a sum of gaussian\n",
    "random variables\n",
    "in equation 40 and gaussians are closed\n",
    "under addition\n",
    "in fact f of x is a gaussian process\n",
    "with a mean function m of x and a\n",
    "covariance function or a kernel\n",
    "k x x prime for two arbitrary inputs x\n",
    "and x prime\n",
    "in this case we can actually do\n",
    "inference directly over f\n",
    "x instead of over parameters w\n",
    "formerly a gaussian process is a\n",
    "collection of random variables\n",
    "any finite number of which have a joint\n",
    "gaussian distribution\n",
    "we can use a gaussian process to define\n",
    "a prior over functions\n",
    "this notation here f x is distributed as\n",
    "a gp\n",
    "means that any collection of function\n",
    "values queried at any collection of\n",
    "inputs\n",
    "x1 up to xn these could be time spatial\n",
    "locations images etc\n",
    "has a joint multivariate normal\n",
    "distribution with a mean vector mu\n",
    "defined by the mean function of the\n",
    "gaussian process\n",
    "and a covariance matrix k defined by the\n",
    "covariance function or kernel of the\n",
    "gaussian process\n",
    "created all possible pairs of inputs x1\n",
    "up to xn\n",
    "in the bottom left panel here we have\n",
    "samples from our induced distribution\n",
    "over functions to create this sample\n",
    "to create these samples i chose a bunch\n",
    "of input points\n",
    "a bunch of x's and then i formed my\n",
    "multivariate normal distribution\n",
    "by building up my mean vector mu\n",
    "evaluating my mean function in this case\n",
    "zero at all the input points that i've\n",
    "chosen\n",
    "and forming my covariance matrix by\n",
    "querying my kernel at all possible pairs\n",
    "of the inputs\n",
    "i then sampled from that multivariate\n",
    "normal distribution\n",
    "to get the black dots the random\n",
    "function values\n",
    "and then i sampled from exactly that\n",
    "same distribution two more times to get\n",
    "the purple curve and the green curve\n",
    "here i've just joined the dots together\n",
    "the gray shade again corresponds to our\n",
    "95\n",
    "credible set and the blue curve here\n",
    "corresponds to our prior mean function\n",
    "in this case zero for convenience\n",
    "in the right panel i've conditioned on\n",
    "some data denoted by crosses\n",
    "and i've sampled several posterior\n",
    "functions in black\n",
    "purple and green we also have the\n",
    "posterior mean\n",
    "function in solid blue which we can use\n",
    "to make point predictions\n",
    "and a 95 credible set in gray shape\n",
    "very importantly all of the statistical\n",
    "properties of these functions\n",
    "are controlled by our kernel\n",
    "and in this case we're using what's\n",
    "called an rbf kernel\n",
    "also known as a squared exponential or a\n",
    "gaussian kernel\n",
    "this kernel has the functional form a\n",
    "squared\n",
    "times the exponential of the negative\n",
    "euclidean distance\n",
    "between our two inputs x and x prime\n",
    "over two\n",
    "l squared our length scale\n",
    "it incorporates the very intuitive\n",
    "inductive bias the functions that are\n",
    "close together in the input space\n",
    "should be more correlated than functions\n",
    "that are far away in the input space\n",
    "for example airline passenger numbers in\n",
    "1951 and 1952\n",
    "should be more similar than airline\n",
    "passenger numbers in 1951 and 1960.\n",
    "so this is a very natural inductive bias\n",
    "the extent of the correlations is\n",
    "controlled by the length scale\n",
    "hyperparameter\n",
    "l if l is very small\n",
    "the function values become uncorrelated\n",
    "we just have a white noise process\n",
    "if l is very large then the function\n",
    "values become\n",
    "all very correlated\n",
    "a is the amplitude hyperparameter if a\n",
    "is large then the amplitude of these\n",
    "functions will be large\n",
    "in the top panel here we have\n",
    "visualization of the entries of our\n",
    "covariance matrix when we have 1d\n",
    "ordered inputs we can see that there's\n",
    "this strong diagonal band where we have\n",
    "highest covariance\n",
    "and then the covariances decrease away\n",
    "from that band as we increase distance\n",
    "in the input space\n",
    "so let's consider the distribution over\n",
    "functions that we get\n",
    "when we have particular settings of our\n",
    "length scale\n",
    "and our amplitude parameters l naught\n",
    "and a naught\n",
    "here we have our sample prior functions\n",
    "and our sample posterior functions\n",
    "pause the video for a moment and think\n",
    "about whether these sample functions\n",
    "look strange to you in some way\n",
    "and it might be a little bit subtle\n",
    "to me these functions look a bit too\n",
    "wiggly they're varying a little bit too\n",
    "quickly they're a bit too complex\n",
    "the mean shoots back down to the prior\n",
    "mean\n",
    "a bit too fast\n",
    "so to me that's that that seems like the\n",
    "length scale is too small\n",
    "now let's see what happens is we\n",
    "increase the length scale\n",
    "perhaps here we've increased the length\n",
    "scale a bit too much we see now that the\n",
    "functions look\n",
    "too slowly varying too simple they're\n",
    "over smoothing the data\n",
    "let's consider again the marginal\n",
    "likelihood that we considered\n",
    "in part one of this tutorial we can form\n",
    "the marginal likelihood\n",
    "in this case by integrating away our\n",
    "gaussian process\n",
    "p of f the marginal likelihood again\n",
    "is the probability that we would\n",
    "generate a data set if we were to\n",
    "sample from this distribution over\n",
    "functions\n",
    "now if we have a very small length scale\n",
    "we just have a white noise process\n",
    "so we're going to be able to generate\n",
    "all sorts of different data sets but\n",
    "they're all going to be pretty different\n",
    "from one another so if we keep sampling\n",
    "from that distribution over functions we\n",
    "won't see the same thing again\n",
    "if we have a very large length scale\n",
    "then this induced distribution over\n",
    "functions is pretty simple as we saw\n",
    "everything looks sort of like a straight\n",
    "line everything is is pretty similar\n",
    "and so we're not really generating that\n",
    "much many data sets with very much\n",
    "probability\n",
    "using that large length scale in red\n",
    "for a given data set the marginal\n",
    "likelihood will have an occam's razor\n",
    "property\n",
    "where it will automatically favor a\n",
    "model of appropriate complexity\n",
    "this is described very eloquently by\n",
    "david mackay\n",
    "both in his phd thesis and in his book\n",
    "on information theory it's also\n",
    "described\n",
    "in the gaussian process for machine\n",
    "learning book by rasmussen and williams\n",
    "if we optimize our marginal likelihood\n",
    "with respect to length scale\n",
    "we get the green fit here which\n",
    "intuitively has an appropriate level of\n",
    "complexity\n",
    "if we had to choose between one of these\n",
    "curves we'd probably choose the green\n",
    "curve\n",
    "now we opened this part of the tutorial\n",
    "by deriving\n",
    "covariance functions from classical\n",
    "weight space models\n",
    "and then we said well let's just use a\n",
    "gaussian process with an rbf kernel\n",
    "now let's derive where that rbf kernel\n",
    "comes from\n",
    "we'll again consider an inner product of\n",
    "weights\n",
    "and basis functions in this case not\n",
    "expressed using vector notation\n",
    "we'll put a gaussian distribution over\n",
    "our weights and\n",
    "we'll use as our basis functions\n",
    "gaussian bumps\n",
    "centered at points ci as drawn in this\n",
    "diagram\n",
    "we can grind through the algebra revert\n",
    "to the definition\n",
    "of our covariance function and get our\n",
    "covariance function in equation 55.\n",
    "now let's consider what happens as we\n",
    "let the number of basis functions\n",
    "j go to infinity\n",
    "we'll want these basis functions to\n",
    "cover the whole real line\n",
    "so we'll let cj the center of the j\n",
    "spaces function be\n",
    "log j and c1 the center of the first\n",
    "basis function\n",
    "be minus log j\n",
    "if these basis functions are equally\n",
    "distributed\n",
    "then the difference between the i plus\n",
    "first basis function and i face's\n",
    "function will be\n",
    "2 log j over j so we see that that goes\n",
    "to 0 as j\n",
    "goes to infinity which is why i used log\n",
    "j for the endpoints i could have used\n",
    "root j for instance but not not j\n",
    "and the expression for the kernel in\n",
    "equation 57\n",
    "becomes a riemann sum an integral\n",
    "with limits c naught and c infinity in\n",
    "this case minus infinity\n",
    "and infinity we can substitute in our\n",
    "expression\n",
    "for the basis functions and evaluate\n",
    "this analytic this integral\n",
    "in closed form and get something that's\n",
    "proportional to the rbf kernel\n",
    "now this is a very extraordinary result\n",
    "i'd like you to\n",
    "pause the video and just consider what\n",
    "we've done for a moment\n",
    "we've actually shown that by thinking\n",
    "from a function space perspective\n",
    "we can use a model with an infinite\n",
    "number of basis functions\n",
    "using a finite amount of computational\n",
    "resources\n",
    "so in part one of the talk i open with\n",
    "this question\n",
    "about how we might model airline\n",
    "passenger numbers and i said i'd like to\n",
    "use\n",
    "the ten thousandth order polynomial from\n",
    "the three choices that we were\n",
    "considering\n",
    "you might have wondered well carrying\n",
    "this argument to its limit are we even\n",
    "going to be able to store these models\n",
    "in memory assuming that we want to use\n",
    "them in fact what we're seeing here is\n",
    "we can use something like an infinite\n",
    "order polynomial we can use a model\n",
    "that is extraordinarily flexible in fact\n",
    "this is a universal approximator and we\n",
    "can see that by looking at this\n",
    "derivation if we collapse these basis\n",
    "functions onto point masses we can have\n",
    "densely dispersed point masses with\n",
    "different different heights\n",
    "but we've also seen that this model can\n",
    "generalize very well it has very\n",
    "intuitive inductive biases\n",
    "so we should be very careful not to\n",
    "conflate flexibility with the complexity\n",
    "of our model class\n",
    "and we should also be very careful not\n",
    "to do parameter\n",
    "counting as a proxy for complexity here\n",
    "we're seeing\n",
    "that we can actually use models with an\n",
    "infinite number of parameters\n",
    "that in some sense have fairly simple\n",
    "inductive biases\n",
    "and provide very good generalization\n",
    "in this diagram where we have all\n",
    "possible data sets on the horizontal\n",
    "axis\n",
    "we're saying we want to have heavy tails\n",
    "we want to support all sorts of\n",
    "different data sets\n",
    "but we also want to have mass in\n",
    "reasonable places\n",
    "and we can get that in this case through\n",
    "the inductive biases\n",
    "of the kernel function\n",
    "i'd like to add a brief note about the\n",
    "mean function\n",
    "in this tutorial so far and in many\n",
    "texts about gaussian processes\n",
    "the mean function m of x is often taken\n",
    "to be zero for notational convenience\n",
    "but in fact we can use any deterministic\n",
    "mean function without fundamentally\n",
    "changing\n",
    "the modeling procedure and it's usually\n",
    "pretty standard to subtract off the\n",
    "empirical mean and then use the\n",
    "zero mean gp or subtract some kind of\n",
    "deterministic mean function\n",
    "and then add it back later\n",
    "also typically the covariance function\n",
    "or the kernel is really the\n",
    "the key object of interest there are\n",
    "often degeneracies between specifying\n",
    "the mean\n",
    "and covariance function in which case\n",
    "often it's typically preferred therefore\n",
    "to do the modeling\n",
    "in the covariance function for example\n",
    "the kernel function shows up in\n",
    "the aqua factor term we get in the\n",
    "marginal likelihood a log determinant\n",
    "but not the mean function that said\n",
    "a mean function can be a great way of\n",
    "incorporating scientific\n",
    "inductive biases into a model there's\n",
    "sometimes a tension\n",
    "between using an interpretable\n",
    "scientifically motivated model for\n",
    "example a model with physically\n",
    "interpretable\n",
    "parameters and some black box function\n",
    "approximator\n",
    "in actuality this is often a false\n",
    "choice we can often use both in the\n",
    "sense\n",
    "we can use as our mean function that\n",
    "scientifically motivated\n",
    "model that parametric approach which has\n",
    "interpretable parameters\n",
    "and then we can at the same time use our\n",
    "gaussian process with an rbf kernel to\n",
    "allow for a non-parametric\n",
    "relaxation around that mean function to\n",
    "account for\n",
    "inevitable model misspecification\n",
    "so we can really have both we can we can\n",
    "use that mean function of whatever other\n",
    "model we would have used\n",
    "to calibrate the inductive biases of our\n",
    "approach and we can concentrate\n",
    "our distribution of functions as closely\n",
    "or\n",
    "as loosely as we like around that that\n",
    "scientific mean function\n",
    "interest in gaussian processes in the\n",
    "machine learning community\n",
    "was triggered by work that bradford neil\n",
    "was doing on bayesian neural nets\n",
    "he really embraces this idea that we\n",
    "should build very flexible models and\n",
    "accordingly\n",
    "was pursuing the limits of very large\n",
    "bayesian neural nets\n",
    "and he showed that as we we let the\n",
    "number of hidden units\n",
    "in a bayesian neural net go to infinity\n",
    "we get a gaussian process\n",
    "here let's consider the simple neural\n",
    "net f of x equals a bias\n",
    "plus a sum of a bunch of hidden units\n",
    "with hidden unit weights ui\n",
    "with fairly general assumptions one can\n",
    "show\n",
    "using a central limit theorem argument\n",
    "and not even you know requiring\n",
    "for example that the parameters have\n",
    "gaussian distributions\n",
    "that will get a gaussian process in the\n",
    "limit as j goes to infinity\n",
    "and we can derive the moments of the\n",
    "gaussian process just as we already have\n",
    "been doing in other examples\n",
    "we can make particular design choices\n",
    "like we can let our activation functions\n",
    "be error functions\n",
    "and we can use a gaussian prior over our\n",
    "hidden unit weights\n",
    "and derive a neural network kernel in\n",
    "equation 69.\n",
    "we have samples from a gp with this\n",
    "neural net kernel we can see that\n",
    "they're non-stationary that the\n",
    "at a high order the statistical\n",
    "properties in the center are different\n",
    "than\n",
    "in other regions unlike say the rbf\n",
    "kernel which is translation in variance\n",
    "it's a very flexible model it could look\n",
    "different in different parts of space\n",
    "but\n",
    "in a high order the statistical\n",
    "properties the functions in in those\n",
    "types of kernels will be similar in\n",
    "different regions of space\n",
    "so this result that an infinite neural\n",
    "net converts to a gaussian process\n",
    "was very exciting in the machine\n",
    "learning community and\n",
    "in some cases it drove the idea that\n",
    "we ought to just use gaussian processes\n",
    "instead of neural nets\n",
    "at the time people were becoming very\n",
    "frustrated by all sorts of design\n",
    "decisions associated with working with\n",
    "neural nets like\n",
    "how many hidden units do we want how\n",
    "many hidden layers\n",
    "what are the activation functions what\n",
    "optimization procedure are we using what\n",
    "what learning rate schedule are we using\n",
    "with that optimization procedure\n",
    "and the lack of a principle framework to\n",
    "answer these types of questions\n",
    "gaussian processes by contrast were very\n",
    "simple and interpretable\n",
    "and flexible and you could write the\n",
    "same 10 lines of code and get the same\n",
    "answer\n",
    "anywhere in the world\n",
    "now david mackay was a big supporter of\n",
    "gaussian process research but\n",
    "at the same time he was a bit of a\n",
    "contrarian he wrote this essay\n",
    "in an edited book on neural nets where\n",
    "he has this quote\n",
    "how can gaussian processes possibly\n",
    "replace neural nets have we thrown the\n",
    "baby out with the bathwater\n",
    "and what he meant was that when neural\n",
    "nets were being developed they were\n",
    "envisaged as becoming intelligent agents\n",
    "that could\n",
    "discover interesting hidden\n",
    "representations in data\n",
    "and while gaussian processes have all\n",
    "these nice statistical properties\n",
    "they're also basically just smoothing\n",
    "devices\n",
    "in which case are we throwing the baby\n",
    "out with the bath water and treating gps\n",
    "as\n",
    "replacements for neural nets\n",
    "the answer to this question is to build\n",
    "more expressive kernel functions\n",
    "which can discover interesting hidden\n",
    "representations in data\n",
    "although kernel methods and gaussian\n",
    "processes and neural nets are somehow\n",
    "treated as competing approaches they're\n",
    "often actually quite complementary\n",
    "what we get with the neural net is a way\n",
    "to create\n",
    "highly adaptive basis functions which\n",
    "have very good inductive biases for a\n",
    "number of different application domains\n",
    "such as image recognition\n",
    "what we get through a kernel method in a\n",
    "gaussian process\n",
    "is a way to have an infinite number of\n",
    "basis functions\n",
    "using a finite amount of computation\n",
    "we can combine both of these both of\n",
    "these properties together\n",
    "into approaches like deep kernel\n",
    "learning where we have a neural net\n",
    "transforming the inputs of a base kernel\n",
    "to create a deep kernel which is then\n",
    "jointly trained through the marginal\n",
    "likelihood of the gaussian process as\n",
    "one model\n",
    "so this gives us an infinite number of\n",
    "highly adaptive\n",
    "basis functions importantly this is\n",
    "quite different than what we would get\n",
    "if we were to just\n",
    "use a neural net as a feature extractor\n",
    "train the neural net\n",
    "and then apply a gp to the result\n",
    "rather deep kernel learning here is a\n",
    "one-step end-to-end procedure it's a\n",
    "single\n",
    "model and it's trained through the\n",
    "marginal likelihood objective\n",
    "we can use gaussian processes with deep\n",
    "kernels to do representation learning\n",
    "in this example here we have a gaussian\n",
    "process with a deep kernel applied to\n",
    "the all of eddie faces problem\n",
    "where we're considering faces with\n",
    "different orientation angles and we're\n",
    "trying to predict the orientation angle\n",
    "for new faces\n",
    "we can see projecting what the model\n",
    "learns into two dimensions\n",
    "that it discovers that faces with\n",
    "similar orientation angles\n",
    "here each face is given by a line\n",
    "segment and the orientation angle is\n",
    "given by the slope of that line segment\n",
    "are clustered together are similar in\n",
    "some way\n",
    "we can also see this by visualizing\n",
    "the learned covariance matrices that we\n",
    "get ordering the faces by orientation\n",
    "angle\n",
    "in the left two panels where we're using\n",
    "deep kernels we see a pronounced\n",
    "diagonal band\n",
    "which shows that the model learns that\n",
    "faces with similar orientation angles\n",
    "should be more correlated this is\n",
    "non-euclidean\n",
    "metric learning in a sense we can\n",
    "describe what a neural net\n",
    "is doing as learning a flexible\n",
    "non-euclidean\n",
    "similarity metric for the data in this\n",
    "sense\n",
    "training a neural net is a lot like\n",
    "learning a kernel\n",
    "in the far right panel we have the\n",
    "entries of the trained rbf kernel matrix\n",
    "which we see are quite diffuse\n",
    "euclidean distance is just not a good\n",
    "proxy for similarity in this application\n",
    "looking at euclidean distance of vectors\n",
    "of pixel intensities\n",
    "is not going to describe what we need to\n",
    "describe in order to solve this kind\n",
    "of representation learning problem very\n",
    "effectively\n",
    "in this example we're considering a\n",
    "discontinuous step function\n",
    "and we see that a gp with a deep kernel\n",
    "in green here the green shows the 95\n",
    "credible set describes that data very\n",
    "effectively\n",
    "a gp with a spectral mixture kernel a\n",
    "different type of kernel is shown in red\n",
    "it still fits the data all right but\n",
    "it's sort of over smooth\n",
    "we see a gp with an rbf kernel fit in\n",
    "blue\n",
    "from this perspective of a gp with an\n",
    "rbf kernel even though this is a\n",
    "flexible model\n",
    "um this kind of data is a very unlikely\n",
    "draw from the prior we would have to\n",
    "sample for a very very long time to see\n",
    "anything like the step function and so\n",
    "since we can learn the noise here the\n",
    "model is just saying well\n",
    "it's it's quite probable if we you know\n",
    "believe the gpu with the rbf kernel that\n",
    "this this data is basically just noise\n",
    "and will have very simple fit and a lot\n",
    "of\n",
    "uncertainty so this example shows that\n",
    "gaussian processes for example don't\n",
    "necessarily\n",
    "over smooth the data or have trouble\n",
    "fitting discontinuous data\n",
    "it totally depends on the kernel and\n",
    "with the deep kernel we can learn\n",
    "this kind of discontinuous step function\n",
    "data\n",
    "here we've applied a deep kernel\n",
    "structured with an lstm network\n",
    "to an autonomous driving application\n",
    "where decision making\n",
    "with predictive distributions is very\n",
    "important we don't want to just know\n",
    "where lane boundaries might be but we\n",
    "want to know you know\n",
    "error bars where these lean boundaries\n",
    "could be in making decisions\n",
    "and we can see in the bottom row here\n",
    "that\n",
    "the predictive distributions do a good\n",
    "job of capturing the ground\n",
    "truth and the point predictions are also\n",
    "better than the point predictions that\n",
    "what we get\n",
    "if we use the classic lstm neural net\n",
    "conventional conventionally gaussian\n",
    "processes have suffered from\n",
    "computational constraints we need to\n",
    "solve a linear system\n",
    "involving our n by n covariance matrix\n",
    "for n training points and also\n",
    "compute log determinants and derivatives\n",
    "of log determinants\n",
    "which incurs naively an n cube\n",
    "computational cost and\n",
    "exact gaussian processes have often been\n",
    "intractable for problems with more than\n",
    "a few thousand points\n",
    "recently by embracing advances in\n",
    "hardware krylov subspace methods have\n",
    "been developed that enable\n",
    "very scalable exact gaussian processes\n",
    "through\n",
    "gpu acceleration and parallelization\n",
    "and so in a sense deep learning has\n",
    "progressed not only through\n",
    "advances in methodological design but\n",
    "also\n",
    "by building algorithms that can really\n",
    "benefit\n",
    "from hardware acceleration by thinking\n",
    "about how to benefit from systems\n",
    "and gpi torch this package here\n",
    "really is inspired by that approach in\n",
    "order to scale gaussian processes\n",
    "with exact inference to very large\n",
    "problems\n",
    "and when we do that we can see that gps\n",
    "with deep kernels\n",
    "often outperform standalone deep neural\n",
    "nets\n",
    "also we can see that when we scale an\n",
    "exact gaussian process\n",
    "to large problems we can really realize\n",
    "the benefit of a non-parametric\n",
    "representation\n",
    "as we add more and more data points we\n",
    "can see that the error decreases\n",
    "very substantially with a non-parametric\n",
    "method that has an infinite number of\n",
    "basis functions\n",
    "the capacity of the model scales\n",
    "automatically with the amount of\n",
    "available information\n",
    "unlike a parametric model which is\n",
    "entirely determined by a finite number\n",
    "of parameters\n",
    "many approximate scalable inference\n",
    "techniques for gaussian processes\n",
    "introduce low rank kernel matrices so\n",
    "this would this would apply to inducing\n",
    "point methods stochastic variational\n",
    "inducing point methods\n",
    "as well as random feature expansions\n",
    "which basically turns the model into a\n",
    "parametric method\n",
    "and we can see that on really large data\n",
    "sets the gap between the\n",
    "scalable exact gps and the approximate\n",
    "gps is especially\n",
    "large there are several other\n",
    "popular gaussian process libraries such\n",
    "as gp flow\n",
    "g pi bow torch for bayesian optimization\n",
    "that's\n",
    "black box optimization automatic ml\n",
    "hyper parameter tuning a b\n",
    "testing and these libraries have\n",
    "different algorithmic foundations and\n",
    "use cases\n",
    "there's also a lot of additional work on\n",
    "combining gaussian processes\n",
    "and neural nets in various different\n",
    "ways for example gaussian process\n",
    "regression networks and deep gaussian\n",
    "processes\n",
    "build hierarchical models replacing\n",
    "neurons in neural nets with gaussian\n",
    "processes\n",
    "several other recent works have extended\n",
    "bradford niels limits to\n",
    "multi-layer neural nets and other\n",
    "architectures like convolutional neural\n",
    "nets\n",
    "and those limits are also very related\n",
    "to neural tangent kernels which have\n",
    "been a very\n",
    "hot topic in deep learning recently\n",
    "where we take infinite\n",
    "neural net limits and derive kernels\n",
    "which have recently achieved fairly\n",
    "promising empirical\n",
    "results now most of these kernels from\n",
    "infinite neural net limits\n",
    "have fixed covariance structure and we\n",
    "described how training a neural net\n",
    "in many ways is like learning a kernel\n",
    "that's what's doing the representation\n",
    "learning\n",
    "and so bridging this gap and developing\n",
    "these infinite limits\n",
    "so that we can also do kernel learning i\n",
    "think is a very exciting direction for\n",
    "future work in this space\n",
    "now there are many ways to realize\n",
    "bayesian principles in deep learning in\n",
    "addition to the standard approach of\n",
    "margin line\n",
    "marginalizing distributions over\n",
    "parameters in a neural net\n",
    "in this part we've really been\n",
    "considering how we can use neural nets\n",
    "to provide inductive biases for gaussian\n",
    "processes\n",
    "for a bayesian non-parametric approach\n",
    "to deep learning\n",
    "in general combining principles of\n",
    "bayesian non-parametrics\n",
    "with deep learning is a very exciting\n",
    "area for future work\n",
    "that's the end of part two in part three\n",
    "we'll be considering practical methods\n",
    "for\n",
    "bayesian deep learning hi i'm andrew\n",
    "wilson at new york university\n",
    "and welcome back to the icml 2020\n",
    "tutorial on bayesian deep learning\n",
    "this is part three methods for practical\n",
    "bayesian deep learning\n",
    "in this part we'll be using several\n",
    "modern approaches for highly practical\n",
    "bayesian deep learning\n",
    "to exemplify many of the foundational\n",
    "concepts that we've introduced in other\n",
    "parts of the tutorial\n",
    "while i've long been interested in\n",
    "bayesian machine learning\n",
    "i ironically became quite interested in\n",
    "bayesian deep learning\n",
    "after listening to a talk about\n",
    "optimization the irony is that i might\n",
    "consider\n",
    "bayesian approaches as trying to avoid\n",
    "optimization at all costs\n",
    "rather than use one parameter use all\n",
    "the perimeters and weight them by their\n",
    "posterior probabilities\n",
    "the argument being made was that mini\n",
    "batch sgd\n",
    "would converge to flatter regions of the\n",
    "loss than\n",
    "full batch gradient methods and\n",
    "therefore provide better\n",
    "generalization in this figure\n",
    "the loss is on the vertical axis train\n",
    "in black\n",
    "chest and red and there's a\n",
    "conceptualization of parameters on the\n",
    "horizontal axis\n",
    "we can see that a flat solution has\n",
    "still reasonably good\n",
    "testing loss but sharp solution has\n",
    "pretty bad\n",
    "testing loss once there's this\n",
    "horizontal shift between\n",
    "train and test which happens because\n",
    "our model won't be entirely specified by\n",
    "a finite amount of data and in fact we\n",
    "would want the different\n",
    "optimal solutions to change as we query\n",
    "our loss on different sets of data even\n",
    "if they're from the same distribution\n",
    "at the same time the shape of the loss\n",
    "is unchanged because\n",
    "we imagine that the loss is still a\n",
    "reasonable proxy for generalization\n",
    "in actuality will change a bit but not\n",
    "necessarily that much\n",
    "now this was actually a really great\n",
    "argument for bayesian integration\n",
    "because\n",
    "in order for this to be true\n",
    "the flat regions of the loss would have\n",
    "to have parameters corresponding to\n",
    "very different functions which would\n",
    "provide complementary explanations of\n",
    "the data\n",
    "otherwise we could just contrive\n",
    "flatness for example is\n",
    "re-parametrization\n",
    "it would really mean that much and if\n",
    "that's the case\n",
    "that we have all of these low-loss\n",
    "parameter settings that are providing\n",
    "complementary explanations of the data\n",
    "that we really really want to do that\n",
    "integral we want to use all these\n",
    "parameters and it's kind of arbitrary in\n",
    "a sense to say\n",
    "let's just put everything on one\n",
    "hypothesis we want to integrate\n",
    "a flipped version of this curve\n",
    "and that's going to give a very\n",
    "different answer than just using one\n",
    "solution\n",
    "in particular recall that the bayesian\n",
    "model average here given by this\n",
    "integral\n",
    "says that we want to use a conditional\n",
    "predictive distribution given w\n",
    "weighted by the posterior distribution w\n",
    "given d\n",
    "and integrating over all possible values\n",
    "of w that's why it's called\n",
    "marginalization we're not\n",
    "depending on w anymore we've\n",
    "marginalized it out when we find this\n",
    "conditional predictive distribution the\n",
    "posterior w\n",
    "given the uh often the losses taken as\n",
    "the negative log posterior\n",
    "for neural nets is extremely complex\n",
    "containing many complementary solutions\n",
    "which is why bayesian leveraging is\n",
    "especially significant in deep learning\n",
    "you know more significant for neural\n",
    "nets than for most other model classes\n",
    "in order to come up with a really good\n",
    "approximation of this integral however\n",
    "we're going to\n",
    "really need to carefully understand the\n",
    "structure of the neural net lost\n",
    "landscapes\n",
    "towards that end we showed that if you\n",
    "retrained a neural network\n",
    "twice and uh uh uh\n",
    "got different architectures different\n",
    "solutions different basins of attraction\n",
    "you could actually find subspaces along\n",
    "which these different solutions were\n",
    "connected\n",
    "meaning that you could walk from one\n",
    "point to another\n",
    "along a curve in this subspace and the\n",
    "training loss wouldn't change very much\n",
    "as we traverse this curve and so this is\n",
    "a surprising result because if we take a\n",
    "direct linear path between\n",
    "the different uh solutions found by by\n",
    "training sgd with different\n",
    "initializations\n",
    "we often occur incredibly high loss at\n",
    "almost 100\n",
    "uh training error and so um\n",
    "you know in a sense the intuition is\n",
    "that the points were isolated but in\n",
    "fact there are subspaces along which\n",
    "they aren't and this actually means in a\n",
    "sense that\n",
    "we're not even finding local optima when\n",
    "we train sgd for these neural nets we're\n",
    "just finding these kind of basins of\n",
    "attraction and there's some directions\n",
    "along which\n",
    "they're extremely flat and we can walk\n",
    "from one solution\n",
    "to another and we can find these paths\n",
    "by minimizing the loss that we found\n",
    "used sort of for sgd training like cross\n",
    "entropy\n",
    "uniformly an expectation over the curve\n",
    "so this this corresponds to something\n",
    "like uh\n",
    "a line integral of cross entropy\n",
    "normalized by\n",
    "arc length so in this visualization we\n",
    "have a two two-dimensional slice\n",
    "through a very high dimensional loss\n",
    "surface and\n",
    "each point in this plane here\n",
    "corresponds to an affine composition of\n",
    "three high dimensional weight vectors\n",
    "and the\n",
    "height of the loss the color is is is is\n",
    "is\n",
    "is the value of the loss of that\n",
    "combination\n",
    "and so these visualizations are in\n",
    "collaboration with with\n",
    "javier idme lost landscape.com very cool\n",
    "website\n",
    "here we have several other\n",
    "visualizations of mode connectivity\n",
    "we can see even in the two-dimensional\n",
    "slices that the structure of the lost\n",
    "surface is extremely\n",
    "complex multimodal\n",
    "why would we choose just one of these\n",
    "points\n",
    "so the next question to ask is whether\n",
    "we can actually\n",
    "recycle geometric information in the sgd\n",
    "trajectory for scalable posterior\n",
    "approximations\n",
    "centered on flat regions of the loss so\n",
    "this mode connecting result\n",
    "is very significant in the sense that it\n",
    "empirically verifies\n",
    "this diagram that was used to motivate\n",
    "sort of flat optima\n",
    "it shows that in fact there are these\n",
    "extraordinarily flat regions\n",
    "of the lost surface and in fact as we'll\n",
    "see they contain\n",
    "a variety of different solutions we\n",
    "really want to do this integral\n",
    "and we want to see now how we can\n",
    "practically kind of learn something\n",
    "about the shape of the law surface just\n",
    "through something that resembles kind of\n",
    "basic sgd training\n",
    "and so we found that if we use the\n",
    "learning rate schedule that decayed to a\n",
    "relatively high constant learning rate\n",
    "and took an equal average of the weights\n",
    "traversed by sgd with\n",
    "that high constant learning rate we were\n",
    "spinning around\n",
    "a region of the lost surface that\n",
    "contained a bunch of different\n",
    "models which all had very low training\n",
    "loss and so it would normally be very\n",
    "hard to penetrate inside that region\n",
    "but when we're spinning around we can\n",
    "take an equal average of the iterates\n",
    "it's very important this is an equal\n",
    "average and a constant learning rate\n",
    "schedule\n",
    "we can we can move inside to a fairly\n",
    "centered region so this is\n",
    "and and that those those solutions in in\n",
    "this procedure called stochastic weight\n",
    "averaging often leads to much better\n",
    "generalization without a lot of\n",
    "additional\n",
    "computational overhead and so this is\n",
    "very you know importantly different than\n",
    "say polyacr\n",
    "rupert averaging where uh uh often we're\n",
    "we're using um exponential moving\n",
    "averages with decaying rates and\n",
    "the idea is to get better convergence\n",
    "say in convex optimization\n",
    "now in addition to taking the mean\n",
    "of these estimates we can also come with\n",
    "a low rank\n",
    "plus diagonal approximation to the\n",
    "second moment the covariance matrix\n",
    "and then we can use that to come up with\n",
    "a gaussian approximate posterior for our\n",
    "parameters\n",
    "and then we can use that and sample from\n",
    "it to get our simple monte carlo\n",
    "estimate\n",
    "of our predictive distribution and the\n",
    "whole method here\n",
    "uh called swag swa gaussian\n",
    "because it's a gaussian approximate\n",
    "posterior using\n",
    "ideas kind of motivated by swa\n",
    "uh uh can be written here just on you\n",
    "know just just just one\n",
    "one on this one slide and uh you know\n",
    "that's why we call it a simple baseline\n",
    "for bayesian uncertainty and deep\n",
    "learning\n",
    "now there's theory suggesting that if we\n",
    "run sgd with modified learning rate\n",
    "schedules asymptotically we're sampling\n",
    "from a gaussian distribution which is a\n",
    "reasonable description of the\n",
    "posterior however this theory has\n",
    "assumptions which are violated in deep\n",
    "learning\n",
    "so as a sanity check we also visualized\n",
    "the lost surface in the subspaces\n",
    "spanned by the principal components of\n",
    "the sgd\n",
    "iterates with these modified learning\n",
    "rate schedules and showed that\n",
    "the swag procedure came up with a pretty\n",
    "good posterior approximation of the loss\n",
    "surface in these subspaces\n",
    "here we're considering uh the\n",
    "calibration\n",
    "of various models so on the vertical\n",
    "axis of each of these plots\n",
    "we have confidence minus accuracy\n",
    "confidence corresponds to the highest\n",
    "softmax\n",
    "output in our comp net and accuracy is\n",
    "the accuracy of using the associated\n",
    "class label\n",
    "and so here a horizontal curve at zero\n",
    "corresponds to a perfectly calibrated\n",
    "model\n",
    "a curve above that horizontal curve\n",
    "corresponds to\n",
    "uh over confidence and below that curve\n",
    "corresponds to under confidence in the\n",
    "horizontal\n",
    "axis here we just have bins for\n",
    "different confidence levels\n",
    "so green is sgd training we see that\n",
    "it's almost always\n",
    "overconfident and in a way that isn't\n",
    "surprising because those models are\n",
    "ignoring epistemic uncertainties so\n",
    "they're\n",
    "you know in classical training we're\n",
    "betting everything on a single model\n",
    "whether or not it's regularized\n",
    "and um this means that we're ignoring\n",
    "all the other\n",
    "possible functions that are consistent\n",
    "with our observations so of course in a\n",
    "sense we're overconfident\n",
    "we try to account for epistemic\n",
    "uncertainty through model averaging that\n",
    "we\n",
    "get through swag for example then we see\n",
    "in many cases we end up with quite good\n",
    "calibration\n",
    "we're also considering a number of other\n",
    "approaches so swag here is in blue\n",
    "there's a dropout like approach in gold\n",
    "we have\n",
    "k factor class in in pink temperature\n",
    "scaling\n",
    "procedure in brown we can also notably\n",
    "see in the bottom row here we have\n",
    "imagenet results with huge neural nets\n",
    "like a dense net 161 or a resnet 152 and\n",
    "so this is a very scalable procedure it\n",
    "can be applied\n",
    "virtually wherever you would just do\n",
    "standard classical sgd training\n",
    "and in the top right we have a transfer\n",
    "learning problem which is\n",
    "important because a lot of non-bayesian\n",
    "alternatives like temperature scaling\n",
    "where we just scale\n",
    "our logits by a particular parameter um\n",
    "uh the the the logics just before we\n",
    "pass it pass through the softmax\n",
    "uh uh a special case of flat scaling\n",
    "um uh require you know a validation set\n",
    "that's really representative of the\n",
    "target distribution\n",
    "and uh that's hard if we have covariate\n",
    "shift and so\n",
    "capturing epistemic uncertainty is going\n",
    "to be especially robust in these kinds\n",
    "of settings\n",
    "compared to the alternatives but still\n",
    "we see all the different approaches here\n",
    "are\n",
    "are you know over confident some extent\n",
    "we can very naturally visualize\n",
    "epistemic uncertainty and regression\n",
    "problems so\n",
    "here we see that with the swag method as\n",
    "we move away from the data point\n",
    "where we're getting a wider predictive\n",
    "distribution and this is because there\n",
    "are\n",
    "many different types of curves that are\n",
    "consistent with our observations\n",
    "away from the data but not towards the\n",
    "data will become constrained\n",
    "and so that's really you know what\n",
    "epistemic uncertainty\n",
    "is trying to capture compared to say\n",
    "illeatoric uncertainty which would\n",
    "correspond in this case to noise in the\n",
    "measurements themselves\n",
    "on the other hand we see with the full\n",
    "space vi\n",
    "using standard variational approach for\n",
    "for\n",
    "approximate marginalization of neural\n",
    "net parameters we get a reasonably good\n",
    "mean fit point predictions but the\n",
    "uncertainty is pretty homogeneous and it\n",
    "doesn't grow that much as we move away\n",
    "from the data so it's not doing a great\n",
    "job of epidemic\n",
    "uncertainty representation so in short\n",
    "this\n",
    "swa gaussian swag procedure provides a\n",
    "simple and scalable method for bayesian\n",
    "deep learning\n",
    "ideas just fit the estee uterus to a low\n",
    "rank plus diagonal gaussian distribution\n",
    "and you know we can capture geometric\n",
    "properties\n",
    "of of the posterior uh in the subspace\n",
    "of sgd and we can we can\n",
    "improve predictions and uncertainty at\n",
    "the imagenet scale\n",
    "now in order to motivate this swag\n",
    "procedure\n",
    "we created a visualization of the\n",
    "subspace spanned by the principal\n",
    "components of the yes deuteros and this\n",
    "led to the\n",
    "question of whether it might make sense\n",
    "just to try to do bayesian\n",
    "marginalization in the very low\n",
    "dimensional subspace directly and so\n",
    "here we're saying let's construct a\n",
    "subspace of a network\n",
    "that has a very high dimensional\n",
    "parameter space perform inference in\n",
    "that subspace and then sample from the\n",
    "approximate posterior\n",
    "uh just using bayesian and then and then\n",
    "then use those samples for bayesian\n",
    "model averaging and what we found is\n",
    "that we could approximate the posterior\n",
    "of a wide resnet with\n",
    "36 million parameters in a 5d subspace\n",
    "and achieve state-of-the-art results so\n",
    "the contention here\n",
    "is that even though the parameter space\n",
    "is very high dimensional\n",
    "a lot of the functional variability\n",
    "could be captured in a very low\n",
    "dimensional subspace and\n",
    "once we've reduced that integral that\n",
    "we're trying to compute to\n",
    "a very low dimensional integral then\n",
    "it's going to be a lot easier to\n",
    "estimate\n",
    "so a lot of the challenges with bayesian\n",
    "neural nets\n",
    "are around the fact that we have to\n",
    "compute this very high dimensional\n",
    "integral typically\n",
    "so in particular in this approach uh\n",
    "what we did was we\n",
    "collected all the weights of the drill\n",
    "net and said they were equal to an\n",
    "offset like the swa solution\n",
    "plus a linear projection p of a very low\n",
    "dimensional vector z\n",
    "and then we did inference in z space so\n",
    "we could even use slice sampling if\n",
    "we're in a low\n",
    "enough dimensional space which is a\n",
    "really great mcmc method but it has a\n",
    "cursive dimensionality\n",
    "um and um then we can sort of use this\n",
    "equation to go back into w space and do\n",
    "the bayesian model average\n",
    "so let's first consider the mode\n",
    "connecting subspace so in the right\n",
    "panel here we're traversing through\n",
    "parameter space\n",
    "in this mode connecting subspace and in\n",
    "the left panel we're looking at the\n",
    "corresponding functions in purple\n",
    "and we can see first that there's a lot\n",
    "of functional variability\n",
    "in this very flat region of the\n",
    "posterior\n",
    "and if i were to ask you which\n",
    "curve you preferred you know this purple\n",
    "curve or that purple curse you probably\n",
    "have a hard time saying and this is why\n",
    "it's quite arbitrary just to bet\n",
    "everything on one solution we want to\n",
    "consider all of these solutions\n",
    "informing our predictive distribution\n",
    "when we do that we see we get\n",
    "good epistemic uncertainty\n",
    "representation\n",
    "we can see that the spread of the\n",
    "predictive distribution here showing the\n",
    "95\n",
    "credible set you know increases as we as\n",
    "we move away from the data\n",
    "decreases as we move towards the data\n",
    "here we consider two other subspaces the\n",
    "random subspace of the pca subspace\n",
    "random subspace is what we get when we\n",
    "just use\n",
    "uh samples from a gaussian distribution\n",
    "for for our projection matrix\n",
    "independence\n",
    "standard gaussian and we can see that\n",
    "the\n",
    "curve that here actually fits the data\n",
    "pretty\n",
    "well in terms of point predictions but\n",
    "the uncertainty is very homogeneous so\n",
    "we're not doing a good job of epidemic\n",
    "uncertainty representation\n",
    "the bottom row here we see the the\n",
    "subspaces in parameter space so\n",
    "each point is sort of a different\n",
    "composition of\n",
    "parameters and the colors correspond to\n",
    "the values of\n",
    "the loss at those points in the middle\n",
    "column here we have the pca subspace\n",
    "which is what we get when that\n",
    "projection matrix is found\n",
    "by looking at the principal components\n",
    "of the sg iterates in particular as\n",
    "we're traversing the lost surface with\n",
    "this\n",
    "high constant learning rate schedule we\n",
    "can\n",
    "store a matrix which has its columns the\n",
    "weights that we're passing through\n",
    "minus the average of those weights then\n",
    "we can do a partial svd and that gives\n",
    "us our projection matrix it's super\n",
    "efficient it's just\n",
    "a single computation and uh uh\n",
    "you know uh uh once we've got that\n",
    "subspace we perform inference and we can\n",
    "see that we\n",
    "we get a pretty good predictive\n",
    "distribution here which\n",
    "is doing an intuitively good job of\n",
    "representing epidemic uncertainty\n",
    "now in part one i emphasize that because\n",
    "there's all this functional\n",
    "variability uh inside the lost surface\n",
    "of a neural net\n",
    "uh we're really going to benefit not\n",
    "just in terms of uncertainty\n",
    "representation which can be\n",
    "measured by say negative log likelihood\n",
    "but also in terms of just the accuracy\n",
    "of our point predictions\n",
    "and we can really realize that here with\n",
    "that subspace approach so here we have a\n",
    "resnet on c400 it's getting 78.5\n",
    "accuracy with classical sgd training\n",
    "it's getting\n",
    "80.17 with the random\n",
    "projection um and we're getting eighty\n",
    "point five percent then\n",
    "with the pca subspace and eighty one\n",
    "point two eight percent of the curve\n",
    "subspace so these are really\n",
    "non-trivial gains in accuracy and a pca\n",
    "subspace for example is\n",
    "not really much more expensive than just\n",
    "classical training\n",
    "and so we can really empirically\n",
    "empirically realize the benefits of\n",
    "bayesian marginalization\n",
    "in terms of not just uncertainty\n",
    "representation measured by an ll which\n",
    "we see is also increasing\n",
    "or decreasing but uh but also accuracy\n",
    "so historically the laplace\n",
    "approximation has been very\n",
    "important in performing inference with\n",
    "bayesian neural nets david mackay who\n",
    "did a lot of the\n",
    "the first work in this space um was\n",
    "considering\n",
    "laplace approximations and recently\n",
    "they've come back with\n",
    "chronic factorizations so the the basic\n",
    "idea\n",
    "is we approximate the posterior with a\n",
    "gaussian\n",
    "has its parameters theta it's it's mean\n",
    "mu and it's it's covariance matrix a\n",
    "inverse\n",
    "and uh these parameters are determined\n",
    "by a second-order taylor approximation\n",
    "around the log of normalized true\n",
    "posterior pw\n",
    "given d and we do that taylor expansion\n",
    "we see that we set\n",
    "the mean equal to the map setting of the\n",
    "parameter is what we get when we\n",
    "maximize the posterior with respect to w\n",
    "and we set a equal to the the\n",
    "negative hessian of the log posterior\n",
    "evaluated at w\n",
    "map so for a high dimensional parameter\n",
    "space\n",
    "this a matrix is way too large to store\n",
    "uh\n",
    "so you know if it's the w is 10 million\n",
    "dimensional\n",
    "typically a is taken to be diagonal like\n",
    "in david bukhai's work\n",
    "or recently you can you can it's been\n",
    "k's been expressed as a chronicler\n",
    "product of much smaller matrices which\n",
    "is more expressive\n",
    "than the diagonal approximation and\n",
    "leads to better results but it's still\n",
    "scalable\n",
    "and you know once we have our gaussian\n",
    "approximate posterior we can sample from\n",
    "it\n",
    "using a simple monte carlo estimate of\n",
    "our\n",
    "unconditional predictive distribution so\n",
    "overall the laplace approach is\n",
    "compelling because\n",
    "it's simple and can be relatively\n",
    "efficient\n",
    "perhaps as a drawback it's constrained\n",
    "to unimodal approximations\n",
    "uh uh it's like it's a gaussian\n",
    "approximate posterior and\n",
    "uh it's fairly local in its description\n",
    "of the loss you know even for gaussian\n",
    "so um\n",
    "the curvature is entirely different\n",
    "defined by the hessian evaluated at that\n",
    "map setting of parameters\n",
    "and so this means if for example there's\n",
    "a little kink\n",
    "in the the lost surface then it'll be\n",
    "very concentrated around a very small\n",
    "region and a\n",
    "very very compact representation of the\n",
    "posterior\n",
    "uh swag by contrast even though it\n",
    "provides a gaussian approximation\n",
    "it's more global because we have sgd\n",
    "with this fairly high constant learning\n",
    "rate bouncing around\n",
    "within some basement of attraction so if\n",
    "their little kings\n",
    "will still get a fairly kind of global\n",
    "gaussian approximation that won't be\n",
    "trapped in those those regions at least\n",
    "for that basis\n",
    "mc dropout is another in uh you know\n",
    "very popular approach\n",
    "uh for for bayesian neural nets and\n",
    "really catalyzed a lot of\n",
    "renewed interest in bayesian deep\n",
    "learning uh in 2016\n",
    "and so the idea is to run dropout during\n",
    "both train and test you randomly drop\n",
    "out each hidden unit\n",
    "probability r at each input and this\n",
    "creates you know a mask in regression\n",
    "each network can be trained to output\n",
    "both a mean mu and a variance sigma\n",
    "square by maximizing a gaussian\n",
    "likelihood\n",
    "we then create an equally weighted\n",
    "ensemble of the corresponding sub\n",
    "networks with a different dropout mass\n",
    "at test time\n",
    "so this is a you know very compelling\n",
    "approach it's\n",
    "very easy to apply it's very scalable\n",
    "and it's had great empirical results uh\n",
    "representing episode uncertainty a\n",
    "number of problems uh\n",
    "we know in the in this case the ensemble\n",
    "doesn't collapse as we get more data on\n",
    "like a standard bayesian model average\n",
    "so in part one we\n",
    "considered how bayesian model averaging\n",
    "isn't sort of a model combination\n",
    "in the sense that you know it it's just\n",
    "representing a statistical inability to\n",
    "distinguish between hypothesis given in\n",
    "limited\n",
    "information um whereas uh\n",
    "here uh you know we're still going to be\n",
    "sampling from this bernoulli\n",
    "distribution to get the dropout mass and\n",
    "so this isn't something that collapses\n",
    "as we get more data and you know\n",
    "figure out how to maybe modify it for\n",
    "that would be you know an interesting\n",
    "direction for future work\n",
    "so uh another historically important\n",
    "approach\n",
    "for bayesian deep learning is called\n",
    "bayes by back prop this is where we\n",
    "introduce\n",
    "typically gaussian approximate posterior\n",
    "for our parameters\n",
    "and then we learn those parameters using\n",
    "a variational method which minimizes the\n",
    "kl divergence between our approximate\n",
    "posterior and the true posterior\n",
    "even though the we can't integrate with\n",
    "respect to the true posterior we can\n",
    "usually write down\n",
    "the normalized true posterior exactly we\n",
    "just multiply our likelihood in our\n",
    "prior\n",
    "and we can manipulate the kale\n",
    "divergence again evidence lower bound\n",
    "which can be\n",
    "optimized with with sgd and back\n",
    "propagation\n",
    "so even though in principle we can\n",
    "choose sort of different distributions\n",
    "for q it is often a lot easier to use\n",
    "gaussian stochastic\n",
    "mcmc is a very up-and-coming\n",
    "direction for bayesian deep learning so\n",
    "rad for neil's mcmc methods in the mid\n",
    "90s were achieving state of the\n",
    "art results for all sorts of interesting\n",
    "interesting problems but these\n",
    "these standard hamiltonian monte carlo\n",
    "methods\n",
    "um wouldn't really scale to you know big\n",
    "architectures\n",
    "stochastic gradient longitudinal\n",
    "dynamics and stochastic radiant hmc for\n",
    "example\n",
    "on the other hand are very broadly\n",
    "applicable they uh\n",
    "algorithmically resemble sgd noisy sgd\n",
    "pretty closely uw here is a log\n",
    "posterior and so we can really apply\n",
    "these wherever we we apply std and often\n",
    "achieve\n",
    "better results through the bayesian\n",
    "model average\n",
    "now when we're um doing optimization\n",
    "you know even though we sort of explore\n",
    "the law surface to try to find a good\n",
    "solution in the end all we care about\n",
    "in principle is finding a solution with\n",
    "low loss\n",
    "whereas when we're sampling and we're\n",
    "doing bayesian model averaging more\n",
    "generally we really care about exploring\n",
    "this surface and so\n",
    "in this case the learning rate schedule\n",
    "is particularly crucial for good\n",
    "performance\n",
    "and it's recently been found that a\n",
    "cyclical learning rate schedule can\n",
    "really\n",
    "greatly enhance the ability for the\n",
    "stochastic mcmc procedure to explore\n",
    "complex multimodal loss surfaces and\n",
    "achieve you know much better\n",
    "practical performance so here we\n",
    "increase the learning rate and we're\n",
    "sort of exploring and then we decrease\n",
    "and we sort of\n",
    "explore within a mode and and you know\n",
    "use those samples\n",
    "deep ensembles has recently gained a lot\n",
    "of popularity and has achieved a lot of\n",
    "success\n",
    "the idea is to specify neural net\n",
    "architecture\n",
    "retrain the neural net a bunch of times\n",
    "to get a bunch of different sgd\n",
    "solutions typically starting from\n",
    "different initializations\n",
    "as we get a bunch of different weights\n",
    "uh and then\n",
    "you know again in regression each model\n",
    "can be specified to output a mean mu and\n",
    "a variance sigma squared transfer say\n",
    "gaussian likelihood\n",
    "and once we have all these different\n",
    "architectures corresponding the\n",
    "different weights that we found through\n",
    "sgd retraining we take an equally\n",
    "weighted\n",
    "ensemble\n",
    "so you might be wondering why is deep\n",
    "ensembles\n",
    "in a section about practical methods for\n",
    "bayesian neural nets\n",
    "are deep ensembles bayesian in fact\n",
    "aren't they often explicitly treated as\n",
    "a competing approach to bayesian methods\n",
    "in the next part part 4 will argue that\n",
    "deep ensembles in fact provide\n",
    "a better approximation to bayesian model\n",
    "averaging than many of the methods that\n",
    "we've described so far that\n",
    "provide single basin marginalization\n",
    "we'll also introduce an approach\n",
    "multi-swag which generalizes deep\n",
    "ensembles for\n",
    "even more accurate bayesian model\n",
    "averaging\n",
    "we'll note briefly now though that for\n",
    "example the average here\n",
    "unlike with mc dropout uh actually does\n",
    "collapse in the same way as a bayesian\n",
    "model average\n",
    "and so uh for example we're not\n",
    "enriching the hypothesis space we're\n",
    "just using\n",
    "uh the same architecture retrained a\n",
    "bunch of times with different maximum\n",
    "likelihood or map solutions so as the\n",
    "likelihood collapses so will the the\n",
    "bayesian model average so\n",
    "this ensemble also by\n",
    "looking at different basis of attraction\n",
    "we're capturing functional variability\n",
    "which intuitively is going to be very\n",
    "important for\n",
    "estimating this bayesian predictive\n",
    "distribution\n",
    "when we're trying to do marginalization\n",
    "we're trying to estimate that integral\n",
    "and\n",
    "given a finite amount of computation\n",
    "being able to\n",
    "uh spread where we're querying the lost\n",
    "surfaces\n",
    "across different basins of attraction is\n",
    "going to be\n",
    "very important for coming up with a good\n",
    "approximation to that that integral so\n",
    "in a sense it's\n",
    "it's doing very well at approximating\n",
    "the bayesian predictive distribution\n",
    "given a finite number of resources\n",
    "next we'll have part 4 on bayesian model\n",
    "construction\n",
    "and generalization hi i'm andrew wilson\n",
    "at new york university\n",
    "and welcome back to the icml 2020\n",
    "tutorial on bayesian deep learning\n",
    "this is part 4 on bayesian model\n",
    "construction and generalization\n",
    "in this part we'll consider deep\n",
    "ensembles and their connection with\n",
    "bayesian marginalization\n",
    "we'll use this connection to describe\n",
    "multi-swag a procedure which\n",
    "marginalizes within multiple basins of\n",
    "attraction\n",
    "we'll also consider tempering prior\n",
    "specification\n",
    "rethinking generalization double descent\n",
    "and with depth trade-offs\n",
    "here we'll return to a function space\n",
    "perspective that we considered in depth\n",
    "in part two\n",
    "we have a straight line function with a\n",
    "distribution over its parameters w\n",
    "naught and w1\n",
    "we can visualize this induced\n",
    "distribution over functions by sampling\n",
    "from this distribution over parameters\n",
    "and looking at the different straight\n",
    "lines we get the different slopes and\n",
    "intercepts\n",
    "the gray shade here corresponds to a 95\n",
    "credible set\n",
    "and the solid blue line corresponds to\n",
    "the expectation of this distribution\n",
    "over functions\n",
    "in this diagram here we have a\n",
    "conceptualization of all possible data\n",
    "sets on the horizontal axis\n",
    "and the marginal likelihood or evidence\n",
    "on the vertical axis\n",
    "expressed in equation 74. the marginal\n",
    "likelihood is the probability\n",
    "that we would generate a data set if we\n",
    "were to randomly sample from the\n",
    "parameters the distribution of over the\n",
    "parameters of our model\n",
    "we can see here in equation 74 the\n",
    "marginal likelihood is what we get when\n",
    "we integrate away these parameters\n",
    "in the example from the last slide we're\n",
    "just generating\n",
    "straight lines with different slopes and\n",
    "intercepts so we're not able to generate\n",
    "many data sets at all\n",
    "but because the marginal likelihood is a\n",
    "proper normalizable probability density\n",
    "it'll have to assign a lot of mass to\n",
    "those data sets\n",
    "alternatively we can consider a model\n",
    "like maybe a huge multi-layer perceptron\n",
    "with the broad\n",
    "distribution of its parameters which can\n",
    "generate lots of different data sets but\n",
    "each with not very much probability\n",
    "we could alternatively consider a model\n",
    "like a convolutional neural net which is\n",
    "very flexible\n",
    "so it can generate all sorts of\n",
    "different data sets but it has very\n",
    "particular inductive biases like\n",
    "translation equivalence this idea that\n",
    "if we translate an image its class label\n",
    "remains unchanged and so\n",
    "it will give a lot of mass to structured\n",
    "image data sets\n",
    "in order for a model to generalize it\n",
    "needs to both have large support it\n",
    "needs to be very flexible\n",
    "and it should have good inductive biases\n",
    "a good distribution of that support\n",
    "so models like convolutional nets can\n",
    "generalize very effectively on image\n",
    "recognition problems because it supports\n",
    "all sorts of different\n",
    "data sets in this application domain but\n",
    "it has a good distribution of support\n",
    "through biases like translation\n",
    "equivariance\n",
    "we should be very careful not to\n",
    "conflate flexibility and complexity as\n",
    "we are describing in part two of the\n",
    "talk we can have models like gaussian\n",
    "processes\n",
    "which are extraordinarily flexible even\n",
    "universal approximators but\n",
    "in a sense have very simple inductive\n",
    "biases and can generalize very well on\n",
    "problems with even a small number of\n",
    "data points\n",
    "in this figure here in addition to the\n",
    "panel that we considered on the last\n",
    "slide\n",
    "we look at what happens as these three\n",
    "models are exposed to\n",
    "a given data set we can see that the\n",
    "model with large support\n",
    "with but good inductive biases both\n",
    "contains the\n",
    "ground truth description and is able to\n",
    "contract efficiently around it\n",
    "the model with truncated support\n",
    "contracts quickly it's quickly\n",
    "constrained by the available data but\n",
    "erroneous\n",
    "it contracts around an erroneous\n",
    "solution the model with large support\n",
    "but poor inductive biases that spreads\n",
    "its support too thinly across too many\n",
    "data sets\n",
    "contains the truth but it doesn't\n",
    "contract very efficiently\n",
    "at the end of the last part part three\n",
    "we were considering deep ensembles\n",
    "and we started to hint at their\n",
    "potential connection with bayesian\n",
    "marginalization\n",
    "recall that the predictive distribution\n",
    "that we want to compute\n",
    "is this integral of our conditional\n",
    "predictive distribution given parameters\n",
    "weighted by the posterior probabilities\n",
    "of parameters given data\n",
    "and so we want to in words consider\n",
    "every possible setting of the parameters\n",
    "and weight them by their posterior\n",
    "probabilities\n",
    "now this bayesian approach and the\n",
    "classical approach will be\n",
    "similar when the posterior is very\n",
    "concentrated around a small number of\n",
    "parameters or\n",
    "when the conditional predictive\n",
    "distribution y given w\n",
    "is not varying a lot where\n",
    "the posterior distribution w given d has\n",
    "most of its mass\n",
    "now in this top panel here we have a\n",
    "conceptualization of the posterior\n",
    "distribution\n",
    "for a neural net multimodal lots of\n",
    "global optima\n",
    "and in the second row we have\n",
    "a conditional distribution now with\n",
    "with a deep neural net there isn't a lot\n",
    "of functional variability within a\n",
    "single basis of attraction\n",
    "compared to between different bases of\n",
    "attraction so we can see that\n",
    "conceptualized in this second row where\n",
    "we see that p of y given w\n",
    "doesn't change very much within a basin\n",
    "but it changes a lot between the basins\n",
    "in the bottom row here we're thinking\n",
    "about estimating this integral as an\n",
    "active learning problem\n",
    "we've observed a single solution say an\n",
    "sgd solution here\n",
    "in one of the basins and we're seeing\n",
    "where we need to move in weight space\n",
    "to to decrease the distance between\n",
    "our approximate predictive distribution\n",
    "and the exact predictive distribution\n",
    "the exact answer to this integral\n",
    "we can see that we would benefit a lot\n",
    "more by moving to a new basin\n",
    "then continuing to query points within\n",
    "the same basin\n",
    "and so indeed a lot of approaches\n",
    "to bayesian marginalization focus their\n",
    "efforts just on a single basis and they\n",
    "do gaussian approximate posteriors\n",
    "but in deep learning if we view this\n",
    "integration problem as really an active\n",
    "learning problem rather than through the\n",
    "lens of simple monte carlo\n",
    "then it's very advantageous given finite\n",
    "computational resources to do something\n",
    "like deep ensembles and just select\n",
    "points from different basins and we'll\n",
    "get\n",
    "a better approximation to this integral\n",
    "and in fact a more bayesian approach\n",
    "than the bayesian approaches which are\n",
    "marginalizing within a single basin\n",
    "so we're in practice in in you know real\n",
    "world we're always\n",
    "handed computational constraints and we\n",
    "try to do our best\n",
    "given those constraints and deep\n",
    "ensembles is actually a very good\n",
    "heuristic for you know achieving a good\n",
    "approximation to this integral given\n",
    "those constraints\n",
    "here we empirically test how close the\n",
    "dpon solves predictive distribution\n",
    "is to a near exact predictive\n",
    "distribution\n",
    "which we can see in panel a in panel b\n",
    "we have the deep ensembles we can see\n",
    "visually it's fairly similar to the\n",
    "exact distribution\n",
    "in panel c we have a variational\n",
    "procedure doing single basin\n",
    "marginalization with a gaussian\n",
    "approximate posterior we can see that it\n",
    "doesn't capture epistemic uncertainty so\n",
    "well it's\n",
    "it's doesn't have a lot of uncertainty\n",
    "between different\n",
    "uh clusters of data points and certainly\n",
    "doesn't grow very consistently away from\n",
    "the data\n",
    "in the bottom right panel we look at\n",
    "what happens is we\n",
    "in terms of the distance between the\n",
    "approximate predictive distribution\n",
    "given by both deep ensembles and\n",
    "stochastic variational inference as we\n",
    "increase the number of samples we have\n",
    "access to\n",
    "we can see that that distance decreases\n",
    "very quickly for deep ensembles\n",
    "but hardly at all as we increase the\n",
    "number of samples that we have at the\n",
    "variational procedure so\n",
    "continuing to see samples within a\n",
    "single basin with this variational\n",
    "approach\n",
    "is not really improving our estimate of\n",
    "the\n",
    "of the of the bayesian model average\n",
    "corresponding to this integral that we\n",
    "want to compute\n",
    "but training more models in the deep\n",
    "ensembles is pretty dramatically\n",
    "decreasing\n",
    "the distance between our approximation\n",
    "and the exact answer\n",
    "so in addition to just selecting\n",
    "different\n",
    "points in different basins we can also\n",
    "try to marginalize within basins of\n",
    "attraction and we do that in a procedure\n",
    "called multi-swag where we train\n",
    "multiple independent swag\n",
    "methods discussed in in part three uh to\n",
    "create a mixture of gaussian's\n",
    "approximation to the posterior\n",
    "so we're marginalizing within multiple\n",
    "basins\n",
    "when we do that and apply multi-swag to\n",
    "several of the applications in reference\n",
    "one\n",
    "evaluating predictive uncertainty under\n",
    "data set shift\n",
    "which show that deep ensembles often\n",
    "outperform single basin marginalization\n",
    "procedures\n",
    "we see two key trends\n",
    "one is that multi-swag tends to\n",
    "significantly outperform deep ensembles\n",
    "in cases where we're not training\n",
    "very many independent models or when\n",
    "there's a lot of data corruption\n",
    "in purple here we have an additional\n",
    "model multi-swa which takes the\n",
    "means of the different gaussian\n",
    "components in multi-swag and that\n",
    "those will be in flatter regions of\n",
    "those different basins of attraction\n",
    "than\n",
    "uh than say sgd solutions and so we see\n",
    "uh you know a nice compromise here\n",
    "recently a phenomenon called double\n",
    "descent\n",
    "has been of great interest a belkan at\n",
    "all a couple years ago\n",
    "showed that we could find these\n",
    "so-called double descent curves on a\n",
    "wide range of problems\n",
    "where uh we have uh our loss on the\n",
    "vertical axis\n",
    "and we can see as we increase model\n",
    "flexibility\n",
    "initially loss appears to be decreasing\n",
    "and then it\n",
    "increases corresponding to overfitting\n",
    "and then\n",
    "it turns out it decreases again and so\n",
    "this first regime is called the\n",
    "classical regime which is in line with\n",
    "classical intuitions about\n",
    "generalization and the next regime is\n",
    "referred to as the modern interpolating\n",
    "regime so the training loss here just\n",
    "you know keeps going down the test loss\n",
    "has this really non-monotonic\n",
    "behavior and the question is well why\n",
    "don't we just keep overfitting why did\n",
    "it what is suddenly generalization start\n",
    "getting\n",
    "better especially once we move past\n",
    "where the training loss is zero\n",
    "so i'd like to ask you and feel free to\n",
    "pause the video for a moment\n",
    "whether you think a bayesian model\n",
    "should experience double descent given\n",
    "what we've discussed so far and\n",
    "considering this problem\n",
    "especially think back to that first\n",
    "example at the beginning of the talk\n",
    "where we're considering airline\n",
    "passenger numbers and which model we\n",
    "would want to fit that data with\n",
    "using a bayesian approach\n",
    "so in that airline passenger number\n",
    "examples i was saying that if we\n",
    "have you know a reasonable prior and\n",
    "we're doing exhaustive marginalization\n",
    "then\n",
    "we would expect a bayesian method to\n",
    "improve\n",
    "monotonically with increases in\n",
    "flexibility we should embrace\n",
    "flexibility of large support\n",
    "and indeed that is actually what we see\n",
    "if we apply this multi-swag procedure we\n",
    "see that performance is essentially\n",
    "monotonic\n",
    "whereas sgd in this example where we\n",
    "have c4 100 20\n",
    "label corruption uh has a prominent\n",
    "double descent\n",
    "behavior uh swag where we're doing\n",
    "single basin marginalization has a less\n",
    "prominent double descent but you know\n",
    "it's still\n",
    "clearly there another really important\n",
    "feature to note in this plot\n",
    "is the fact that the multi-swag is\n",
    "actually just\n",
    "a lot more accurate than classical\n",
    "training so\n",
    "if we have uh you know a resnet uh with\n",
    "layers of width 20\n",
    "here we see just below 30\n",
    "test error on c400 whereas if we're\n",
    "using sgd we see about 45 percent tester\n",
    "so it's a really massive discrepancy and\n",
    "you know an empirical realization\n",
    "this idea that if we're doing multimodal\n",
    "marginalization\n",
    "with a neural net which can capture all\n",
    "sorts of complementary explanations to\n",
    "the data\n",
    "then we're really going to benefit a lot\n",
    "empirically not just in terms of\n",
    "uncertainty representation but also in\n",
    "terms of accuracy\n",
    "now we'll consider the prior and\n",
    "function space that's induced by having\n",
    "a gaussian prior in weight space where\n",
    "we're just\n",
    "varying the the the signal variance of\n",
    "the prior for each of the parameters\n",
    "alpha squared\n",
    "and so there are two key results\n",
    "that suggest that this actually induces\n",
    "a pretty reasonable prior and function\n",
    "space\n",
    "first is the deep image prior which\n",
    "showed that randomly initialized\n",
    "confidence without training provide\n",
    "excellent performance for image\n",
    "denoising super resolution and in\n",
    "painting\n",
    "in other words a sample function from\n",
    "this induced distribution over functions\n",
    "captures low level image statistics\n",
    "before any training another result\n",
    "from the paper by zhang adol on\n",
    "rethinking generalization\n",
    "uh shows that if we pre-process c410\n",
    "with a randomly initialized untrained\n",
    "cnn\n",
    "we get dramatically improved test\n",
    "performance when we're\n",
    "using a gaussian kernel directly on\n",
    "pixels so it goes from 54\n",
    "accuracy to 71 accuracy and then we just\n",
    "get another two percent from l2\n",
    "regularization which would be like\n",
    "tuning this alpha perimeter\n",
    "and so this is really saying that these\n",
    "this induced distribution over functions\n",
    "is fairly reasonable and\n",
    "and in a way it's not surprising because\n",
    "the properties of the induced\n",
    "distribution over functions comes\n",
    "largely from the functional form of the\n",
    "model that's really a big\n",
    "part of the prior translation\n",
    "equivariance in sort of a similarity\n",
    "metric that we get for\n",
    "for different images etc you know this\n",
    "is this is going to come from mostly the\n",
    "functional form of the model\n",
    "now um in bayesian deep learning it's\n",
    "typical to consider what's called a\n",
    "tempered posterior where we raise the\n",
    "likelihood to a power one over t\n",
    "where t is a temperature parameter t\n",
    "less than one corresponds to a cold\n",
    "posterior where the posterior is\n",
    "more concentrated around solutions with\n",
    "high likelihood t equals one corresponds\n",
    "to the standard\n",
    "bayesian posterior t is greater than one\n",
    "corresponds to warm posteriors\n",
    "where the prior effect is stronger and\n",
    "posterior collapse is slower\n",
    "in a paper at this icml by wenzel it all\n",
    "uh\n",
    "there's a result that's highlighted that\n",
    "the the that with a standard\n",
    "gaussian prior over parameters cold\n",
    "posteriors often provide\n",
    "improved performance but if we use just\n",
    "a temperature equals one we can\n",
    "we can get even worse performance than\n",
    "classical training\n",
    "in this paper they suggest the result is\n",
    "due to prior misspecification and show\n",
    "here that\n",
    "sample functions uh from this induced\n",
    "distribution or functions with the\n",
    "standard normal prior seem to assign one\n",
    "label to most classes on c410 and we\n",
    "know the classes are balanced\n",
    "so here we just sample from the standard\n",
    "normal get\n",
    "our function uh with our neural nets and\n",
    "we see that it's giving most of the data\n",
    "points class six here and another\n",
    "sample function is giving most of the\n",
    "data points class 9.\n",
    "we examine this behavior as we vary the\n",
    "scale\n",
    "of the prior variance on the parameters\n",
    "so for with for alpha equals root 10\n",
    "we reproduce this results and we see\n",
    "that one sample function is assigning\n",
    "most of the data one class\n",
    "another sample function was the date of\n",
    "the other class however if we reduce\n",
    "alpha we see that the samples are very\n",
    "quickly\n",
    "assigning about the same amount of data\n",
    "at each of the classes\n",
    "and in fact of course in practice we\n",
    "would specify alpha through cross\n",
    "validation or through\n",
    "uh for example just you know what we\n",
    "normally use for l2 regularization which\n",
    "is roughly near 0.1 in this case\n",
    "we also see that even for the\n",
    "misspecified alpha the unconditional\n",
    "predictive distribution\n",
    "is actually quite reasonable it's\n",
    "basically you know uniform over the\n",
    "different\n",
    "classes so it doesn't affect the\n",
    "predictions\n",
    "we can also examine the effect of data\n",
    "on the posterior here so\n",
    "um here we start with the prior the\n",
    "misspecified prior that this you know\n",
    "where sample functions are assigning\n",
    "most of the data one class\n",
    "we then observe just ten data points\n",
    "with this huge resnet and we see that\n",
    "already the predictive just that the\n",
    "samples are giving almost uniform\n",
    "predictions across the\n",
    "the different different points 100 data\n",
    "points you know even closer to uniform\n",
    "and so this is a prior bias that is very\n",
    "quickly modulated by data we can imagine\n",
    "with the gaussian process for instance\n",
    "if we\n",
    "if we multiplied our kernel by some\n",
    "factor uh\n",
    "the the amplitude would be awkward if we\n",
    "observe um\n",
    "you know a few just few data points that\n",
    "the posterior would\n",
    "quickly collapse in that region so this\n",
    "is the kind of\n",
    "prior bias which isn't really going to\n",
    "affect generalization that much in\n",
    "practice what's much more important for\n",
    "example is the induced covariance\n",
    "function that we get\n",
    "in this distribution over functions and\n",
    "we saw this a bit in part two where\n",
    "things like covariance function really\n",
    "affected generalization\n",
    "so here we have the induced correlation\n",
    "function for\n",
    "uh mnis digits different classes\n",
    "um we can see that the correlations\n",
    "generally decrease as we increase\n",
    "alpha we also see that um pretty\n",
    "consistently\n",
    "uh the same classes the same classes are\n",
    "most correlated and classes that are\n",
    "visually similar more correlated than\n",
    "other classes\n",
    "this is a good sign this is also\n",
    "suggestive in addition to the\n",
    "the the deep image prior and the random\n",
    "network features that\n",
    "reducing a prior that is actually pretty\n",
    "reasonable and you know which covariance\n",
    "function for example you use\n",
    "in a gaussian process is going to affect\n",
    "generalization much much more\n",
    "than you know whether you have a signal\n",
    "variance perimeter that's a bit\n",
    "misspecified\n",
    "so some thoughts on tempering um i i\n",
    "think it would be very surprising if t\n",
    "equals one just happened to be the best\n",
    "setting of this hyper parameter\n",
    "um you know i in fact i think you know\n",
    "we should we should always be doing\n",
    "tempering it's basically just an\n",
    "acknowledgement that our model is\n",
    "mis-specified\n",
    "that's not to say that we should you\n",
    "know just say well\n",
    "who cares about the model specification\n",
    "will correct it with tempering we should\n",
    "do our very best\n",
    "to specify our prior as as honestly as\n",
    "we can\n",
    "but it's still going to be misspecified\n",
    "and we should be honest about that too\n",
    "and try to correct it by by learning the\n",
    "temperature\n",
    "and in fact this isn't really too\n",
    "different than learning other properties\n",
    "of\n",
    "likelihood like noise\n",
    "now while the the prior p of f x is\n",
    "certainly\n",
    "misspecified um in in\n",
    "you know most cases in bayesian deep\n",
    "learning the result of assigning one\n",
    "class to most data is\n",
    "is really a soft prior bias which one\n",
    "doesn't hurt predict the predictive\n",
    "distribution two\n",
    "is easily corrected by appropriately\n",
    "setting the prior parameter variance\n",
    "alpha squared and three is quickly\n",
    "modulated\n",
    "by data what's much more important is\n",
    "this induced covariance function over\n",
    "images\n",
    "uh in addition to not tuning alpha\n",
    "in this whole posteriors paper um the\n",
    "results\n",
    "could have been exacerbated due to lack\n",
    "of multimodal marginalization which\n",
    "we've shown is extremely important for\n",
    "generalization\n",
    "it's also interesting to note that there\n",
    "are cases when a cold\n",
    "posterior a t less than one will be\n",
    "helpful\n",
    "in coming up with estimates even if we\n",
    "believe the prior and the likelihood\n",
    "if we have you know access to a finite\n",
    "number of samples so\n",
    "we can imagine for instance estimating\n",
    "the mean of a standard normal\n",
    "distribution\n",
    "in high dimensions in this case the\n",
    "samples will be very concentrated\n",
    "around a norm of root d and so uh\n",
    "if we decrease the temperature we'll\n",
    "come up with a better estimate for the\n",
    "mean\n",
    "and so i encourage you to try this i\n",
    "just sample from a high t\n",
    "distribution look at a histogram of the\n",
    "norms\n",
    "so there was a paper a few years ago uh\n",
    "called understanding deep learning\n",
    "requires rethinking generalization they\n",
    "showed that\n",
    "confnets could fit cfar with random\n",
    "labels and this was presented as if it\n",
    "were you know in the face of everything\n",
    "we know about generalization because\n",
    "um uh uh it showed that the confidence\n",
    "could\n",
    "fit in noise they could greatly overfit\n",
    "yeah they were generalizing in all sorts\n",
    "of different\n",
    "problems we can understand this result\n",
    "from\n",
    "the probabilistic functions based\n",
    "perspectives so\n",
    "in the top row here we have samples from\n",
    "a gaussian process\n",
    "in in panel a and panel b we see\n",
    "some structured data and a gaussian\n",
    "process predictive distribution which\n",
    "looks reasonable\n",
    "in panel c we see a lot of corrupted\n",
    "data in red and we see an updated gp\n",
    "predictive distribution which achieves\n",
    "zero training error\n",
    "now this red curve unlike the green\n",
    "curve in panel b\n",
    "looks nothing like the sample prior\n",
    "functions however\n",
    "the gp with the rbf kernel is very\n",
    "flexible it contains somewhere in its\n",
    "support\n",
    "this red curve even though we might have\n",
    "to sit there sampling for a very very\n",
    "long time to see anything\n",
    "like it however if there's a strong\n",
    "enough likelihood signal we're saying\n",
    "there's no noise then\n",
    "it's going to run through that data\n",
    "perfectly and produce this predictive\n",
    "distribution\n",
    "and so it doesn't want to fit that data\n",
    "but it can\n",
    "and we can quantify how much a model\n",
    "wants to fit data with something called\n",
    "the marginal likelihood\n",
    "and so we can actually fit a gaussian\n",
    "process\n",
    "on a c410 with with altered labels and\n",
    "show that we get zero percent training\n",
    "error\n",
    "so that means that you know this result\n",
    "in the rethinking generalization paper\n",
    "wasn't unique to deep neural nets you\n",
    "can reproduce it with gaussian processes\n",
    "and we can also compute the approximate\n",
    "marginal likelihood and show that it\n",
    "gets\n",
    "very very low as we increase the number\n",
    "of altered labels so that noisy c4 set\n",
    "is somewhere in the tails of that\n",
    "distribution where we're considering all\n",
    "possible data sets and the marginal\n",
    "likelihood of the vertical axis when\n",
    "we're considering this\n",
    "gp model with an rbf kernel uh the\n",
    "uh we can also compute exactly the same\n",
    "thing using a bayesian neural net\n",
    "and find the approximate marginal\n",
    "likelihood using whole class and we see\n",
    "that it decreases\n",
    "in in the same way as we increase the\n",
    "number of altered labels\n",
    "so this bayesian neural net is able to\n",
    "fit that kind of data set but it really\n",
    "doesn't want to\n",
    "and we can quantify that with the\n",
    "merchant likelihood and this is sort of\n",
    "in accordance with\n",
    "what we would want when we're thinking\n",
    "about model construction we want\n",
    "large support but we want to distribute\n",
    "that support carefully and that means\n",
    "not giving a lot of mass to things like\n",
    "noise you see far\n",
    "but if we see noisy c4 there's a strong\n",
    "likelihood signal and we can fit it\n",
    "okay so of course having said all that\n",
    "um\n",
    "the the priors in function space aren't\n",
    "going to be perfect by just having sort\n",
    "of\n",
    "generic dowsing distributions or\n",
    "parameters combine the functional forms\n",
    "of\n",
    "of neural nets um and we can certainly\n",
    "do better and i i certainly embrace the\n",
    "function space perspective in\n",
    "constructing\n",
    "compelling priors for these neural nets\n",
    "at the same time we should be careful\n",
    "not to contrive priors over parameters w\n",
    "to induce distributions over functions\n",
    "that resemble familiar models such as\n",
    "gaussian processes with rbf kernels\n",
    "we could be throwing the baby out with\n",
    "the bath water and doing that\n",
    "indeed drill nets are useful as their\n",
    "own model class precisely because\n",
    "they have different inductive biases\n",
    "from other models we already have\n",
    "gaussian processes with\n",
    "rbf kernels and we can try to gain\n",
    "insights in thinking in function space\n",
    "but this is in a sense what we're\n",
    "already doing when we're doing\n",
    "architecture design that thinking and\n",
    "function space trying to encode\n",
    "properties such as translation\n",
    "equivariance rotation equivariance\n",
    "color and scale invariants and other\n",
    "interesting properties that would\n",
    "induce a compelling similarity metric\n",
    "across our data instances\n",
    "and so these properties really imbue the\n",
    "associated distribution of our functions\n",
    "with desirable properties for\n",
    "generalization\n",
    "and really all of the heavy lifting\n",
    "for the the prior functions that we get\n",
    "is is is from the the architecture and\n",
    "um you know even if we're thinking\n",
    "broadly\n",
    "uh the functional form of a model is\n",
    "like a strong prior\n",
    "um it's a very strong assumption we\n",
    "can't escape assumptions and we\n",
    "shouldn't try to we should embrace\n",
    "assumptions they're\n",
    "you know we need to make assumptions to\n",
    "to uh\n",
    "have good generalization pack bays has\n",
    "been a very exciting approach for\n",
    "deriving explicit generalization error\n",
    "bounds and stochastic networks with\n",
    "posterior's q prior p\n",
    "training points in uh uh the the pac\n",
    "bay's generalization error bounds are\n",
    "are based on this this term and equation\n",
    "79\n",
    "and uh we've uh recently uh uh you know\n",
    "people have\n",
    "have derived non-vacuous bounds\n",
    "exploiting flatness in\n",
    "queue with at least 80 generalization\n",
    "accuracy\n",
    "predicted on binary mnist uh this is you\n",
    "know very exciting\n",
    "um it's it's a promising framework uh\n",
    "but it tends not to be prescriptive\n",
    "about model construction\n",
    "or informative for for understanding why\n",
    "a model generalizes so in a sense it's\n",
    "very complementary to what we've been\n",
    "describing so far\n",
    "in this tutorial and in fact if we were\n",
    "to try to treat it as a prescription\n",
    "what we would get is this sort of almost\n",
    "contrary to a lot of what we've\n",
    "discussed the bounds are improved by\n",
    "compact priors for example\n",
    "um and we've been saying well we want\n",
    "prioritize with support for all sorts of\n",
    "different data sets\n",
    "um and a low dimensional parameter space\n",
    "in a sense that the kl divergence is\n",
    "going to be\n",
    "uh often sort of hard to assign a lot of\n",
    "overlap between two high dimensional\n",
    "distributions and so we can\n",
    "very often achieve better bounds by\n",
    "doing model compression etc\n",
    "um but here we've really been trying to\n",
    "embrace having as many parameters as\n",
    "possible for good generalization\n",
    "um also generalization as we've shown\n",
    "can be significantly improved by\n",
    "a multi-modal posterior especially in\n",
    "bayesian deep learning\n",
    "uh this is you know really uh one of the\n",
    "key practical takeaways\n",
    "and in order to realize a good\n",
    "approximation to the\n",
    "the unconditional predictive\n",
    "distribution we really need to do\n",
    "multimodal marginalization\n",
    "but the pac-based generalization bounds\n",
    "aren't typically too influenced by\n",
    "multimodal posteriors you end up with a\n",
    "log factor it doesn't really change\n",
    "things that much so\n",
    "um that the pack based bounds are\n",
    "are quite quite complementary and\n",
    "they're very exciting in the sense that\n",
    "they\n",
    "they help us make sort of quantitative\n",
    "sort of\n",
    "explicit statements about generalization\n",
    "error bounds\n",
    "um uh but they don't tend to be too\n",
    "prescriptive and this could be partly\n",
    "also because although we in some cases\n",
    "get non-vacuous bounds they're also\n",
    "typically fairly loose\n",
    "so what improves the bounds won't\n",
    "necessarily improve generalization\n",
    "we can relate posterior contraction to a\n",
    "quantity called\n",
    "effective dimension and actually gain\n",
    "insights into\n",
    "into behavior such as double descents so\n",
    "uh\n",
    "effective dimension is of the hessian\n",
    "here\n",
    "is the sum of lambda i over lambda i\n",
    "plus alpha where alpha is a\n",
    "regularization parameter lambda i are\n",
    "the eigenvalues of the hessian when we\n",
    "compute this quantity\n",
    "of networks resnets of varying width\n",
    "here we can see that it\n",
    "tracks a test loss very closely as well\n",
    "as you know test error\n",
    "and um in fact the effective dimension\n",
    "is going down\n",
    "as we increase the size of the models\n",
    "and this suggests that actually we're\n",
    "getting simpler models even though they\n",
    "have\n",
    "more parameters we should be very\n",
    "careful not to treat parameter counting\n",
    "as a proxy for model\n",
    "complexity and we can relate effective\n",
    "dimension to\n",
    "model compression uh in a sense this\n",
    "sort of uh\n",
    "counts the number of kind of sharp\n",
    "directions uh in this this this\n",
    "space given by the eigenvectors of the\n",
    "hessian and um\n",
    "we can see that that in this regime we\n",
    "have basically zero training loss all\n",
    "the different models are providing\n",
    "lossless compressions of the data\n",
    "and so the best compression will capture\n",
    "the most regularity it'll have this\n",
    "occam's razor property and tend to\n",
    "provide the best generalization\n",
    "now the reason sgd finds uh these these\n",
    "kinds of solutions when we make the\n",
    "the model size bigger is is largely\n",
    "because\n",
    "um you know flat region of the loss will\n",
    "occupy a much greater volume in high\n",
    "dimensions and so sgv will\n",
    "be able to find them more easily we also\n",
    "look at with depth trade-offs so looking\n",
    "at\n",
    "confidence of different widths and\n",
    "depths and we can see above the green\n",
    "partition where we have zero training\n",
    "loss\n",
    "effective dimension is a very good proxy\n",
    "for generalization performance\n",
    "the yellow curves here show level\n",
    "cursives constant numbers of parameters\n",
    "um and uh\n",
    "you know what's interesting here to\n",
    "especially to consider\n",
    "different depths uh typically in in\n",
    "recent years we've been looking at\n",
    "infinite\n",
    "widths of neural nets for neural tangent\n",
    "kernels and things like this\n",
    "um but in a sense you know depth is what\n",
    "what gives\n",
    "deep learning a lot of useful\n",
    "hierarchical inductive biases that\n",
    "provides good\n",
    "generalization and representation\n",
    "learning and so we can see that\n",
    "this this quantity associated with\n",
    "posterior contraction based in deep\n",
    "learning\n",
    "uh is is a reasonably good proxy for\n",
    "generalization\n",
    "um we can also look at the properties in\n",
    "function space\n",
    "as we move in directions given by\n",
    "eigenvectors of the hessian with the\n",
    "smallest eigenvalues and show that\n",
    "the decision boundaries are essentially\n",
    "unchanged that we get a lot of\n",
    "functional homogeneity and this provides\n",
    "a mechanism for why things like subspace\n",
    "inference discussed in part\n",
    "three of the talk uh work so well why we\n",
    "can have a\n",
    "model with tens of millions of\n",
    "parameters and then basically just do\n",
    "marginalization in a five dimensional\n",
    "subspace and you know\n",
    "see a big difference for that um and so\n",
    "uh we can really understand you know\n",
    "things like model compression by looking\n",
    "at these quantities\n",
    "in conclusion what we've been really\n",
    "reiterating through this tutorial is\n",
    "that the key defining feature of\n",
    "bayesian methods is marginalization aka\n",
    "bayesian model averaging and that's\n",
    "going to be especially relevant in deep\n",
    "learning because neural nets are very\n",
    "underspecified by the data contain\n",
    "all sorts of complementary and exciting\n",
    "solutions to a given problem\n",
    "and it really makes a lot of sense just\n",
    "to use the sum and product rules of\n",
    "probability to marginalize those\n",
    "solutions\n",
    "and really in trying to do this\n",
    "marginalization as best as we can\n",
    "we shouldn't think of the integration\n",
    "purely through the lens of simple monte\n",
    "carlo integration\n",
    "um we should probably think of it more\n",
    "as an active learning problem and by\n",
    "doing that we can gain insights into\n",
    "methods like deep ensembles and also\n",
    "propose other kind of bayesian methods\n",
    "that um provide even\n",
    "better marginalization like multi-swag\n",
    "um\n",
    "and you know very excited to say in the\n",
    "last year or so that a lot of bayesian\n",
    "methods are now providing\n",
    "better results than classical training\n",
    "both in terms of accuracy\n",
    "and uncertainty representation without a\n",
    "lot of additional overhead\n",
    "uh and uh you know really this\n",
    "emphasizes there's a big difference\n",
    "between your marginalization just\n",
    "regularization as we saw in that coin\n",
    "toss\n",
    "example uh and you know we should be\n",
    "careful not to conflate flexibility and\n",
    "complexity\n",
    "gaussian processes for instance can be\n",
    "extremely flexible of an infinite number\n",
    "of parameters\n",
    "simple inductive biases good\n",
    "generalization of a small number of\n",
    "points\n",
    "and also careful not to parameter count\n",
    "to the proxy for complexity sometimes\n",
    "the models of many more parameters in a\n",
    "sense are helping us find simpler\n",
    "solutions and we can really resolve a\n",
    "lot of mysterious results in deep\n",
    "learning\n",
    "by thinking about model construction and\n",
    "generalization from a probabilistic\n",
    "perspective\n",
    "so that's everything and i really like\n",
    "to thank you for attending this tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
