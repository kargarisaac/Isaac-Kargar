{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Model Based Reinforcement Learning (MBRL)\"\n",
    "> \"This is a summary of MBRL from ICML-2020 tutorial.\"\n",
    "\n",
    "- toc: True\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [fastpages, jupyter]\n",
    "- image: images/some_folder/your_image.png\n",
    "- hide: false\n",
    "- search_exclude: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post is a summary of the model-based RL tutorial at ICML-2020. You can find the videos [here](https://sites.google.com/view/mbrl-tutorial). The pictures are from the slides in the talk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction and Motivation\n",
    "\n",
    "Having access to a model of the world and using it for decision making is a powerful idea. \n",
    "There are a lot of applications of MBRL in different areas like robotics (manipulation- what will happen by doing an action), \n",
    "self-driving cars (having a model of other agents decisions and future motions and act accordingly),\n",
    "games (AlphaGo- search over different possibilities), Science ( chemical usecases),\n",
    "and peration research and energy applications (how to allocate renewable energy in different points in time to meet the demand).\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "In sequential decision making, the agent will interact with the world by doing action $a$ and getting the next state $s$ and reward $r$.\n",
    "\n",
    "\n",
    "<img src=\"images/rl.png\" width=\"400\">\n",
    "\n",
    "\n",
    "We can write this problem as a Markov Decision Process (MDP) as follows:\n",
    "\n",
    "- States $S \\epsilon R^{d_S}$\n",
    "- Actions $A \\epsilon R^{d_A}$\n",
    "- Reward function $R: S \\times A \\rightarrow R$\n",
    "- Transition function $T: S \\times A \\rightarrow S$\n",
    "- Discount $\\gamma \\epsilon (0,1)$\n",
    "- Policy $\\pi: S \\rightarrow A$\n",
    "\n",
    "The goal is to find a policy which maximizes the sum of discounted future rewards:\n",
    "$$\n",
    "\\text{argmax}_{\\pi} \\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t)\n",
    "$$\n",
    "subject to\n",
    "$$\n",
    "a_t = \\pi(s_t) , s_{t+1}=T(s_t, a_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to solve this optimization problem?! \n",
    "\n",
    "- Collect data $D= \\{ s_t, a_t, r_{t+1}, s_{t+1} \\}_{t=0}^T$.\n",
    "- Model-free: learn policy directly from data\n",
    "\n",
    "$$ D \\rightarrow \\pi \\quad \\text{e.g. Q-learning, policy gradient}$$\n",
    "\n",
    "- Model-based: learn model, then use it to **learn** or **improve** a policy \n",
    "\n",
    "$$ D \\rightarrow f \\rightarrow \\pi$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a model?\n",
    "\n",
    "a model is a representation that explicitly encodes knowledge about the structure of the environment and task.\n",
    "\n",
    "This model can take a lot of different forms:\n",
    "\n",
    "- A transition/dynamic model: $s_{t+1} = f_s(s_t, a_t)$\n",
    "- A model of rewards: $r_{t+1} = f_r(s_t, a_t)$\n",
    "- An inverse transition/dynamics model (which tells you what is the action to take and go from one state to the next state): $a_t = f_s^{-1}(s_t, s_{t+1})$\n",
    "- A model of distance of two states: $d_{ij} = f_d(s_i, s_j)$\n",
    "- A model of future returns: $G_t = Q(s_t, a_t)$ or $G_t = V(s_t)$\n",
    "\n",
    "Typically when someone says MBRL, he/she means the firs two items.\n",
    "\n",
    "<img src=\"images/model.png\">\n",
    "\n",
    "Sometimes we know the ground truth dynamics and rewards. Might as well use them! Like game environments or simulators like Mujoco, Carla, and so on.\n",
    "\n",
    "But we don't have access to the model in all cases, so we need to learn the model. In cases like in robots, complex physical dynamics, and interaction with humans.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use model?\n",
    "\n",
    "In model-free RL agent we have a policy and learning algorithm like the figure below:\n",
    "\n",
    "<img src=\"images/rl2.png\">\n",
    "\n",
    "In model-based RL we can use the model in three different ways:\n",
    "- simulating the environment: replacing the environment with model and use it to generate data and use it to update the policy.\n",
    "- Assisting the learning algorithm: modify the learning algorithm to use the model to interpret the data it is getting in a different way. \n",
    "- Strengthening the policy: allow the agent at test time to use the model to try out different actions before it commits to one of them (taking the action in the real world).\n",
    "\n",
    "<img src=\"images/mbrl.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, to compare model-free and model-based:\n",
    "    \n",
    "<img src=\"images/mbrl_vs_mfrl.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to learn a model?\n",
    "\n",
    "\n",
    "There are two different dimensions that are useful to pay attention to:\n",
    "- representation of the features for the states that the model is being learned over them\n",
    "\n",
    "- representation of the transition between states\n",
    "\n",
    "<img src=\"images/learn_model.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In continue we take a look at different transition models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### state-transition models\n",
    "\n",
    "In some cases, we know equations of motion and dynamics but we don't know the exact parameters like mass. We can use system identification to estimate unknown parameters like mass. But these sort of cases require having a lot of domain knowledge about how exactly the system works.\n",
    "\n",
    "<img src=\"images/learn_model2.png\">\n",
    "<img src=\"images/learn_model3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In some cases that we don't know the dynamics of motion, we can simply use an MLP to get a concatenation of $s_t, a_t$ and output the next state $s_{t+1}$.\n",
    "\n",
    "<img src=\"images/learn_model4.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cases that we have some, not perfect, domain knowledge about the environment, we can use graph neural networks (GNNs) to model the agent (robot). For example in Mujoco we can model a robot (agent) with nodes as its body parts and edges as joint and learn the physics engine.\n",
    "\n",
    "<img src=\"images/learn_model5.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### observation-transition models\n",
    "\n",
    "In this cases, we don't have access to states (low level states like joint angles), but we have access to images. The MDP for this cases would be like this:\n",
    "\n",
    "<img src=\"images/learn_model6.png\">\n",
    "\n",
    "So what can we do with this? \n",
    "\n",
    "- Directly predict transitions between observations (observation-transition models)\n",
    "\n",
    "<img src=\"images/learn_model7.png\">\n",
    "\n",
    "- Reconstruct observation at every timestep: Using sth like LSTMs. Here we need to reconstruct the whole observation in each timestep. The images can be blurry in these cases.\n",
    "\n",
    "<img src=\"images/learn_model8.png\">\n",
    "\n",
    "<img src=\"images/learn_model88.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### latent state-transition models\n",
    "\n",
    "Another option when we have just access to observation is to instead of making transition between observations we can infere a latent state and then make transitions in that latent space (latent state-transition models) not in the observation space. It would be much faster than reconstructing the observation on every timestep. We take our initial observation or perhaps the last couple of observations and embed them into the latent state and then unroll it in time and do predictions in $z$ instead of $o$.\n",
    "\n",
    "<img src=\"images/learn_model9.png\">\n",
    "\n",
    "Usually we use the observation and reconstruct it during training but at test time we can unroll it very quickly. we can also reconstruct observation at each timestep we want (not necessarily in all timesteps).\n",
    "\n",
    "<img src=\"images/learn_model10.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured latent state-transition models\n",
    "\n",
    "Another thing that you can do if you have a little bit more domain knowledge is to add a little bit of structure into your latent state. For example, if you know that the scene that you are trying to model consists of objects, then you can try to actually explicitly detect those objects, segment them out and then learn those transitions between objects.\n",
    "\n",
    "<img src=\"images/learn_model11.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent value models\n",
    "\n",
    "The idea is that when you unroll your latent-state, you additionally predict the value of the state at each point of the furture, in addition to reward. we can train the model without necessarily needing to train using observations, but just training it by predicting the value progressing toward actual observed values when you roll it out in the real environment.\n",
    "\n",
    "<img src=\"images/learn_model12.png\">\n",
    "\n",
    "why this is useful? because some types of planners actually only need you to predict values rather than predicting states lime MCTS (monte carlo tree search).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Parametric models\n",
    "\n",
    "So far we talked about parametric ways of learning the model. We can also use non-parametric methods like graphs.\n",
    "\n",
    "For example the replay buffer that we use in off-policy methods can be seen as an approximation to a type of model where if you have enough data in your replay buffer then you sample from buffer, it basically access the density model over your transitions. You can use extra replay to basically get the same level performances you would get using a model based method which learns a parametric model.\n",
    "\n",
    "<img src=\"images/learn_model13.png\">\n",
    "\n",
    "The other work we can do using data in buffer is to use data points and learn the transition between them and interpolate to find states between those states in buffer. Somehow learning a distribution and use it to generate new datapoints.\n",
    "\n",
    "<img src=\"images/learn_model14.png\">\n",
    "\n",
    "Another form of non-parametric transition is a symbolic description which is popular in the planning community not in the deep learning community. \n",
    "\n",
    "<img src=\"images/learn_model15.png\">\n",
    "\n",
    "The other form of non-parametric models is gaussian processes which gives us strong predictions using very very small amout of data. PILCO is one example of these algorithms.\n",
    "\n",
    "<img src=\"images/learn_model16.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based control and how to use a model?\n",
    "\n",
    "We will be using this landscape of various methods and categories that exist, including some representative algorithms:\n",
    "\n",
    "<img src=\"images/mbc1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw earlier, we can use the model in three different ways. In continue, we will see some examples of each case.\n",
    "\n",
    "<img src=\"images/mbc.png\" width=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Simulating the environment\n",
    "\n",
    "<img src=\"images/mbc2.png\" width=\"150\">\n",
    "\n",
    "One way is to mix the real data with model-generated experience and then apply traditional model-free algorithms like Q-learning, policy gradient, etc. In this cases, the model offfers a larger and augmented training dataset.\n",
    "\n",
    "**Dyna-Q** is an example that uses Q-learning with a learned model. Dyna does the traditional Q-learning updates on real transitions, and also use a model to create fictitious imaginary transitions from the real states and perform exactly the same Q-learning updates on those. So it's basically just a way to augment the experience.\n",
    "\n",
    "<img src=\"images/mbc3.png\">\n",
    "\n",
    "This can also be applied in policy learning. We don't need to perform just a single step but multiple steps according to the **model** to generate experience even further away from the real data and do policy parameter updates entirely on this fictitious experiences.\n",
    "\n",
    "<img src=\"images/mbc4.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assisting the learning algorithm\n",
    "\n",
    "<img src=\"images/mbc5.png\" width=\"150\">\n",
    "\n",
    "One important way that this can be done is to allow end-to-end training through our models. End-to-end training recently has been very successfull in improving and simplifying supervised learning methods in computer vision, NLP, etc. \n",
    "\n",
    "The question is \"can we apply the same type of end-to-end approaches to RL?\"\n",
    "\n",
    "One example is just the policy gradient algorithm. Let's say we want to maximize the sum of discounted future reward of some parametric policy. We can write the objective function with respect to the policy parameters $\\theta$\n",
    "\n",
    "$$\n",
    " J(\\theta) = \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)  , \\quad a_t = \\pi_{\\theta}(s_t) , \\quad s_{t+1} = T(s_t, a_t)\n",
    "$$\n",
    "\n",
    "Now we need to apply gradient ascent (for maximization) on policy gradient with respect to policy parameters $\\theta  \\rightarrow  \\nabla_{\\theta}J$.\n",
    "\n",
    "So how can we calculate this $\\nabla_{\\theta}J$ ?\n",
    "\n",
    "sampling-based methods have been proposed like REINFORCE to estimate this gradient. But the problem with them is that they cannot have a very high variance and they often require the policy to have some randomness to it to make decisions. This can be unfavorable.\n",
    "\n",
    "<img src=\"images/mbc6.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Accurate and smooth models, aside from imaginary experiences, offer derivatives:\n",
    "\n",
    "$$\n",
    "s_{t+1} = f_s(s_t, a_t) \\quad  r_t = f_r(s_t, a_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{s_t}(s_{t+1}), \\quad \\nabla_{a_t}(s_{t+1}), \\quad \\nabla_{s_t}(r_t), \\quad \\nabla_{a_t}(r_t), \\quad ...\n",
    "$$\n",
    "\n",
    "And they are able to answer questions such as: *how do small changes in action change next state or reward any of other quantities?*\n",
    "\n",
    "<img src=\"images/mbc7.png\" width=\"150\">\n",
    "\n",
    "\n",
    "Why is this useful? This is useful because it will allow us to do this type end-to-end differentiation algorithms like **back-propagation**.\n",
    "\n",
    "Let's rewrite our objective function using models:\n",
    "\n",
    "$$\n",
    " J(\\theta) \\approx \\sum_{t=0}^{H} \\gamma^t r_t  , \\quad a_t = \\pi_{\\theta}(s_t) , \\quad s_{t+1} = f_s(s_t, a_t), \\quad r_t=f_r(s_t,a_t)\n",
    "$$\n",
    "\n",
    "\n",
    "So how we can use this derivatives to calculate $\\nabla_{\\theta}J$ ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/mbc8.png\">\n",
    "\n",
    "The highlighted derivatives are easy to calculate using some libraries like PyTprch or TensorFlow.\n",
    "\n",
    "By calculating $\\nabla_{\\theta}J$ in this way:\n",
    "\n",
    "**pros**:\n",
    "+ The policy gradient that we get is actually a deterministic quantity and there is no variance to it. \n",
    "+ It can support potentially much longer term credit assignment\n",
    "\n",
    "**cons**:\n",
    "- It is prone to local minima\n",
    "- Poor conditioning (vanishing/exploding gradients)\n",
    "\n",
    "Here are two examples to use model-based back-propagation (derivatives) either along real or model-generated ttrajectories to do end to end training:\n",
    "\n",
    "<img src=\"images/mbc9.png\">\n",
    "\n",
    "- real trajectories are safer, but need to be from the current policy parameters (so it’s less sample-efficient)\n",
    "- model-generated trajectories allow larger policy changes without interacting with the real world, but might suffer more from model inaccuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strengthening the policy \n",
    "\n",
    "So far we talked about the first two ways of using a model in RL. These two ways are in category of **Background Planning**. \n",
    "\n",
    "There is another category based on the *Sutton and Barto (2018)- Reinforcement Learning: An Introduction* categorization, called **Decision-Time Planning**, which is a unique option we have available in model-based settings.\n",
    "\n",
    "#### What is the difference between background and decision-time planning?\n",
    "In background planning, we can think of it as answering the question \"how do I learn how to act in any possible situation to succeed and reach the goal?\"\n",
    "\n",
    "<img src=\"images/mbc10.png\" width=\"300\">\n",
    "\n",
    "- The optimization variables are parameters of a policy or value function or ... and are trained using expectation over all possible situations.\n",
    "- Conceptually we can think of background planning as learning a set of habits that I could just reuse.\n",
    "- We can think of background planning as learning fast type of thinking.\n",
    "\n",
    "\n",
    "In decision-time planning, we want to answer the question \"what is the best sequence of actions just for my current situation to succeed or reach the goal?\"\n",
    "\n",
    "<img src=\"images/mbc11.png\" width=\"300\">\n",
    "\n",
    "- The optimization parameters are just a sequence of actions or states.\n",
    "- Conceptually we can think of decision-time planning as finding our consciously improvising just for the particular situation that we find ourself in.\n",
    "- We can think of decision-time planning as learning slow type of thinking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why use one over the other?\n",
    "\n",
    "<img src=\"images/mbc12.png\">\n",
    "\n",
    "\n",
    "- *Act on most recent state of the world*: decision-time planning is just concerned about the current state in finding the sequence of actions. You can act based on the most recent state of the world. By contrast, in background planning the habits may be stale and might take a while to get updated as the world's changes.\n",
    "\n",
    "- *Act without any learning*: decision-time planning allows us to act without any learning at all. There is no need for policy or value networks before we can start making decisions. It is just an optimization problem as long as you have the model.\n",
    "\n",
    "- *Competent in unfamiliar situations*: if you find yourself in situations that are far away from where you were training, your set of habits or policy network might not have competence (the ability to do something successfully or efficiently) there. So you don't have any information to act or are very uncertain or even in the worst case, it will with confidence make decisions that just potentionally make no sense. This is out of distribution and generalization problem. In these cases the decision-time planning would be more beneficial.\n",
    "\n",
    "- *Independent of observation space*: another advantage of decision-time planning is that it is also independent of the observation space that you decide on. In background methods we need to consider some encoding or description of the state, joint angles or pixels or graphs, into our policy function. This decisions may play a large role on the total learning performance. When something is not working, you will not really know that is it because of the algorithm, or state space which doesn't contain enough information. In contrast, decision-time planning avoid this confounded, which in practice can be actually quite useful whn you're prototyping new methods.\n",
    "\n",
    "<img src=\"images/mbc13.png\" width=\"500\">\n",
    "\n",
    "- *Partial observability*: decision-time plannings have some issues with it. They assume that you know the full state of the world when you're making the plan. So it's hard to hide information from decision-time planners. It is possible but it is more costly.\n",
    "\n",
    "- *Fast computation at deployment*: decision-time planners require more computation. It is not just evaluating a habbit, but it needs more thinking.\n",
    "\n",
    "- *Predictability and coherence*: decision-time planners do some actions which are not necessarily predictable or coherent. Because you are consciously thinking about each foot step, you might not have a plan that's exactly the same. So you may have a very chaotic behavior that still succeeds. In contrast, the background planning, because it learns a set of habbits, it can perform a very regular behavior.\n",
    "\n",
    "- *Same for discrete and contiinuous actions*: background planning has a very unified treatment of discrete and continuous actions which is conceptually simpler. In decision-time planning, there are different algorithms for discrete and continuous actions. we will see in the following sections more about them.\n",
    "\n",
    "We can also mix and match background and decision-time plannings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the difference between discrete and continuous planning?\n",
    "\n",
    "It depends on the problem which you want to solve. So it is not a choice that you can make. For example in controlling a robot, the actions might be the torques for the motors (continuous), or in biomechanical settings it might be muscle excitations (continuous), or in medical problems the treatment that should be applied (discrete).\n",
    "\n",
    "The distinction between discrete and continuous actions is not significant for background planning methods. \n",
    "- You just learn a stochastic policies that sample either from discrete or continuous distributions.\n",
    "\n",
    "$$\n",
    "a \\sim \\pi(.|s) \\quad \\leftarrow Gaussian, categorical, ...\n",
    "$$\n",
    "\n",
    "- Backpropagation is still possible via some reparametrization techniques. See *Jang et al (2016). Categorical reparametrization with Gumbel-Softmax* for an example.\n",
    "\n",
    "In either of these cases (continuous and discrete in background planning methods), because you are optimizing over expectations, your final objective and optimization problem is still smooth wrt the policy parameters.\n",
    "\n",
    "$$\n",
    "J(\\theta) = E_{\\pi}[\\sum_t r_t], \\quad a_t \\sim \\pi(.|s_t, \\theta)\n",
    "$$\n",
    "\n",
    "But for decision-time planning, this distinction leads to specialized methods for discrete and continuous actions: discrete search or continuous trajectory optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some examples to be able to compare them.\n",
    "\n",
    "<img src=\"images/mbc14.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MCTS (monte carlo tree search)\n",
    "\n",
    "This algorithm is in discrete actions group and is used in alpha-go and alpha-zero. You keep track of Q-value, which is long term reward, for all states and actions ideally that you want to consider. And also the number of times that the state and action has been previously visited. \n",
    "\n",
    "1. Initialize $Q_0(s, a) = 0, N_0(s, a)=0, k=0$\n",
    "\n",
    "<img src=\"images/mbc15.png\" width=\"150\">\n",
    "\n",
    "2. Expansion: Starting from the current situation and expand nodes and selecting actions according to a search policy: \n",
    "\n",
    "$$\\pi_k(s) = Q_k(s,a)$$\n",
    "\n",
    "<img src=\"images/mbc16.png\" width=\"150\">\n",
    "\n",
    "3. Evaluation: When a new node is reached, estimate its long-term value using Monte-Carlo rollouts\n",
    "\n",
    "<img src=\"images/mbc17.png\" width=\"200\">\n",
    "\n",
    "4. Backup: Propagate the Q-values to parent nodes:\n",
    "\n",
    "$$\n",
    "Q_{k+1}(s, a) = \\frac{Q_k(s,a) N_k(s,a) + R}{N_k(s,a)+1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "N_{k+1}(s,a) = N_k(s,a)+1\n",
    "$$\n",
    "\n",
    "<img src=\"images/mbc18.png\" width=\"300\">\n",
    "\n",
    "\n",
    "5. Repeat Steps 2-4 until search budget is exhausted.\n",
    "$$\n",
    "k = k + 1\n",
    "$$\n",
    "\n",
    "<img src=\"images/mbc19.png\"  width=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trajectory Optimization\n",
    "\n",
    "Instead of keeping track of a tree of many possibilities, you just keep track of one possible action sequence.\n",
    "\n",
    "1. Initialize $a_0, ..., a_H$ from guess\n",
    "\n",
    "<img src=\"images/mbc20.png\" width=\"200\">\n",
    "\n",
    "2. **Expansion**: execute sequence of actions $a = a_0, ..., a_H$ to get a sequence of states $s_1, ..., s_H$\n",
    "\n",
    "<img src=\"images/mbc21.png\" width=\"200\">\n",
    "\n",
    "3. **Evaluation**: get trajectory reward $J(a) = \\sum_{t=0}^H r_t$\n",
    "\n",
    "4. **Back-propagation**: because everything is differentiable, you can just calculate the gradient of the reward via back-propagation using reward model derivatives and transition model derivatives.\n",
    "\n",
    "$$\n",
    "\\nabla_a J = \\sum_{t=0}^H \\nabla_a r_t\n",
    "$$\n",
    "$$\n",
    "\\nabla_a r_t = \\nabla_s f_r(s_t, a_t) \\nabla_a s_t + \\nabla_a f_r (s_t, a_t)\n",
    "$$\n",
    "$$\n",
    "\\nabla_a s_t = \\nabla_a f_s(s_{t-1}, a_{t-1}) + \\nabla_s f_s(s_{t-1}, a_{t-1})\\nabla_a s_{t-1}\n",
    "$$\n",
    "$$\n",
    "\\nabla_a s_{t-1} = ...\n",
    "$$\n",
    "\n",
    "5. Update all actions via gradient ascent $ a \\leftarrow a + \\nabla_a J$ and repeat steps 2-5.\n",
    "\n",
    "<img src=\"images/mbc22.png\" width=\"200\">\n",
    "\n",
    "The differences between discrete and continuous actions can be summarized as follows:\n",
    "\n",
    "<img src=\"images/mbc23.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The continuous example we saw above can be categorize in **shooting methods**.\n",
    "\n",
    "#### Variety and motivations of continuous planning methods\n",
    "\n",
    "Why so many variations? They all try to mitigate the issues we looked at like:\n",
    "- Sensitivity and poor conditioning\n",
    "- Only reaches local optimum\n",
    "- Slow convergence\n",
    "\n",
    "Addressing each leads to a different class of methods. \n",
    "\n",
    "<img src=\"images/mbc24.png\" width=\"200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Sensitivity and poor conditioning\n",
    "\n",
    "<img src=\"images/mbc24-2.png\" width=\"200\">\n",
    "\n",
    "**Shooting methods** that we have seen have this particular issue that small changes in early actions lead to very large changes downstream.\n",
    "\n",
    "<img src=\"images/mbc25.png\" width=\"200\">\n",
    "\n",
    "By expanding the objective function, this can be understood more clearly.\n",
    "\n",
    "$$\n",
    "\\max_{a_0,...,a_H} \\sum_{t=0}^H r(s_t, a_t), \\quad s_{t+1} = f(s_t, a_t)\n",
    "$$\n",
    "$$\n",
    "\\sum_{t=0}^H r(s_t, a_t) = r(s_0, a_0) + r(f(s_0, a_0), a_1)+...+r(f(f(...),...), a_H)\n",
    "$$\n",
    "\n",
    "It means that each state implicitly is dependent on all actions that came before it. This is similar to exploding/vanishing gradient ptoblem in RNNs that hurts long-term credit assignment. But unlike the RNN training, we cannot change the transition function because it is dictated to us by the environment. \n",
    "\n",
    "<img src=\"images/mbc26.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this problem, **Collocation** is introduced, which is optimizing for states and/or actions *directly*, instead of actions only. So we have different set of parameters that we are optmizing over.\n",
    "\n",
    "$$\n",
    "\\max_{s_0,a_0,...,s_H,a_H} \\sum_{t=0}^H r(s_t, a_t), \\quad ||s_{t+1} - f(s_t, a_t) || = 0 \\leftarrow \\text{explicit optimization constraint}\n",
    "$$\n",
    "\n",
    "It is an explicit contrained optimization problem, rather than just beeng satisfied by construction as in shooting methods.\n",
    "\n",
    "As a result, you only have pairwise dependencies between variables, unlike the dense activity graph in the previous figure for shooting methods.\n",
    "\n",
    "<img src=\"images/mbc27.png\" width=\"400\">\n",
    "\n",
    "An these methods have:\n",
    "- Good conditioning: changing $s_0, a_0$ has similar effect as changing $s_H, a_H$.\n",
    "- Larger but easier to optimize search space. It is useful for contact-rich problems such as some robotics applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Only reaches local optimum\n",
    "\n",
    "<img src=\"images/mbc28.png\" width=\"200\">\n",
    "\n",
    "Some approaches try to avoid local optima like sampling based methods: Cross-Entropy Methods (CEM) and $\\text{PI}^2$.\n",
    "\n",
    "For example in CEMs, instead of just maintaining the optimal trajectory, it maintains the mean and covariance of that optimal trajectory. \n",
    "\n",
    "<img src=\"images/mbc29.png\" width=\"500\">\n",
    "<img src=\"images/mbc30.png\" width=\"500\">\n",
    "<img src=\"images/mbc31.png\" width=\"500\">\n",
    "\n",
    "\n",
    "Despite being very simple, this works surprisingly well and has very nice guarantees on performance.\n",
    "\n",
    "Why does this work?\n",
    "- Search space of decision-time plans much smaller than space of policy parameters: ex. 30x32 vs 32x644x32\n",
    "- More feasible plans than policy parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slow convergence\n",
    "\n",
    "<img src=\"images/mbc32.png\" width=\"200\">\n",
    "\n",
    "Gradient descent is too slow to converge and we need to wait thousands-millions of iterations to train a policy. But this is too long for a one-time plan that we want to through it away after.\n",
    "\n",
    "Can we do something like Newton’s method for trajectory optimization like as non-linear optimization? YES!\n",
    "\n",
    "We can approximate transitions with linear functions and rewards with quadratics: \n",
    "\n",
    "$$\n",
    "\\max_{a_0,...,a_H} \\sum_{t=0}^H r_t, \\quad s_{t+1} = f_s(s_t, a_t), \\quad r_t=f_r(s_t, a_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_s(s_t, a_t) \\approx As_t + Ba_t, \\quad f_r(s_t, a_t) \\approx s_t^TQs_t + a_t^TRa_t\n",
    "$$\n",
    "\n",
    "Then it becomes Linear-Quadratic Regulator (LQR) problem and can be solved exactly.\n",
    "\n",
    "For iLQR, locally approximate the model around current solution, solve LQR problem to update solution, and repeat.\n",
    "\n",
    "For Differential dynamic programming (DDP) is similar, but with higher-order expansion of $f_s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based control in the loop\n",
    "\n",
    "We want to answer this question that how to both learn the model and act based on that simultaneusly?\n",
    "\n",
    "<img src=\"images/mbc33.png\" width=\"300\">\n",
    "\n",
    "### Gathering data to train models\n",
    "\n",
    "How we can gather data to train the model? this is kind of a chicken or the egg problem. Bad policy leads to bad experience, leads to bad model, leads to bad policy ...\n",
    "\n",
    "This leads to some training stability issues in practice. There are some recent works in game theory to provide criteria for stability. See \"Rajeswaran et al (2020). A Game Theoretic Framework for Model Based Reinforcement Learning.\" for example.\n",
    "\n",
    "#### Fixed off-line datasets \n",
    "\n",
    "Another way to address this in the loop issues is to see if we can actually train from a fixed experience that is not related to the policy. Some options that we have are:\n",
    "\n",
    "- Human demonstration\n",
    "- Manually-engineered policy rollouts\n",
    "- Another (sub-optimal) policy\n",
    "\n",
    "<img src=\"images/mbc34.png\" width=\"400\">\n",
    "\n",
    "This leads to a recent popular topic *model-based offline reinforcement learning*. You can see some recent works like *Kidambi et al (2020). MOReL: Model-Based Offline Reinforcement Learning.*, \n",
    "*Yu et al (2020). MOPO: Model-based Offline Policy Optimization.\n",
    "See also: Levine et al (2020).*, and *Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation\n",
    "\n",
    "Another way to generate data is to use the model to generate data to train itself. For example, in *Venkatraman et al (2014). Data as Demonstrator.*. You might have some trajectory of real experiment that you got by taking certain actions, then you roll out the model and train to pull its predicted next states to true next states.\n",
    "\n",
    "<img src=\"images/mbc35.png\" width=\"200\">\n",
    "\n",
    "<img src=\"images/mbc36.png\" width=\"400\">\n",
    "\n",
    "There are also some adversarial approaches to generate data to self-audit the model like *Lin et al (2020). Model-based Adversarial Meta-Reinforcement Learning.* and *Du et al (2019). Model-Based Planning with Energy Models.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But even if we do all of these works, models are not going to be perfect. We cannot have experience everywhere and there will be some approximation errors always.These small errors propagate and compound. We may end up in some states that are a little bit furthur away from true data which might be an unfamiliar situation. So it might end up making even bigger errors next time around and so on and so forth that the model rollouts might actually land very far away over time from where you would expected them to be.\n",
    "\n",
    "What's worse is that the planner might actually intentionally *exploit* these model errors to achieve the goal.\n",
    "\n",
    "<img src=\"images/mbc37.png\" width=\"200\">\n",
    "\n",
    "This leads to longer model rollouts to be less reliable. \n",
    "\n",
    "<img src=\"images/mbc38.png\" width=\"400\">\n",
    "\n",
    "\n",
    "You can check *Janner et al (2019). When to Trust Your Model:\n",
    "Model-Based Policy Optimization* for more details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acting under imperfect models\n",
    "\n",
    "The question is that \"Can we still act with imperfect models?\" the answer is yes!\n",
    "\n",
    "#### Replan via model-predictive control\n",
    "\n",
    "The first approach is to not commit to just one single plan (open loop control), but continually re-plan as you go along (closed-loop control). \n",
    "\n",
    "Let's see one example. \n",
    "\n",
    "you might start at some initial state and create an imaginary plan using the trajectory optimization methods like CEM or other methods. Then apply just the first action of this plan. That might take you to some state that might not in practice match with your model imagined you would end up with. But it's ok! you can just re-plan from this new state again and again take the first action and ... and by doing this there is a good chance to end up near the goal.\n",
    "\n",
    "<img src=\"images/mbc39.png\">\n",
    "<img src=\"images/mbc40.png\">\n",
    "<img src=\"images/mbc41.png\">\n",
    "<img src=\"images/mbc42.png\">\n",
    "<img src=\"images/mbc43.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing this, the errors don't accumulate. So you don’t need a perfect model, just one pointing in the right direction is enough. This re-planning might be expensive but one solution is to reuse solutions from previous steps as initial guesses for next plan.\n",
    "\n",
    "<img src=\"images/mbc44.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plan conservatively\n",
    "\n",
    "We have'seen that longer rollouts become more unreliable. One soulution would be to just keep the rollouts short. So we don't deviate too far from where we have real data. And as we saw in Dyna, just one single rollout can be also very helpful to improve learning.\n",
    "\n",
    "<img src=\"images/mbc45.png\">\n",
    "\n",
    "The other option to plan conservatively is to consider a distribution over your models and plan for either the average or worst case wrt distribution over your model or model uncertainty.\n",
    "\n",
    "$$\n",
    "\\max_{\\theta} E_{f \\sim F} [\\sum_t \\gamma^t r_t], \\quad a_t=\\pi_{\\theta}(s_t), \\quad s_{t+1}=f_s(s_t, a_t), \\quad r_t=f_r(s_t, a_t)\n",
    "$$\n",
    "\n",
    "<img src=\"images/mbc46.png\" width=\"400\">\n",
    "\n",
    "<img src=\"images/mbc47.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option for conservative planning is to try to stay close to states where the model is certain. There are a couple of ways to do this:\n",
    "\n",
    "<img src=\"images/mbc48.png\" width=\"150\">\n",
    "\n",
    "* Implicitly: stay close to past policy that generated the real data\n",
    "    - Peters et al (2012). Relative Entropy Policy Search\n",
    "    - Levine et al (2014). Guided Policy Search under Unknown Dynamics.\n",
    "\n",
    "* Explicitly: add penalty to reward or cost function for going into unknown region \n",
    "    - Kidambi et al (2020). MOReL: Model-Based Offline Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last two options for conservative planning, we need the uncertainty. So how do we get this model uncertainty? \n",
    "\n",
    "### Estimating model uncertainty\n",
    "\n",
    "Model uncertainty if necessary for conservative planning, but it has other applications too that we will see later.\n",
    "\n",
    "We consider two sources of uncertainty:\n",
    "\n",
    "1. Epistemic uncertainty\n",
    "    - Model's lack of knowledge about the world\n",
    "    - Distribution over beliefs\n",
    "    - Reducible by gathering more experience about the world\n",
    "    - Changes with learning\n",
    "2. Aleatoric uncertainty/Risk\n",
    "    - World's inherent stochasticity\n",
    "    - Distribution over outcomes\n",
    "    - Irreducible\n",
    "    - Static as we keep learning\n",
    "    \n",
    "There are multiple approaches to estimate these uncertainties which are listed as follows:\n",
    "\n",
    "- Probabilistic neural networks that try to model distributions over the outputs of your model.\n",
    "\n",
    "    - Model explicitly outputs means and variances (typically Gaussian)\n",
    "    \n",
    "    $$ p(s_{t+1}|s_t, a_t) = N(\\mu_{\\theta}(s_t, a_t), \\sigma_{\\theta}(s_t, a_t))$$\n",
    "\n",
    "    - Simple and reliable (supervised learning)\n",
    "    - Only captures aleatoric uncertainty / risk\n",
    "    - No guarantees for reasonable outputs outside of training data\n",
    "\n",
    "\n",
    "- Bayesian neural network\n",
    "    - Model has distribution over neural network weights\n",
    "    \n",
    "    $$ p(s_{t+1}|s_t, a_t) = E_{\\theta}[p(s_{t+1}|s_t, a_t, \\theta)]$$\n",
    "\n",
    "    - Captures epistemic and aleatoric uncertainty\n",
    "    - Factorized approximations can underestimate uncertainty\n",
    "    - Can be hard to train (but an active research area)\n",
    "\n",
    "<img src=\"images/mbc49.png\" width=\"500\">\n",
    "\n",
    "    \n",
    "- Gaussian processes \n",
    "    - Captures epistemic uncertainty\n",
    "    - Explicitly control state distance metric\n",
    "    - Can be hard to scale (but an active research area)\n",
    "- Pseudo-counts\n",
    "    - Count or hash states you already visited\n",
    "    - Captures epistemic uncertainty\n",
    "    - Can be sensitive to state space in which you count\n",
    "\n",
    "<img src=\"images/mbc50.png\" width=\"500\">\n",
    "\n",
    "    \n",
    "- Ensembles\n",
    "    - Train multiple models independently and combine predictions across models\n",
    "    - Captures epistemic uncertainty\n",
    "    - Simple to implement and applicable in many contexts\n",
    "    - Can be sensitive to state space and network architecture\n",
    "\n",
    "For discussion in the context of reinforcement learning see *Osband et al (2018). Randomized Prior Functions for Deep Reinforcement Learning.*\n",
    "\n",
    "Between the above options, Ensembles currently are popular due to simplicity and flexibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining planning and learning\n",
    "\n",
    "We compared these two methods in previous sections and saw that background and decision-time planning have complementary strengths and weaknesses.\n",
    "\n",
    "How to combine decision-time planning and background planning methods and get the benefits of both?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distillation\n",
    "\n",
    "In this approach we gather a collection of initial states and run our decision-time planner for each initial state and get a collection of trajectories that succeed at reaching the goal. Once we collected these collection of optimal trajectories, we can use a supervised learning algorithm to train either policy function or any other function to map states to actions. This is similar to Behavioral Cloning (BC). \n",
    "\n",
    "<img src=\"images/comb1.png\">\n",
    "<img src=\"images/comb2.png\">\n",
    "\n",
    "Some issues that can arrise:\n",
    "\n",
    "- **What is the learned policies have compounding errors?** If we rollout the policy from one of the states, it does something different than what we intended to do.\n",
    " \n",
    "1. Create new decision-time plans from these statse that have been visited by policy.\n",
    "2. Add these trajectories (new decision-time plans) to the distillation dataset (expand dataset where policy makes errors)\n",
    "    \n",
    "<img src=\"images/comb3.png\" width=\"300\">\n",
    "\n",
    "This is the idea of Dagger algorithm:\n",
    "    \n",
    "<img src=\"images/comb4.png\" width=\"300\">\n",
    "    \n",
    "- **What if the plans are not consistent?** There are sevaral ways to achieving a goal and we've seen that by changing the initial condition only a little bit, the decision-time planner can give us pretty different solutions to reach a single goal. This chaotic behavior might be hard to distill into the policy\n",
    "\n",
    "<img src=\"images/comb5.png\">\n",
    "\n",
    "1. we can make it so that the policy function that we are learning actually feeds back and influences our planner. \n",
    "2. To do this, we can add an additional term in our cost that says stay close to the policy. $D$ in the below cost function is the distance between actions of the planner, $a_t$, and the policy outputs, $\\pi(s_t)$. \n",
    "    \n",
    "<img src=\"images/comb6.png\">\n",
    "<img src=\"images/comb7.png\" width=\"300\">\n",
    "<img src=\"images/comb8.png\" width=\"400\">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terminal value functions (value of the terminal state)\n",
    "\n",
    "One of the issues with many trajectory optimization or discrete search approaches is that the planning horizon is typically finite. This may lead to myopic or greedy behavior.\n",
    "\n",
    "$$\n",
    "J^H = \\sum_{t=0}^H \\gamma^t r_t\n",
    "$$\n",
    "\n",
    "To solve this problem, we can use the value function at terminal state and add it to the objective function. This learned value fuinction guides plans to long-term good states. So the objective function would be infinite horizon:\n",
    "\n",
    "$$\n",
    "J^{\\infty} = \\sum_{t=0}^{\\infty} \\gamma^t r_t = \\sum_{t=0}^H \\gamma^t r_t + \\gamma^H V(s_H)\n",
    "$$\n",
    "\n",
    "This is another kind of combining decision-time planning (optimization problem) with background planning (learned value function).\n",
    "\n",
    "<img src=\"images/comb9.png\" width=\"200\">\n",
    "\n",
    "This can be used in both discrete and continuous action spaces:\n",
    "\n",
    "<img src=\"images/comb10.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Planning as policy improvement\n",
    "\n",
    "So far we used policy (background) or decision-time planner to make decision and generate trajectory and actions. \n",
    "\n",
    "<img src=\"images/comb11.png\" width=\"300\">\n",
    "\n",
    "But we can combine them and use planner as policy improvement. We can use the policy to provide some information for the planner. For example policy can output its set of trajectories and the planner can use it as a warm start or initialization to improve upon. We would like to train the policy such that the improvement proposed by the planner has no effect. So the policy trajectory be the best that we can do. I think we can see the planner as a teacher for the policy.\n",
    "\n",
    "Some related papers are listed here:\n",
    "- *Silver et al (2017). Mastering the game of Go without human knowledge.*\n",
    "- *Levine et al (2014). Guided Policy Search under Unknown Dynamics.*\n",
    "- *Anthony et al (2017). Thinking Fast and Slow with Deep Learning and Tree Search.*\n",
    "\n",
    "<img src=\"images/comb12.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implicit planning\n",
    "\n",
    "In addition to use planner to improve policy trajectory, we can put the planner as a component *inside* the policy network and train end-to-end. \n",
    "\n",
    "<img src=\"images/comb13.png\" width=\"300\">\n",
    "\n",
    "The advantage of doing this is that the policy network dictates abstract state/action spaces to plan in. But the downside of this is that it requires differentiating through the planning algorithm. But the good news is that multiple algorithms we've seen have been made differentiable and amenable to integrating in such a planner.\n",
    "\n",
    "some examples are as follows:\n",
    "\n",
    "<img src=\"images/comb14.png\">\n",
    "<img src=\"images/comb15.png\">\n",
    "<img src=\"images/comb16.png\">\n",
    "\n",
    "There are also some works that show the planning could *emerge* in generic black-box policy network and model-free RL training.\n",
    "\n",
    "<img src=\"images/comb17.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
