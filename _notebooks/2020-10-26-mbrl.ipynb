{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Model Based Reinforcement Learning (MBRL)\"\n",
    "> \"This is a summary of MBRL from ICML-2020 tutorial.\"\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Isaac Kargar\n",
    "- categories: [jupyter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction and Motivation\n",
    "\n",
    "Having access to a model of the world and using it for decision making is a powerful idea. \n",
    "There are a lot of applications of MBRL in different areas like robotics (manipulation- what will happen by doing an action), \n",
    "self-driving cars (having a model of other agents decisions and future motions and act accordingly),\n",
    "games (AlphaGo- search over different possibilities), Science ( chemical usecases),\n",
    "and peration research and energy applications (how to allocate renewable energy in different points in time to meet the demand).\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "In sequential decision making, the agent will interact with the world by doing action $a$ and getting the next state $s$ and reward $r$.\n",
    "\n",
    "\n",
    "<img src=\"files/images/rl.png\">\n",
    "\n",
    "\n",
    "We can write this problem as a Markov Decision Process (MDP) as follows:\n",
    "\n",
    "- States $S \\epsilon R^{d_S}$\n",
    "- Actions $A \\epsilon R^{d_A}$\n",
    "- Reward function $R: S \\times A \\rightarrow R$\n",
    "- Transition function $T: S \\times A \\rightarrow S$\n",
    "- Discount $\\gamma \\epsilon (0,1)$\n",
    "- Policy $\\pi: S \\rightarrow A$\n",
    "\n",
    "The goal is to find a policy which maximizes the sum of discounted future rewards:\n",
    "$$\n",
    "argmax_{\\pi} \\sum_{t=0}^\\inf \\gamma^t R(s_t, a_t)\n",
    "$$\n",
    "subject to\n",
    "$$\n",
    "a_t = \\pi(s_t) , s_{t+1}=T(s_t, a_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to solve this optimization problem?! \n",
    "\n",
    "- Collect data $D= \\{ s_t, a_t, r_{t+1}, s_{t+1} \\}_{t=0}^T$.\n",
    "- Model-free: learn policy directly from data\n",
    "\n",
    "$ D \\rightarrow \\pi$ e.g. Q-learning, policy gradient\n",
    "\n",
    "- Model-based: learn model, then use it to **learn** or **improve** a policy \n",
    "\n",
    "$ D \\rightarrow f \\rightarrow \\pi$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a model?\n",
    "\n",
    "a model is a representation that explicitly encodes knowledge about the structure of the environment and task.\n",
    "\n",
    "This model can take a lot of different forms:\n",
    "\n",
    "- A transition/dynamic model: $s_{t+1} = f_s(s_t, a_t)$\n",
    "- A model of rewards: $r_{t+1} = f_r(s_t, a_t)$\n",
    "- An inverse transition/dynamics model (which tells you what is the action to take and go from one state to the next state): $a_t = f_s^{-1}(s_t, s_{t+1})$\n",
    "- A model of distance of two states: $d_{ij} = f_d(s_i, s_j)$\n",
    "- A model of future returns: $G_t = Q(s_t, a_t)$ or $G_t = V(s_t)$\n",
    "\n",
    "Typically when someone says MBRL, he/she means the firs two items."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use model?\n",
    "\n",
    "<img src=\"files/images/rl2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
