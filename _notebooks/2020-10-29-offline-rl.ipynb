{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Offline Reinforcement Learning\"\n",
    "> \"This is a summary of Sergey Levine's talk on Offline RL\"\n",
    "\n",
    "- toc: True\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [fastpages, jupyter]\n",
    "- image: images/some_folder/your_image.png\n",
    "- hide: false\n",
    "- search_exclude: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what makes modern machine learning work?\n",
    "\n",
    "so to start off let's start with a big question what is it that makes machine learning work you know i'm going to give a\n",
    "simple answer to a big question this is\n",
    "going to be\n",
    "maybe somewhat controversial but perhaps\n",
    "many of you will agree with this at\n",
    "least at a very very high level\n",
    "i think that what makes machine learning\n",
    "work today is really\n",
    "the combination of large and highly\n",
    "diverse data sets\n",
    "and large and high capacity models and\n",
    "what we've seen time and time again\n",
    "is that across a range of domains from\n",
    "image recognition\n",
    "to machine translation to text\n",
    "recognition and speech recognition\n",
    "this kind of formula seems to be the\n",
    "formula that leads to good results\n",
    "if you collect a large data set like\n",
    "imagenet or ms coco\n",
    "and then train a huge model with dozens\n",
    "or even hundreds of layers\n",
    "that's going to be the thing that leads\n",
    "to very good performance and arguably\n",
    "the widespread enthusiasm about machine\n",
    "learning in recent years\n",
    "has really been spurred on by\n",
    "applications\n",
    "that follow this basic formula\n",
    "so what about reinforcement learning\n",
    "what about uh using\n",
    "learning to figure out how to make\n",
    "decisions\n",
    "well uh reinforcement learning is\n",
    "fundamentally\n",
    "at least in the textbook setting an\n",
    "active learning framework where you have\n",
    "an agent\n",
    "that interacts with the world collects\n",
    "some experience\n",
    "uses that experience to update its model\n",
    "policy or value function\n",
    "and then collects more experience and\n",
    "this process is repeated many many times\n",
    "i and we've seen that this basic recipe\n",
    "does lead to good results across\n",
    "a range of domains from from playing\n",
    "video games to basic robotic\n",
    "locomotion and manipulation skills and\n",
    "even to play in the game of go\n",
    "however when you want\n",
    "learning systems that generalize\n",
    "effectively to large-scale real-world\n",
    "settings\n",
    "like the ones that i showed on the\n",
    "previous slide you still have to collect\n",
    "large and diverse data sets and in an\n",
    "active learning framework\n",
    "this means that each time the agent\n",
    "interacts with the world they need to\n",
    "collect\n",
    "a breadth of experience that covers the\n",
    "sort of situations they might see in the\n",
    "world\n",
    "for example if you imagine using\n",
    "reinforcement learning to train an\n",
    "autonomous driving system\n",
    "now this system might need to practice\n",
    "driving in\n",
    "many many different cities each time it\n",
    "updates the model\n",
    "so it goes and drives in san francisco\n",
    "new york berlin\n",
    "london updates the model and goes back\n",
    "to san francisco new york berlin and\n",
    "london\n",
    "now this very quickly becomes\n",
    "impractical and indeed\n",
    "if we look at the kind of domains in\n",
    "which modern reinforcement learning\n",
    "algorithms have been successful\n",
    "and then contrasted side by side with\n",
    "the kind of domains where we've seen the\n",
    "success of supervised learning methods\n",
    "that can leverage very large and diverse\n",
    "data sets\n",
    "we see that there's a really big gulf\n",
    "it's not that the reinforcing learning\n",
    "algorithms are not capable they're\n",
    "learning very sophisticated strategies\n",
    "very sophisticated behaviors\n",
    "but they're not exhibiting the same kind\n",
    "of open world generalization\n",
    "all of the domains that i'm showing on\n",
    "the left can all be characterized as\n",
    "closed world settings\n",
    "we know exactly what the rules of go are\n",
    "we know how the emulator for the video\n",
    "game works even the robotics application\n",
    "is a laboratory setting\n",
    "whereas all the applications shown on\n",
    "the right are open world domains\n",
    "images mined from flickr from all over\n",
    "the world\n",
    "natural text natural speech collected\n",
    "from real human beings\n",
    "actually speaking or writing text in the\n",
    "real world\n",
    "so if we want to bridge this gulf of\n",
    "generalization\n",
    "what can we do how can we bring\n",
    "reinforcement learning into these open\n",
    "world settings\n",
    "to address the kind of applications that\n",
    "we actually want\n",
    "well i'm going to posit that in order to\n",
    "enable this we really need to develop\n",
    "data-driven reinforcement learning\n",
    "methods and data driven\n",
    "means that you need to be able to use\n",
    "large and diverse previously collected\n",
    "data sets\n",
    "in the same way that supervised learning\n",
    "algorithms can utilize large and diverse\n",
    "previously collected data sets\n",
    "the classic textbook version of\n",
    "reinforcement learning\n",
    "is really an on policy formulation in an\n",
    "on policy reinforcement learning\n",
    "algorithm\n",
    "you have a setting where each time the\n",
    "agent updates its policy\n",
    "it has to go and collect new data\n",
    "a very common area of study in\n",
    "reinforcement learning is also to study\n",
    "off policy reinforcement learning\n",
    "algorithms and these algorithms can\n",
    "utilize previous data\n",
    "but they typically still learn in an\n",
    "active online fashion\n",
    "meaning that the agent interacts with\n",
    "the world collects some data\n",
    "adds that data to its buffer and then\n",
    "uses the latest data and all pass data\n",
    "to improve its policy\n",
    "and we might think that we could simply\n",
    "cut that connection we could simply use\n",
    "prior data\n",
    "using the same exact algorithms but as\n",
    "i'll discuss in today's talk\n",
    "that actually doesn't work very well and\n",
    "if we really want data driven\n",
    "rl algorithms then we need to come up\n",
    "with some new algorithmic techniques to\n",
    "make this possible\n",
    "and develop a new class of what i'm\n",
    "going to call offline reinforcement\n",
    "learning methods\n",
    "these have also been called batch\n",
    "reinforcement methods in the literature\n",
    "the idea here is that a data set was\n",
    "collected previously\n",
    "with some behavior policy that i'm going\n",
    "to note pi beta in general you might not\n",
    "even know what pi beta was maybe it was\n",
    "a person driving a car a person\n",
    "controlling the robot\n",
    "maybe it was the robot's own experience\n",
    "and other tasks whatever it was\n",
    "it collected a buffer of data a large\n",
    "and diverse data set\n",
    "that covers many different open world\n",
    "situations and now you have to learn the\n",
    "best policy you can\n",
    "for your current task using just that\n",
    "data without the\n",
    "luxury of interacting with the world\n",
    "further to try out uh different methods\n",
    "now of course in reality you might want\n",
    "something more like a hybrid in reality\n",
    "maybe\n",
    "what we have is we have a big data set\n",
    "from all the past interactions that our\n",
    "agent has had\n",
    "maybe you have a robot that has cleaned\n",
    "the kitchen and mocked the floors and\n",
    "did many other things and that's sort of\n",
    "it's it's foundational data it's it's\n",
    "imagenet\n",
    "style data set that it's going to use to\n",
    "get generalizable skills\n",
    "we're going to use that data set in an\n",
    "offline rl fashion\n",
    "and then perhaps we'll actually come\n",
    "back and interact with the world a\n",
    "little bit more\n",
    "just to fine-tune that skill just to\n",
    "collect task specific\n",
    "data in small amounts and i'll actually\n",
    "discuss some methods that can\n",
    "do this as well but the main component\n",
    "the main challenge of this recipe is\n",
    "really the first part is using large and\n",
    "diverse previous data sets\n",
    "to get the best policy you can without\n",
    "having to revisit all those diverse and\n",
    "varied situations\n",
    "without having the car go back and drive\n",
    "in san francisco and new york and berlin\n",
    "and london each time its policy changes\n",
    "and if we can accomplish this if we can\n",
    "build algorithms that have this\n",
    "capability\n",
    "then we will not only be able to train\n",
    "better policies for\n",
    "robots and things like that but we'll\n",
    "also be able to apply reinforcement\n",
    "learning\n",
    "to domains where conventionally it has\n",
    "been very difficult to use\n",
    "for instance we could imagine using past\n",
    "data to use reinforcement learning\n",
    "to train a policy for advising doctors\n",
    "on treatment plans and diagnoses\n",
    "it would be pretty crazy to imagine an\n",
    "active exploration procedure for doing\n",
    "this because that would result in\n",
    "enormous costs and liability but if you\n",
    "can use previously expected offline data\n",
    "to get the best decision making system\n",
    "you can to support doctors in a medical\n",
    "setting\n",
    "well that seems like a pretty good\n",
    "recipe you can imagine the same\n",
    "procedure being applied to scientific\n",
    "discovery problems\n",
    "controlling power plants chemical plants\n",
    "operations research\n",
    "logistics and inventory management\n",
    "finance in all of these domains\n",
    "there's ample previously collected\n",
    "historical data\n",
    "but it's extremely difficult and costly\n",
    "to run active online data collection so\n",
    "we would expect that these would be the\n",
    "domains that could be revolutionized\n",
    "with effective offline reinforcement\n",
    "learning algorithms\n",
    "so in today's talk i'm going to cover\n",
    "the following topics\n",
    "first i'll discuss why offline\n",
    "reinforcement learning is difficult\n",
    "then i'll talk about some basic recipes\n",
    "for designing offline rl algorithms and\n",
    "a little bit about recent progress in\n",
    "this area i'll talk about\n",
    "some of our recent work on model based\n",
    "offline rl\n",
    "and then i'll discuss a new algorithm\n",
    "that we're pretty excited about called\n",
    "conservative q learning\n",
    "which seems to be a way to do offline\n",
    "url that works quite a bit better\n",
    "than many of the previous methods and\n",
    "then i'll conclude with a discussion\n",
    "of how we should be evaluating our\n",
    "offline reinforcement learning\n",
    "algorithms\n",
    "and discuss a benchmark that we've\n",
    "developed recently called d4rl\n",
    "all right but let's start with the first\n",
    "topic let's talk about what makes\n",
    "offline reinforcement learning hard\n",
    "and why we can't use our standard off\n",
    "policy rl algorithms\n",
    "to solve it so first\n",
    "let's start with a quick primer on off\n",
    "policy reinforcement learning\n",
    "in a standard reinforced learning\n",
    "problem you have an agent that interacts\n",
    "with the world by selecting actions\n",
    "and the world responds with states and\n",
    "the rewards the consequences of those\n",
    "actions\n",
    "what the agent needs to do is select a\n",
    "policy which i'm going to denote as pi\n",
    "of a given s\n",
    "and that policy needs to be selected so\n",
    "as to maximize\n",
    "the reinforced learning objective which\n",
    "is the sum of expected rewards over all\n",
    "time\n",
    "so the agent doesn't just take the\n",
    "action that yields the highest reward\n",
    "right now\n",
    "it's supposed to take the action that\n",
    "yields the highest rewards\n",
    "in the long run now a very useful object\n",
    "for maximizing this reinforcement\n",
    "learning objective\n",
    "is the q function the q function tells\n",
    "you\n",
    "if you start a particular state take a\n",
    "particular action\n",
    "and then follow your current policy pi\n",
    "what is the total reward that you will\n",
    "accumulate\n",
    "and if you can somehow determine the q\n",
    "function for a given policy\n",
    "then you can always improve that policy\n",
    "by taking an action with probability one\n",
    "if it is the r max of the q function and\n",
    "zero otherwise\n",
    "so this greedy policy with respect to q\n",
    "pi will always be at least as good or\n",
    "better than the previous pi\n",
    "and you can also skip the middleman and\n",
    "directly enforce\n",
    "the bellman optimality equation for all\n",
    "state action tuples\n",
    "so if you may if you can make qsa equal\n",
    "to rsa\n",
    "plus the max of the q value at the next\n",
    "time step then you can also show\n",
    "that recovering the greedy policy from\n",
    "this will get you the optimal policy\n",
    "and one way you could do this is you\n",
    "could enforce this equation at all\n",
    "states\n",
    "by minimizing the difference between its\n",
    "left-hand side and right-hand side\n",
    "now this is all fairly basic stuff this\n",
    "is kind of what we learned\n",
    "uh you know when we first learned about\n",
    "reinforcement learning it's textbook\n",
    "rl but you know as many of you might\n",
    "already know\n",
    "the nice thing about this equation is\n",
    "that you don't necessarily need\n",
    "on policy data to enforce it now the\n",
    "stock is going to focus pretty much\n",
    "entirely on approximate dynamic\n",
    "programming methods basically\n",
    "q learning style methods or after critic\n",
    "methods there are many other methods for\n",
    "offline rl\n",
    "based more around policy gradient style\n",
    "estimators and i'm not going to talk\n",
    "about this so we'll basically start with\n",
    "q learning and actor critic and sort of\n",
    "stay there just as a as a warning\n",
    "okay so this procedure that i've laid\n",
    "out here this classic q learning\n",
    "approach\n",
    "is an off policy algorithm which means\n",
    "that you don't need on policy data\n",
    "to make the left-hand side and\n",
    "right-hand side of the bellman\n",
    "optimality equation\n",
    "equal to each other so you could imagine\n",
    "an off policy\n",
    "include q learning method or a theta q\n",
    "iteration procedure that has the\n",
    "following basic recipe this is sort of\n",
    "classic stuff from literature collect\n",
    "the data set of transitions\n",
    "using some policy add it to a buffer\n",
    "sample a batch from that buffer minimize\n",
    "the difference between the left hand\n",
    "side and right hand side\n",
    "of the bellman optimality equation and\n",
    "then repeat this process some number of\n",
    "times\n",
    "and then periodically you could go out\n",
    "and collect more data\n",
    "you could also think of this graphically\n",
    "you interacted with the world\n",
    "collected a data set of transitions\n",
    "you're going to run your q-learning\n",
    "procedure on that data set\n",
    "and then periodically go back and\n",
    "explore and of course if we're doing\n",
    "offline rl then this is the part that we\n",
    "would omit we would just\n",
    "take our buffer and just keep crunching\n",
    "away with q learning on that buffer\n",
    "so this is an off policy algorithm and\n",
    "it can accommodate previously collected\n",
    "data in principle\n",
    "so does it solve the awful nrl problem\n",
    "and if not what goes wrong\n",
    "well we studied this question a little\n",
    "bit so what i'm going to show next is\n",
    "some results from a large-scale\n",
    "reinforced learning project that we\n",
    "actually did at google a couple of years\n",
    "back called qt opt\n",
    "our goal in this project is to develop a\n",
    "highly generalizable and powerful\n",
    "robotic grasping system that could learn\n",
    "to grasp any object\n",
    "directly from raw images but in the\n",
    "process of doing this research\n",
    "we actually evaluated a little bit how\n",
    "these q learning style methods compare\n",
    "in the fully offline regime\n",
    "versus the regime where they're allowed\n",
    "to collect additional online data\n",
    "now here there's you know everything is\n",
    "kind of scaled up so there are seven\n",
    "different robots that are all collecting\n",
    "lots of data in parallel they're pushing\n",
    "it to a\n",
    "decentralized uh distributed buffer\n",
    "storing all the data from past\n",
    "experiments and there are multiple\n",
    "workers that are actually updating q\n",
    "values\n",
    "on everything in the buffer\n",
    "and we looked at how the system worked\n",
    "it actually worked really well\n",
    "it could pick up novel objects heavy\n",
    "objects awkward objects i could respond\n",
    "to perturbations that i'd never seen\n",
    "before so basically\n",
    "when we scale up reinforcement learning\n",
    "it does actually generalize something\n",
    "is really working and we're seeing\n",
    "generalization that kind of resembles\n",
    "the kind of\n",
    "uh open world generalization that we saw\n",
    "in the supervised system\n",
    "of course here one might argue it's not\n",
    "entirely open world it's still in the\n",
    "laboratory\n",
    "but it is generalizing to never before\n",
    "seen objects\n",
    "so we evaluated this method on a test\n",
    "set of objects of the policy never saw\n",
    "during training we actually bought\n",
    "entirely new objects just to ensure that\n",
    "they were not in the lab\n",
    "during the training process uh and\n",
    "and then we compared the algorithm train\n",
    "in fully offline mode\n",
    "meaning using just the data collected\n",
    "before trainable policy and see how it\n",
    "does and this was fairly good offline\n",
    "data it came from past rl experiments\n",
    "and we also evaluated the algorithm in\n",
    "fine-tuned mode where\n",
    "the policy was allowed to collect a\n",
    "little bit of additional data and\n",
    "fine-tune online the offline data set\n",
    "consisted of\n",
    "580 000 uh episodes\n",
    "so this is a very large grasping episode\n",
    "the online fine tuning added another 28\n",
    "000.\n",
    "so the amount of additional online data\n",
    "was pretty negligible\n",
    "so it was clearly not the increase in\n",
    "the data set size it was really that was\n",
    "online\n",
    "and we saw the following numbers the\n",
    "success rate was 87\n",
    "for the offline method and 96 with the\n",
    "additional online fine-tuning\n",
    "and while this might seem like a small\n",
    "difference if we rewrite this as error\n",
    "rates we have an error rate of 13\n",
    "for the offline method and four percent\n",
    "uh for the fine-tuned method so\n",
    "less than a third of the number of\n",
    "mistakes that's a pretty big difference\n",
    "actually\n",
    "so the system clearly works very well\n",
    "but\n",
    "you're getting three times fewer\n",
    "mistakes\n",
    "if you fine-tune you believe in a small\n",
    "amount of data so something about the\n",
    "fully offline setting\n",
    "seems to be pretty hard now\n",
    "more recently uh we actually studied\n",
    "this question\n",
    "in a paper called stabilizing off policy\n",
    "queue learning via bootstrapping error\n",
    "reduction\n",
    "by overall kumar justin food george\n",
    "tucker and myself\n",
    "and we want to understand what what are\n",
    "the reasons why this might be so\n",
    "difficult\n",
    "we had a few hypotheses that we wanted\n",
    "to investigate one hypothesis we had was\n",
    "maybe there's some kind of or fitting\n",
    "effect maybe when you train on offline\n",
    "data\n",
    "if you train too long you sort of\n",
    "overfit to that data and you start\n",
    "seeing bad performance\n",
    "but if it was an overfitting effect then\n",
    "what we would expect\n",
    "is that increasing the data set size\n",
    "should decrease the problem\n",
    "so what this plot shows is offline rl\n",
    "performance\n",
    "on the half cheetah benchmark and\n",
    "different colors denote different data\n",
    "sets so\n",
    "blue is 1000 uh transitions and\n",
    "red is 1 million transitions\n",
    "and you can see that there is virtually\n",
    "no difference between 1 000 and 1\n",
    "million so\n",
    "if there is an overfitting effect it is\n",
    "not the conventional kind of overfitting\n",
    "it doesn't seem to go away as you add\n",
    "more data\n",
    "now we also looked at how well the q\n",
    "function thought it was doing\n",
    "and meaning if you actually look at the\n",
    "q values\n",
    "for the current policy that tells you\n",
    "how well the q function\n",
    "thinks it's going to do when it's\n",
    "executed in the world\n",
    "and this is a plot showing that now\n",
    "something to note here is that the y\n",
    "axis here is actually a log scale so\n",
    "what this is showing is\n",
    "enormous overestimation actual\n",
    "performance is below zero\n",
    "estimated performance is between ten to\n",
    "the seventh and ten to the twentieth\n",
    "power\n",
    "so the q function thinks it'll do\n",
    "extremely well and it's doing extremely\n",
    "poorly\n",
    "well that's kind of weird um another\n",
    "hypothesis we had as well\n",
    "maybe the trouble is just the training\n",
    "date is just not good like you know\n",
    "maybe the best you can do with the state\n",
    "is -250\n",
    "now one way that you can evaluate that\n",
    "hypothesis you guys should look at the\n",
    "best\n",
    "transitions in the data set if you just\n",
    "copy the best transitions\n",
    "how will we do we'll actually do\n",
    "actually pretty well so\n",
    "this is usually not the case in these\n",
    "offline rl problems it's not that the\n",
    "training data doesn't have good behavior\n",
    "it's that somehow the q function becomes\n",
    "excessively confident and optimistic\n",
    "about actions that are not actually very\n",
    "good\n",
    "to understand what's happening here we\n",
    "first have to understand distributional\n",
    "shift\n",
    "so here's distribution shift in a\n",
    "nutshell\n",
    "when we run supervised learning we're\n",
    "typically solving what's called an\n",
    "empirical risk minimization problem\n",
    "so we have some data distribution p of x\n",
    "we have some label distribution p of y\n",
    "given x\n",
    "our training set consists of samples\n",
    "from p of x and p of y given x\n",
    "and then we're going to minimize some\n",
    "loss on those samples\n",
    "and if we're doing vellum bellman error\n",
    "minimization then we're minimizing the\n",
    "squared error\n",
    "between the values predicted by our\n",
    "function f theta\n",
    "and the target values y now when we run\n",
    "empirical risk minimization\n",
    "we could ask the question given some new\n",
    "data point x star\n",
    "is f theta of x star going to be correct\n",
    "well one thing that we do know is if we\n",
    "had enough samples\n",
    "then the expected value of the error\n",
    "under our training distribution should\n",
    "be low that's what generalization means\n",
    "generalization means the training error\n",
    "which is the empirical risk is\n",
    "representative of\n",
    "generalization if you have a large\n",
    "training set minimizing training error\n",
    "is going to minimize generalization\n",
    "error unless you overfit\n",
    "but that doesn't necessarily mean that\n",
    "we're going to make an accurate\n",
    "prediction\n",
    "on some other point x star\n",
    "for example the expected value of our\n",
    "error under some other distribution over\n",
    "x\n",
    "p bar of x is not in general going to be\n",
    "low\n",
    "unless p bar of x is equal to p of x so\n",
    "our error under a different\n",
    "input distribution might be in fact very\n",
    "high\n",
    "in fact even if x star was sampled from\n",
    "p of x we're not guaranteed that error\n",
    "on x star is low because we're\n",
    "minimizing expected error\n",
    "so we might still have some points with\n",
    "higher we're not minimizing point wise\n",
    "there or just expected error\n",
    "now you might say that usually when we\n",
    "train deep neural networks\n",
    "we're not too worried about this kind of\n",
    "distributional shift because deep\n",
    "networks are really powerful they\n",
    "generalize really really well\n",
    "so maybe it's okay but what if we select\n",
    "x star so as to maximize f theta of x\n",
    "see we might have a function that fits\n",
    "the true function really well\n",
    "let's say that the green curve\n",
    "represents the true function and the\n",
    "blue curve represents our fit\n",
    "mostly our fit is extremely good however\n",
    "if we pick the largest value of f theta\n",
    "of x\n",
    "um which point are we going to land in\n",
    "we're going to land exactly on that peak\n",
    "we're going to find the point that has\n",
    "the largest error in the positive\n",
    "direction\n",
    "so even if we're generalizing really\n",
    "really well even if we're doing well for\n",
    "x star points even ones outside of the\n",
    "distribution\n",
    "if we maximize the x we're going to get\n",
    "big errors\n",
    "so what does this have to do with q\n",
    "learning well\n",
    "let's look at our target values in q\n",
    "learning it's r of s\n",
    "a plus the max over a prime of q of s\n",
    "prime comma a prime\n",
    "and i'm going to rewrite this in kind of\n",
    "a funny way i'm going to write this\n",
    "as r plus the expected value of a prime\n",
    "under the policy pi nu where pi nu is\n",
    "this r max policy so pi nu\n",
    "assigns the probability of 1 to the r\n",
    "max now this is kind of just a really\n",
    "weird way of writing the max\n",
    "but i think it makes the distribution\n",
    "shift problem much more apparent\n",
    "so let's say that our target values are\n",
    "called y\n",
    "what is the objective for the q function\n",
    "well it's to minimize\n",
    "the empirical risk the empirical error\n",
    "against y\n",
    "under the distribution over states and\n",
    "actions induced by our behavior policy\n",
    "pi beta\n",
    "which means that we would expect q to\n",
    "give us accurate estimates of q values\n",
    "under pi beta\n",
    "right pi beta is the behavior policy\n",
    "so we expect good accuracy when pi beta\n",
    "is equal to pi nu so if pi nu is pi beta\n",
    "then we're fine we're going we're going\n",
    "to have lower estimates\n",
    "but how often is that true pi news\n",
    "chosen to maximize the q value so unless\n",
    "pi beta\n",
    "was actually also maximizing those same\n",
    "q values before it even knew what they\n",
    "were\n",
    "these things are not going to match and\n",
    "it's even worse\n",
    "pioneer is selected to be the r max\n",
    "and if you think back to the previous\n",
    "slide when you select your point with a\n",
    "max\n",
    "you get some bad news and that's why we\n",
    "see\n",
    "on the safchita experiments that the\n",
    "policy does poorly\n",
    "whereas the q function thinks it's doing\n",
    "really well because it's finding the\n",
    "actions\n",
    "with the largest error in the positive\n",
    "direction and that's why just naively\n",
    "applying off-pulse crl methods to the\n",
    "offline setting\n",
    "does not in general yield great results\n",
    "so we have to somehow combat this\n",
    "overestimation\n",
    "issue we have to combat the\n",
    "distributional shift\n",
    "in the action space so let's talk about\n",
    "how we can do this\n",
    "how do we design offline rl algorithms\n",
    "well one very large class of methods in\n",
    "the literature and this is summarized in\n",
    "a tutorial\n",
    "that we assembled recently called\n",
    "offline reinforcement tutorial review\n",
    "and perspectives on open problems\n",
    "one very large class of methods is what\n",
    "we're going to call policy constraint\n",
    "methods\n",
    "so it's easiest to describe policy\n",
    "constraint methods in the actor critics\n",
    "setting but they can be applied in the q\n",
    "learning setting too\n",
    "so here we're going to update our q\n",
    "function just like before using the\n",
    "expected value under pi new\n",
    "and then we'll update our policy not\n",
    "just by taking the r\n",
    "max but by taking the rmax subject to a\n",
    "constraint\n",
    "that some measure of divergence between\n",
    "pi nu and pi beta\n",
    "is bounded by epsilon so the idea is\n",
    "that if\n",
    "pi nu stays close to pi beta then the\n",
    "distributional shift\n",
    "will be bounded and therefore the\n",
    "effective auto distribution actions will\n",
    "be bounded\n",
    "and then you repeat this process so it's\n",
    "just like an actor critic algorithm\n",
    "only with this additional constraint\n",
    "against the behavior policy\n",
    "so does a solid distributional shift\n",
    "does it mean that we have no more\n",
    "erroneous values\n",
    "well to a degree it actually does so\n",
    "there's a\n",
    "pretty large class of methods that have\n",
    "explored various kinds of policy\n",
    "constraints\n",
    "uh it's it's a very old idea we coined\n",
    "the term policy constraint i think in\n",
    "our tutorial\n",
    "but it's this these kinds of approaches\n",
    "have been called many things in\n",
    "literature\n",
    "including kl divergence control uh\n",
    "maximum prl linearly cell blm dps all\n",
    "sorts of things\n",
    "and there are many uh researchers that\n",
    "have studied this for decades\n",
    "not just in the offline rl literature\n",
    "but in the url literature more broadly\n",
    "more recently this has been used\n",
    "extensively for various off policy and\n",
    "offline rl methods\n",
    "here is just a collection of recent\n",
    "citations kind of from the last five\n",
    "years\n",
    "that have done some variant of this\n",
    "now this does have a number of problems\n",
    "one problem is that\n",
    "this kind of approach might be way too\n",
    "conservative\n",
    "imagine the following scenario let's say\n",
    "that you have\n",
    "a behavior policy that is\n",
    "highly random in some places and almost\n",
    "deterministic in other places\n",
    "now when the behavior policy is highly\n",
    "random you actually get pretty good\n",
    "coverage of actions\n",
    "and the effect of out of distribution is\n",
    "actually minimized in fact if the\n",
    "behavior policy was\n",
    "uniformly random nothing would be out of\n",
    "distribution because it has full support\n",
    "but if you have a policy that is highly\n",
    "random in some places and highly\n",
    "deterministic in others\n",
    "it's actually very difficult to pick a\n",
    "value of epsilon that works\n",
    "because if you if you pick a value of\n",
    "epsilon that is too low\n",
    "then in the places where the behavioral\n",
    "policy is highly random you'll be able\n",
    "to do all sorts of interesting things in\n",
    "its support\n",
    "but in the places where it's not highly\n",
    "random you might take a really bad\n",
    "action\n",
    "right so imagine it's kind of close to\n",
    "deterministic when it's\n",
    "crossing a narrow bridge where with some\n",
    "small probability it falls off\n",
    "if you admit uh that amount of error you\n",
    "might fall off the bridge\n",
    "if you use a tighter constraint if you\n",
    "limit the amount of deviation from the\n",
    "behavior policy very strictly\n",
    "then you won't fall off the bridge\n",
    "you'll match it in that highly\n",
    "deterministic region will you also be\n",
    "forced\n",
    "to match the highly random distribution\n",
    "in the region where it's random and\n",
    "that's just useless right\n",
    "if the policy is being very random being\n",
    "just as random\n",
    "doesn't seem to give you anything like\n",
    "if it's random you can just choose\n",
    "within that support whatever you want\n",
    "so it's hard to pick a single value of\n",
    "epsilon that works better\n",
    "now one thing that can mitigate this is\n",
    "to use a support constraint\n",
    "basically say it's not that you have to\n",
    "match the distribution you have to match\n",
    "the support of that distribution\n",
    "so if the distribution is highly random\n",
    "you can do sort of anything you want\n",
    "within that support\n",
    "but if it's highly deterministic you\n",
    "really need to do whatever it did\n",
    "because you don't have any wiggle room\n",
    "to go out of support and we explored\n",
    "this a little bit\n",
    "uh in a in a paper from 2019 that\n",
    "introduced an algorithm called bear\n",
    "which uses support constraint the second\n",
    "issue\n",
    "which is a bit tougher to deal with and\n",
    "actually pretty problematic in practice\n",
    "is that estimating the behavior of the\n",
    "the behavior policy itself\n",
    "in order to enforce this constraint can\n",
    "be very difficult\n",
    "if you don't know what the behavior\n",
    "policy was maybe the data was collected\n",
    "by a human\n",
    "you have to somehow figure out what\n",
    "policy that human was following\n",
    "and if you make a mistake in figuring it\n",
    "out then your constraint might be\n",
    "incorrect\n",
    "so for example if you fit the behavior\n",
    "policy with supervised learning\n",
    "we know that supervised learning will\n",
    "make certain mistakes it has kind of a\n",
    "moment matching effect which means that\n",
    "it will average across modes\n",
    "which means you might have high\n",
    "probability actions under your behavior\n",
    "policy\n",
    "that at a very low probability under the\n",
    "true behavior policy\n",
    "and when you enforce a constraint\n",
    "against that the constraint will be\n",
    "highly imperfect\n",
    "and this can lead to very poor\n",
    "performance in practice\n",
    "now when is estimating the behavior\n",
    "policy especially hard\n",
    "well it's especially hard when the\n",
    "behavior policy actually consists of a\n",
    "mixture of multiple different behaviors\n",
    "and this is actually exactly the setting\n",
    "that we want because remember i\n",
    "motivated all of this\n",
    "by talking about the setting where you\n",
    "have a large and highly diverse data set\n",
    "and that's exactly what you would expect\n",
    "that your\n",
    "behavior policy would actually be a\n",
    "mixture of many different policies\n",
    "and at that point estimating it with a\n",
    "single neural network is actually very\n",
    "hard\n",
    "so the easy case is when all the data\n",
    "comes from the same markovian policy\n",
    "but this is not very common or very\n",
    "realistic uh you know if your data came\n",
    "from humans they're certainly not going\n",
    "to be markovian if it came from many\n",
    "past tasks you've done\n",
    "while each individual test might be\n",
    "markovian the mixture might not be\n",
    "so the hard case is where the data comes\n",
    "from many different policies\n",
    "and this is very common in reality and\n",
    "it's also very common when you're doing\n",
    "online fine-tuning so if you remember\n",
    "when i motivated all this i also said\n",
    "that we want to collect lots of data\n",
    "from past behaviors train a policy with\n",
    "offline rl and then maybe fine tune it\n",
    "further with a little bit of online data\n",
    "so let's use this online fine-tuning as\n",
    "a kind of test case\n",
    "in reality what we care about is the\n",
    "setting where data comes from many\n",
    "different policies but the online\n",
    "fine-tuning situation\n",
    "is a nice sort of petri dish in which to\n",
    "explore this problem\n",
    "so i'm going to talk about a method that\n",
    "we've developed that specifically\n",
    "targets that setting\n",
    "where first you have offline training on\n",
    "a large previously collected data\n",
    "set and then you have some online\n",
    "fine-tuning and during online\n",
    "fine-tuning you're adding more data\n",
    "from many different policies\n",
    "so this is work by uh two of my students\n",
    "astronautier and amazon gupta together\n",
    "with an undergraduate student\n",
    "named mortis and so the experiment is\n",
    "that we do online fine-tuning from\n",
    "offline initialization\n",
    "and what i'm showing in this plot is the\n",
    "log likelihood\n",
    "of the behavior policy fit so this is\n",
    "for one of these standard\n",
    "policy constraint methods in this case\n",
    "bayer and the left side shows the\n",
    "offline training so you can see during\n",
    "offline training we have pretty good log\n",
    "likelihood for fitting our behavior\n",
    "policy\n",
    "but during online training when we have\n",
    "these additional samples from\n",
    "many different policies being pushed to\n",
    "the buffer the likelihood\n",
    "of the policy estimate drops\n",
    "precipitously it drops very sharply for\n",
    "the online data and even drops for the\n",
    "old offline data because\n",
    "you have to also model the new offline\n",
    "data so you do worse on the offline\n",
    "offline data so so our fit\n",
    "gets bad and in fact we can see that uh\n",
    "this method this is bare but this would\n",
    "be true i think for for most policy\n",
    "constraint methods\n",
    "uh it doesn't do so well so this is just\n",
    "showing the online fine tuning\n",
    "and you're gonna have two choices you\n",
    "can use a strict constraint or a loose\n",
    "constraint\n",
    "if you use a strict constraint then you\n",
    "do a little bit better at the beginning\n",
    "that's the yellow line\n",
    "but you fail to improve because as the\n",
    "behavior policy deteriorates\n",
    "that strict constraint just basically\n",
    "causes everything to get stuck\n",
    "if you use a loose constraint then you\n",
    "improve over the course of online\n",
    "training but you start off in a really\n",
    "bad place\n",
    "and in this case it's no better than\n",
    "just initializing from scratch\n",
    "and in fact this general principle is\n",
    "borne out in a few other papers for\n",
    "example\n",
    "this paper i have cited here called maq\n",
    "shows that using a much more powerful\n",
    "class of behavior policies\n",
    "leads to substantial improvement\n",
    "implying behavior policy modeling\n",
    "is really a major bottleneck for these\n",
    "policy constraint methods\n",
    "so one of the things we could do is we\n",
    "could actually enforce a constraint\n",
    "without explicit modeling of the\n",
    "behavior policy so this is a kind of\n",
    "implicit constraint method\n",
    "so here's the idea the problem we want\n",
    "to solve\n",
    "is this constrained argmax\n",
    "it turns out that you can prove that the\n",
    "solution to this constrained\n",
    "optimization problem\n",
    "can be expressed in closed form and the\n",
    "way that you do this is you write down\n",
    "the lagrangian\n",
    "solve for the dual and then you can\n",
    "actually write down the optimum for the\n",
    "dual\n",
    "and the optimal solution is given by one\n",
    "over z times the behavior policy\n",
    "times the exponential of the advantage\n",
    "function\n",
    "and the advantage function is multiplied\n",
    "by one over lambda where lambda\n",
    "is a lagrange multiplier and this is\n",
    "straightforward to show using a duality\n",
    "argument\n",
    "uh we didn't actually come up with this\n",
    "this is this is actually been brought up\n",
    "in a number of previous papers including\n",
    "the\n",
    "reps papers by peters at all uh the\n",
    "rolex at all sci learning paper and many\n",
    "others this is kind of actually a very\n",
    "well known identity\n",
    "in kl regularized control\n",
    "but the interesting thing about this\n",
    "equation is that it shows\n",
    "that we can approximate the solution to\n",
    "the constrained optimization\n",
    "using a weighted maximum likelihood\n",
    "objective\n",
    "and the way to do this is to note that\n",
    "matching this pi\n",
    "star is the same as\n",
    "maximizing the likelihood of actions\n",
    "taken from\n",
    "pi star and you can do that by taking\n",
    "action instead from pi\n",
    "beta and weighting them by the\n",
    "exponentiated advantage\n",
    "so the if you can find the r\n",
    "max of the expression i have written\n",
    "here then you will get pi star\n",
    "provided you can match pi star exactly\n",
    "and the the cool thing about this\n",
    "expression now is that\n",
    "pi beta only shows up as the\n",
    "distribution the expectation so even\n",
    "though you need pi beta for this\n",
    "in reality all you actually need is\n",
    "samples from pi beta which means that\n",
    "you do not need to actually know the\n",
    "distribution pi beta\n",
    "and samples from pi beta that's exactly\n",
    "uh what our data set is composed of\n",
    "so we no longer need to estimate the\n",
    "behavioral policy we just need samples\n",
    "from it and our data set already\n",
    "contains those\n",
    "and of course the advantage we get from\n",
    "our critic so you could imagine an actor\n",
    "critic algorithm\n",
    "which updates the critic to get the\n",
    "advantages and then updates the policy\n",
    "using this weighted maximum likelihood\n",
    "expression\n",
    "all right so does this procedure work\n",
    "well if we look at some results from the\n",
    "paper we evaluated on these kind of\n",
    "dexterous manipulation tasks\n",
    "where we had a data set from human\n",
    "demonstrations\n",
    "uh with a with a data glove and then if\n",
    "you look at what those human\n",
    "demonstrations do they get a success\n",
    "rate of about 24\n",
    "on this door task with some online fine\n",
    "tuning and get 88\n",
    "success rate so this is doing much\n",
    "better than if you just copy the\n",
    "demonstrations\n",
    "and you get meaningful fine tuning now\n",
    "of course this is an offline rl method\n",
    "so it doesn't actually need\n",
    "demonstrations per se\n",
    "that's just what we had in these\n",
    "experiments we had other experiments\n",
    "that also used random data\n",
    "and quantitatively the method actually\n",
    "does really well it's shown by this red\n",
    "line here\n",
    "the pen task is the easiest the door is\n",
    "kind of medium and the relocate task is\n",
    "the hardest and you can see that as the\n",
    "tasks get harder\n",
    "this method denoted as awac ends up\n",
    "doing the best\n",
    "so it kind of matches previous work on\n",
    "the easy task and then greatly exceeds\n",
    "it on the harder tasks\n",
    "so this shows that something you know is\n",
    "really working with these implicit\n",
    "constraints\n",
    "all right now let me take a little\n",
    "detour to\n",
    "delve a little bit into the world of\n",
    "model based reinforcement learning\n",
    "because\n",
    "we can also develop effective offline rl\n",
    "algorithms in the model based world too\n",
    "so uh just a brief overview of\n",
    "model-based rl\n",
    "for those of you that aren't familiar\n",
    "with it in model based rl\n",
    "what we're going to do using data that\n",
    "we collect from the environment is we're\n",
    "going to fit a model we're going to fit\n",
    "an\n",
    "estimate to the transition probabilities\n",
    "p hat of st plus 1\n",
    "given st comma 80. and then we'll\n",
    "somehow use that model\n",
    "to train a policy pi of a t given st and\n",
    "then typically repeat the process\n",
    "so one way that you could imagine such a\n",
    "model based rl algorithm working and\n",
    "this is sort of a dyna style recipe\n",
    "is that you collect real data denoted by\n",
    "these block trajectories\n",
    "you use that real data to fit your model\n",
    "and then you're going to use that model\n",
    "to make\n",
    "kind of little short roll outs from\n",
    "different states in your data set\n",
    "and these little short rollouts will be\n",
    "on policy using your latest policy\n",
    "so the model kind of answers these\n",
    "what-if questions if i were in this\n",
    "state that i had seen before\n",
    "what would have happened if i took a\n",
    "different action and this of course lets\n",
    "you train your policy much more\n",
    "efficiently\n",
    "and of course in offline rl we're going\n",
    "to omit this error we're not going to\n",
    "allow the algorithm to collect more data\n",
    "it has to contend itself with the data\n",
    "that it already has so what could go\n",
    "wrong\n",
    "well as you might have already guessed\n",
    "the problem again is going to be\n",
    "distributional shift\n",
    "when we make these rollouts under the\n",
    "model\n",
    "if the policy that we're now evaluating\n",
    "is very different from the policy they\n",
    "originally collected the data\n",
    "rolling out the model under that policy\n",
    "will result in state visitations\n",
    "that are very different from the states\n",
    "under which the model was trained\n",
    "and the model will make very large\n",
    "mistakes and of course since the policy\n",
    "is again being trained to maximize the\n",
    "return\n",
    "it'll learn to exploit the model to\n",
    "essentially cheat so the model\n",
    "erroneously produces good states\n",
    "now in the in the literature one of the\n",
    "ways that people have mitigated this\n",
    "effect\n",
    "is by just shortening these rollouts so\n",
    "if you don't allow the policy to raw for\n",
    "very many steps under the model there's\n",
    "only so much exploiting that it can do\n",
    "but in the fully offline setting of\n",
    "course you don't want to do this because\n",
    "your policy might deviate drastically\n",
    "from the behavior policy so you want to\n",
    "give it longer roll outs\n",
    "so you can actually tell how well it's\n",
    "doing\n",
    "so the model's predictions will be\n",
    "invalid if you get these out of\n",
    "distribution states\n",
    "very much like the problem we saw before\n",
    "so\n",
    "one solution we've developed and this is\n",
    "uh joint work with\n",
    "kenya you uh and garrett thomas who are\n",
    "the co-first authors as well as a number\n",
    "of collaborators\n",
    "most of the team here is from stanford\n",
    "university led by professors\n",
    "chelsea finn and thank you mom and\n",
    "there's also some concurrent work\n",
    "by kadambi at all that also proposes a\n",
    "very closely related algorithm called\n",
    "moral\n",
    "but i'm going to talk about our paper\n",
    "called mopo\n",
    "so the solution is to basically punish\n",
    "the policy for exploiting these other\n",
    "distribution states\n",
    "so what we're going to do is we're going\n",
    "to take our original reward function\n",
    "and we'll subtract a penalty\n",
    "mult with a coefficient lambda and this\n",
    "penalty u\n",
    "is essentially going to be some quantity\n",
    "that is larger for states that are more\n",
    "out of distribution or state action\n",
    "tuples that are more out of distribution\n",
    "so it's a kind of uncertainty penalty\n",
    "and that's the only change we're going\n",
    "to make we're just going to change our\n",
    "reward like this\n",
    "and then use some existing model based\n",
    "rl algorithm the one that we used in\n",
    "mopo is based on an algorithm called\n",
    "mbpo model based policy optimization\n",
    "so the idea now is when you visit one of\n",
    "these outer distribution states\n",
    "you get a really big penalty and that\n",
    "results in the policy not wanting to\n",
    "visit those other distribution states\n",
    "instead of curving back\n",
    "to the support of the data\n",
    "okay so there's some pretty interesting\n",
    "theoretical analysis that we can do for\n",
    "this algorithm\n",
    "and the theory this is kind of the\n",
    "statement from the main theorem in the\n",
    "paper\n",
    "uh which states that under two technical\n",
    "assumptions\n",
    "the learned policy pi hat in the moba\n",
    "algorithm satisfies the property that\n",
    "the return of pi hat\n",
    "is greater than or equal to the supremum\n",
    "over pi\n",
    "of uh the return of pi minus two times\n",
    "lambda lambda\n",
    "times epsilon u okay so what is what are\n",
    "all these things\n",
    "there are two technical assumptions one\n",
    "is that we can basically represent\n",
    "the value function so things will break\n",
    "down badly if you have really bad\n",
    "function approximation error and the\n",
    "second one\n",
    "is an assumption on you which says that\n",
    "the true model error\n",
    "is bounded above by you so this is this\n",
    "is essentially saying that u is a\n",
    "consistent estimator\n",
    "for the error in the model and you need\n",
    "this property\n",
    "for basically because you need you to\n",
    "mean something in practice the way that\n",
    "we estimate mu\n",
    "is by training an ensemble of models and\n",
    "measuring their disagreement but any\n",
    "estimator that upper bounds\n",
    "the error in the model will do the job\n",
    "so this is the true return of the policy\n",
    "train under the model so not the return\n",
    "the model thinks it has but the\n",
    "return it actually has and\n",
    "epsilon here is just the expected value\n",
    "of u\n",
    "so what this is saying is that the true\n",
    "return of a policy we optimize under our\n",
    "model\n",
    "will actually have high will actually be\n",
    "at least as high\n",
    "as the best policy that still has to pay\n",
    "this\n",
    "error price another way of saying that\n",
    "it'll be at least as good\n",
    "as the best in distribution policy of\n",
    "course you can get the optimal policy\n",
    "because you don't know what happens out\n",
    "of distribution we can at least have to\n",
    "do at least as well\n",
    "as the best in distribution policy\n",
    "another way of saying this is that um\n",
    "you can construct this policy pi delta\n",
    "which is\n",
    "the best policy whose error is bound\n",
    "basically the best policy that\n",
    "doesn't visit states with error larger\n",
    "than delta\n",
    "so this is under the true mdp but it's\n",
    "saying you're going to\n",
    "find the best policy under the true mdp\n",
    "that doesn't visit states where the\n",
    "model would have made mistakes\n",
    "and this result says that the policy we\n",
    "learn with model-based rl will be at\n",
    "least as good as that policy\n",
    "minus an error term that scales us two\n",
    "times lambda times delta\n",
    "so this basically shows that we'll get a\n",
    "good policy within the support of our\n",
    "data\n",
    "so some implications of this this means\n",
    "that we always improve over the behavior\n",
    "policy\n",
    "and we can actually quantify the\n",
    "optimality gap against the optimal\n",
    "policy in terms of model error so\n",
    "basically\n",
    "if our error on the states that the\n",
    "optimal policy would visit is small\n",
    "then we'll get close to the optimal\n",
    "policy\n",
    "empirically this method does very well\n",
    "it outperforms regular mbpo it also\n",
    "outperforms\n",
    "quite a few of the previously proposed\n",
    "policy constraint methods\n",
    "all right so now let me discuss the last\n",
    "algorithm i'm going to cover\n",
    "which is called conservative q learning\n",
    "conservative q learning takes a slightly\n",
    "different approach\n",
    "to offline rl from policy constraint\n",
    "methods so\n",
    "just to remind you the problem we saw\n",
    "before is that the q function kept\n",
    "thinking that it's going to do much\n",
    "better than it's actually going to do\n",
    "so it's going to overestimate like this\n",
    "because when we take the r\n",
    "max we get the point with the largest\n",
    "positive error\n",
    "so instead of trying to constrain the\n",
    "policy what if we try to directly fix\n",
    "the problem what if we directly push\n",
    "down on erroneously high q values\n",
    "one of the ways we could do this is we\n",
    "can formulate a q\n",
    "learning objective with a penalty so we\n",
    "have the usual term which says\n",
    "minimize development error and then we\n",
    "have a penalty term which says\n",
    "pick the actions with the high q values\n",
    "and push down on those q values so\n",
    "minimize the q values under this\n",
    "distribution mu\n",
    "which is chosen so that the q values\n",
    "under mu are high\n",
    "so this will basically find these\n",
    "erroneous positive points and push them\n",
    "down\n",
    "it turns out that with this very simple\n",
    "regularizer we can actually prove\n",
    "that the q function we learn is a lower\n",
    "bound on the true q function for the\n",
    "policy pi\n",
    "if you choose alpha the weight on the\n",
    "regularizer appropriately\n",
    "that's pretty cool we're actually\n",
    "guaranteed not to overestimate if we do\n",
    "this\n",
    "um so this is work that was primarily\n",
    "led by my student averal kumar\n",
    "and the particular algorithm that\n",
    "overall proposed is a kind of actor\n",
    "critic style algorithm\n",
    "where you learn the slower bound q\n",
    "function for the current policy\n",
    "uh so that q hat is less than or equal\n",
    "to q\n",
    "and then you update your policy now you\n",
    "don't actually need to represent the\n",
    "policy exactly it's just a little\n",
    "explicitly you can still have a q\n",
    "learning algorithm where it's implicit\n",
    "where it's the r max policy\n",
    "but it's a little easier to explain in\n",
    "active critics setting we call this\n",
    "conservative q learning though because\n",
    "it's also very simple to instantiate as\n",
    "a q learning method\n",
    "now you can also derive a much better\n",
    "bound for conservative q learning the\n",
    "bound i had on the previous page\n",
    "was actually too conservative it\n",
    "actually pushed down the q values too\n",
    "much\n",
    "what you can do is you can push down on\n",
    "the high q values\n",
    "but you can compensate by also pushing\n",
    "up on the q values in the data set\n",
    "so you push down on the q values of the\n",
    "q function thinks are high\n",
    "and then you push up on the q values uh\n",
    "for the actions in the data set\n",
    "intuitively what this means that is that\n",
    "if the high q values are all for actions\n",
    "that are in the data set\n",
    "then these right two terms will cancel\n",
    "out and the regularizer goes away so\n",
    "when you're in distribution\n",
    "there is no regularization if you go\n",
    "more out of distribution you get more\n",
    "regularization\n",
    "so these are the two error terms push\n",
    "down on actions from you\n",
    "push up on actions from d from the\n",
    "dataset\n",
    "now you're no longer guaranteed to have\n",
    "a bound for all state action tuples if\n",
    "you do this\n",
    "but you are turns out still guaranteed\n",
    "to have a bound in expectation under\n",
    "your policy\n",
    "so the expected q value under pi for q\n",
    "hat will still be less than or equal to\n",
    "the expected value\n",
    "under the uh the true q function for pi\n",
    "for all the states provided that alpha\n",
    "is chosen appropriately of course\n",
    "so um the full bound uh\n",
    "since it's a the full conservative q\n",
    "learning algorithm is shown here you\n",
    "minimize the big q values and you\n",
    "maximize\n",
    "uh the q values under the data the full\n",
    "bound is written out here\n",
    "so the left side is the estimated value\n",
    "function it's less than or equal to the\n",
    "true value function\n",
    "minus a positive pessimism term due to\n",
    "the regularizer and then there's this\n",
    "error term that accounts for sampling\n",
    "error\n",
    "so you just have to choose alpha so that\n",
    "this positive pessimism term\n",
    "is larger than the positive sampling\n",
    "error term obviously if you have more\n",
    "data then your sampling error is lower\n",
    "and you don't have to worry as much if\n",
    "you have high sampling error then you\n",
    "need a higher alpha\n",
    "to be conservative\n",
    "okay so does this bound hold in practice\n",
    "one of the things we did is we actually\n",
    "empirically measured underestimation\n",
    "versus overestimation on a simple\n",
    "benchmark task\n",
    "so what i'm going to be showing is\n",
    "basically the expected value of the\n",
    "learned q function\n",
    "q hat minus the expected value of the\n",
    "true q function\n",
    "so if these values are negative that\n",
    "means that q hat is less than\n",
    "q and expectation if they're positive\n",
    "that means we're overestimating\n",
    "and we get the true q function by\n",
    "actually rolling out the policy many\n",
    "times and using monte carlo\n",
    "so here are the results so the first\n",
    "column shows this\n",
    "the full cql method with both the\n",
    "minimization and maximization term\n",
    "the second column shows just the basic\n",
    "method that has just the minimization\n",
    "but no maximization\n",
    "then we have four different ensembles so\n",
    "an ensemble of two networks four\n",
    "networks 10 networks and 28 networks\n",
    "and then we have bayer which is a\n",
    "representative policy constraint on it\n",
    "now the first thing that you might note\n",
    "is that all of the ensembles and the\n",
    "policy constraint method\n",
    "are overestimating massively despite the\n",
    "policy constraint\n",
    "the simple cq element that just has the\n",
    "minimization\n",
    "is underestimating but by quite a lot so\n",
    "rewards here are on the order of a few\n",
    "hundred\n",
    "so getting minus 150 means that you're\n",
    "very heavily underestimating\n",
    "whereas the full cql method\n",
    "underestimates but only by a little bit\n",
    "so we are having we do have a lower\n",
    "bound and the lower bound is pretty good\n",
    "so the cqr always has negative errors\n",
    "which means that it's pessimistic\n",
    "all right now before i tell you how well\n",
    "cql actually does empirically\n",
    "i want to talk a little bit about how we\n",
    "should evaluate offline rl methods\n",
    "so how should we evaluate them well uh\n",
    "ultimately what we're going to want to\n",
    "do is train our offline rl methods using\n",
    "large and highly diverse data sets\n",
    "but in the meantime when we're just\n",
    "prototyping algorithmic ideas\n",
    "we need to somehow collect uh you know\n",
    "some data to set up a benchmark task so\n",
    "maybe one thing we could do is we could\n",
    "collect some data using some online rl\n",
    "algorithm and then use it to evaluate\n",
    "offline rl\n",
    "this is actually pretty typical in prior\n",
    "work you train pi beta with online rl\n",
    "and then you either collect data\n",
    "throughout training or you take data\n",
    "from the final policy\n",
    "i'm going to claim that this is a really\n",
    "bad idea this is a really terrible way\n",
    "to evaluate offline reinforcement\n",
    "learning methods and if you're doing\n",
    "research on offline rl\n",
    "you should not use data sets that have\n",
    "just this kind of data\n",
    "because if you already have a good\n",
    "policy why bother with offline rl\n",
    "but perhaps more importantly in the real\n",
    "world that's not what your data is going\n",
    "to look like\n",
    "your data is not going to come from a\n",
    "markovian policy it's going to come from\n",
    "humans from many different sources from\n",
    "your past\n",
    "behavior it's going to be multitask it's\n",
    "going to be diverse it's not going to\n",
    "look like the replay buffer\n",
    "for an online url run so human users and\n",
    "engineer policies etc\n",
    "so if you really want to value your\n",
    "offline rl method you really have to use\n",
    "data that is representative\n",
    "of real-world settings and leaves plenty\n",
    "of room for improvement\n",
    "and then offline allows to learn\n",
    "policies that are much better than the\n",
    "behavior policy that are better than the\n",
    "best thing in your data set\n",
    "without testing for these properties you\n",
    "can't really trust\n",
    "that our offline rl algorithms are good\n",
    "and\n",
    "of course in past work from my group\n",
    "we've also been guilty of doing this but\n",
    "we're we're mending our ways we're not\n",
    "going to do this anymore\n",
    "so we developed a benchmark suite called\n",
    "d4rl it stands for data sets for data\n",
    "driven dprl\n",
    "this was led by my student justin food\n",
    "together with avril kumar\n",
    "uh of your not true truman george tucker\n",
    "and d4rl is actually rapidly uh you know\n",
    "picking up it's rapidly becoming the\n",
    "most popular benchmark for offline rl\n",
    "because it really exercises the kinds of\n",
    "properties you want in offline rl\n",
    "algorithms\n",
    "so what are some important principles to\n",
    "keep in mind well you want data from\n",
    "non-rl policies\n",
    "including data from humans so we\n",
    "included things like the dexterous\n",
    "manipulation data that i showed before\n",
    "this was based on data collected by\n",
    "argentoswar and colleagues\n",
    "you want to evaluate whether your\n",
    "algorithm can put together different\n",
    "parts of different trajectories we'll\n",
    "call this stitching\n",
    "so if you've seen for example you can go\n",
    "from a to b and you've seen that you can\n",
    "go from b to c\n",
    "but it's more optimal to go from a to c\n",
    "the data actually tells you everything\n",
    "you need to know to figure that out\n",
    "so you should be able to do this and do\n",
    "better than the best trajectory in the\n",
    "data set\n",
    "uh so we evaluate this using some maze\n",
    "navigation tasks both in a simple low\n",
    "dimensional 2d setting\n",
    "and a complex setting where you've got a\n",
    "simulated four-legged robot actually\n",
    "walk through the maze\n",
    "so you never see full paths from the\n",
    "start to the goal but you see paths\n",
    "between different places in the maze in\n",
    "your data center\n",
    "and you also have to have realistic\n",
    "tasks we included first person driving\n",
    "data from images using the carlos\n",
    "simulator\n",
    "data manipulating objects in a kitchen\n",
    "from paper by abhishek gupta called\n",
    "relay policy learning\n",
    "and traffic control data from professor\n",
    "alex bines lab\n",
    "that simulates the effect of autonomous\n",
    "cars on traffic\n",
    "so the set of uh d4l tasks includes the\n",
    "standard\n",
    "mujoco gym tasks\n",
    "with some difficult data sets the\n",
    "stitching tasks and the mazes\n",
    "dexterous manipulation tasks with data\n",
    "from real humans\n",
    "robot manipulation tests in this kitchen\n",
    "environment again using human data\n",
    "traffic control data from a flow\n",
    "simulator and\n",
    "image-based driving in karla\n",
    "now if we look at how cql compares on\n",
    "this benchmark first let me show you the\n",
    "prior methods\n",
    "one of the things to note is on the\n",
    "harder benchmark tasks\n",
    "we actually see that first of all\n",
    "nothing works on the harder stitching\n",
    "task so\n",
    "on the larger mazes previous methods\n",
    "basically don't learn anything these\n",
    "scores are all normalized between 0 and\n",
    "100.\n",
    "the most competitive baseline across all\n",
    "of these harder tasks\n",
    "is just simple behavior cloning which\n",
    "suggests that previously proposed\n",
    "offline reinforcement learning methods\n",
    "which have primarily been tested\n",
    "on data from other rl policies are not\n",
    "actually doing very well\n",
    "so nothing beats behavior cloning on\n",
    "these harder tasks\n",
    "if we look at the performance of cql it\n",
    "achieves state-of-the-art results on\n",
    "nearly all of these tasks\n",
    "so i'm showing two variants of cql and\n",
    "one of these two variants\n",
    "is the best on all but one of the tasks\n",
    "or tied for the best so it's up to five\n",
    "times better on the harder dexterous\n",
    "manipulation tasks\n",
    "fifty uh to three hundred percent better\n",
    "on on the\n",
    "adjust the human data 10 to 30 better on\n",
    "the kitchen tasks\n",
    "and essentially infinitely better on the\n",
    "larger mazes where it's the only\n",
    "algorithm that's able to exhibit the\n",
    "stitching behavior\n",
    "we also evaluated the method on atari\n",
    "data from paper by agrowall at all and\n",
    "we saw there also that\n",
    "cql is 50 to 600 percent better uh than\n",
    "previously proposed algorithms\n",
    "so this method is doing really well it\n",
    "seems to work quite well across many\n",
    "tasks\n",
    "and we seem to know why it works because\n",
    "of this lower bound property\n",
    "of course there's still plenty of room\n",
    "for improvement so if you want to\n",
    "develop better offline rl methods\n",
    "there there's plenty of work to do and\n",
    "plenty of ways\n",
    "in which you can improve the results all\n",
    "right\n",
    "so just to wrap up and conclude i talked\n",
    "about how offline rl\n",
    "is quite difficult but has enormous\n",
    "promise and initial results suggest that\n",
    "it can be extremely powerful\n",
    "i talked about how effective dynamic\n",
    "programming offline rl methods can be\n",
    "implemented by imposing constraints on\n",
    "the policy and perhaps implicit\n",
    "constraints\n",
    "can get around the need to model the\n",
    "behavior policy and i talked about how\n",
    "this\n",
    "learning a lower bound on the q function\n",
    "using conservative q learning\n",
    "can substantially improve offline rl\n",
    "performance thank you very much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
