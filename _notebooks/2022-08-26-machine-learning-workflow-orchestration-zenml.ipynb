{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2cb6d7",
   "metadata": {},
   "source": [
    "# \"MLOps project - part 2b: Machine Learning Workflow Orchestration using ZenML\"\n",
    "> \"Machine learning workflow orchestration using ZenML.\"\n",
    "\n",
    "- toc: True\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [mlops]\n",
    "- image: images/some_folder/your_image.png\n",
    "- hide: false\n",
    "- search_exclude: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df8ecf",
   "metadata": {},
   "source": [
    "In the [previous blog post](https://kargarisaac.github.io/blog/mlops/2022/08/09/machine-learning-workflow-orchestration-prefect.html), we discussed how to use Prefect as a workflow orchestration in our MLOps project. In this blog post, we will see how ZenML can help us do the same job and maybe other stuff. Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c9408",
   "metadata": {},
   "source": [
    "# ZenML\n",
    "\n",
    "If you are an ML engineer or data scientist shipping models to production and jumbling many tools. It helps to do data, code, and model versioning. It also helps replicate production pipelines and monitoring models in production.\n",
    "\n",
    "![](images/workflow-orchestration/zenml.gif)\n",
    "*[source](https://docs.zenml.io/getting-started/introduction)*\n",
    "\n",
    "> ZenML is an extensible, open-source MLOps framework for creating portable, production-ready MLOps pipelines. It's built for data scientists, ML Engineers, and MLOps Developers to collaborate as they develop to production. ZenML has simple, flexible syntax, is cloud- and tool-agnostic, and has interfaces/abstractions that are catered towards ML workflows. ZenML brings together all your favorite tools in one place so you can tailor your workflow to cater your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95e7c7",
   "metadata": {},
   "source": [
    "Let's first get familiar with some of the ZenML's concepts briefly:\n",
    "\n",
    "**Pipelines and Steps**: ZenML follows a pipeline-based workflow. A pipeline consists of a series of steps, organized in any order that makes sense for your use case. You can have multiple pipelines for different purposes. For example, a training pipeline to train and evaluate models and an inference pipeline to serve the model. We can use decorators such as `@step` and `@pipeline` to define steps and pipelines in ZenML. \n",
    "\n",
    "**Stacks, Components, and Stores**: A Stack is the configuration of the underlying infrastructure and choices around how your pipeline will be run. For example, you can choose to run your pipeline locally or on the cloud by changing the stack you use.\n",
    "\n",
    "In any Stack, there must be at least three basic Stack Components -\n",
    "- *Orchestrator*: An Orchestrator is a workhorse that coordinates all the steps to run in a pipeline.  \n",
    "- *Artifact Store*: An Artifact Store is a component that houses all data that pass through the pipeline. Data in the artifact store are called artifacts.\n",
    "- *Metadata Store*: A Metadata Store keeps track of all the bits of extraneous data from a pipeline run. It allows you to fetch specific steps from your pipeline run and their output artifacts in a post-execution workflow.\n",
    "\n",
    "ZenML also has some other stack components that can help us scale up our stack to run elsewhere, for example, on a cloud with powerful GPUs for training or CPU's for deployment. You can check [here](https://docs.zenml.io/mlops-stacks/categories) for the full list of these stack components.\n",
    "\n",
    "with ZenML, we can easily switch our stack from running on a local machine to running on the cloud with a single CLI command.\n",
    "Our code (steps and pipelines) stays the same. The only change is in the stack and its components. This is amazing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa244fc",
   "metadata": {},
   "source": [
    "Let's get back to our example and apply ZenML! In the previous blog post, we saw our simple pipeline:\n",
    "\n",
    "![](images/workflow-orchestration/5.png)\n",
    "\n",
    "Your ML workflows may be a lot more complex. The performance of various models will need to be compared, they will need to be deployed in a production environment, and there may be extensive preprocessing that you do not want to repeat every time you train a model. ML pipelines are useful in this situation since they let us describe our workflows as a series of interchangeable modules.\n",
    "\n",
    "[ZenML's caching function](https://docs.zenml.io/developer-guide/steps-and-pipelines/caching), which is turned on by default, is another strong feature I liked. As long as the inputs, outputs, and parameters of steps and pipelines are tracked and versioned automatically by ZenML, the pipeline's steps won't be repeated when the pipeline is performed again. This drastically shortens the development period. You can enable/disable it for the whole pipeline or individual steps.\n",
    "\n",
    "Let's start adding ZenML to our code in a local development environment.\n",
    "\n",
    "\n",
    "## ZenML on Local Development Environment\n",
    "\n",
    "You first need to install ZenML:\n",
    "\n",
    "```bash\n",
    "pip install zenml\n",
    "pip install \"zenml[stacks]==0.13.1\"\n",
    "```\n",
    "\n",
    "Once the installation is completed, you can go ahead and create your first ZenML repository for your project:\n",
    "\n",
    "```bash\n",
    "zenml init\n",
    "```\n",
    "\n",
    "Then, we need to define each step as a ZenML **[Pipeline Step](https://docs.zenml.io/developer-guide/steps-and-pipelines#step)** simply by having several functions with ZenML's `@step` [Python decorator](https://realpython.com/primer-on-python-decorators/).\n",
    "We can also use ZenML's `@pipeline` decorator to define a pipeline and connect all the steps into an ML pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa1b9f5",
   "metadata": {},
   "source": [
    "One thing that I spent some time was the input and output type definition for steps. Especially custom types like the tokenizer. We need to tell ZenML somehow about this custom type. There is a solution called [`Materializer`](https://docs.zenml.io/developer-guide/advanced-usage/materializer#using-a-custom-materializer) in ZenML. You can see the defined custom materializer for our TensorFlow Tokenizer and its use in the following code sections.\n",
    "\n",
    "You need to pass the materializer to the step via `.with_return_materializers({\"tokenizer\": TokenizerMaterializer})` in the pipeline definition. You will see this in a few moments.\n",
    "\n",
    "You can save the tokenizer in the `create_dataset` step without returning it, but I do this just to show you how you can have a custom output from a step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beacca84",
   "metadata": {},
   "source": [
    "In addition, we need to integrate MLflow in ZenML for experiment tracking. It is a bit different from how we did it with Prefect. MLflow can handle various ML lifecycle steps, such as experiment tracking, code packaging, model deployment, and more. In this lesson, we will focus on the [MLflow Tracking](https://mlflow.org/docs/latest/tracking.html) component, but we will learn about other MLflow features in later posts.\n",
    "\n",
    "First, install MLflow ZenML integration:\n",
    "\n",
    "```bash\n",
    "zenml integration install mlflow -y\n",
    "```\n",
    "\n",
    "Then, by adding an `@enable_mlflow` decorator on top of the function, ZenML automatically initializes MLflow, and we can log what we want. In this post, we do not do hyperparameter tuning and set the hyperparameters inside the training step. At the time of writing this blog post, which is ZenML 0.13.1, there is no straight way to do hyperparameter tuning in ZenML. If you use the `@enable_mlflow` decorator, it will consider the whole step as one run. so you cannot have multiple MLflow runs in one single step. You cannot also pass hyperparameters as step arguments as they are not the output of a previous step. The ZenML team is working on this feature, and I hope to have this feature soon.\n",
    "\n",
    "You can also use MLflow without this decorator and in a normal way, like what we did in the previous post, by using the following two lines in the training step:\n",
    "\n",
    "\n",
    "```python\n",
    "mlflow.set_tracking_uri(<tracking uri>)\n",
    "mlflow.set_experiment(\"customer-sentiment-analysis\")\n",
    "```\n",
    "\n",
    "You can get the tracking URI by using `zenml experiment-tracker describe`. This way, you can use MLflow normally without letting ZenML handle it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0485b851",
   "metadata": {},
   "source": [
    "To run our MLflow pipelines with ZenML, we first need to add MLflow into our ZenML MLOps stack. We first register a new experiment tracker with ZenML and then add it to our current stack. To set the `tracking_uri` for MLflow in ZenML, you need to do it as follows, which is a bit different from what we did before and how you do it with pure MLflow. Also, setting the experiment name in ZenML 0.13.1 is not possible, and it uses a default name which is the name of the function you use for training. We will see it in a few moments. I hope they add it in future versions.\n",
    "\n",
    "\n",
    "```bash\n",
    "# Register the MLflow experiment tracker\n",
    "zenml experiment-tracker register mlflow_tracker --flavor=mlflow --tracking_uri=\"sqlite:///mlflow.db\"\n",
    "\n",
    "# Add the MLflow experiment tracker into our default stack\n",
    "zenml stack update default -e mlflow_tracker\n",
    "```\n",
    "\n",
    "After all of the above steps, our local stack, named `Default`, would be as the following image:\n",
    "\n",
    "![](images/workflow-orchestration/zenml-local.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1474ea",
   "metadata": {},
   "source": [
    "Any MLOps stack's foundation comprises artifact stores, metadata stores, and orchestrators since they allow us to save, distribute, and reproduce our work. Without them, it's simple to lose sight of the specific steps to develop our present ML pipelines.\n",
    "\n",
    "All artifacts are automatically stored in an Artifact Store in our ML process. By default, this is just a location in your local file system, but we can enable ZenML to store this information in a cloud bucket instead, such as an Amazon S3 or Google Cloud storage bucket.\n",
    "\n",
    "ZenML automatically stores Metadata, such as the item's location in a Metadata Store, together with the artifact itself. By default, this is an SQLite database on your local computer, but we could just as easily replace it with a cloud service.\n",
    "The orchestrator, which manages your machine learning pipelines, is a crucial part of any MLOps architecture. The orchestrator offers a setting that is ready to carry out the pipeline's steps to accomplish this. Additionally, it ensures that the pipeline's steps are only carried out once all of its inputs, which are the results of earlier processes, have been made available.\n",
    "\n",
    "In this project, we also added MLFlow as the experiment tracker to our stack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ab692",
   "metadata": {},
   "source": [
    "As I mentioned, when you use `@enable_mlflow` decorator for a step, it will be one single run, and you cannot have multiple runs in that step. You need to call the step multiple times if you want to test different sets of hyperparameters and do a hyperparameter search. The other problem you may face is that you cannot pass parameters to a step if it is not the result of a previous step. You can do the same trick as [this example](https://github.com/zenml-io/zenbytes/blob/main/2-1_Experiment_Tracking.ipynb).\n",
    "\n",
    "Our final code would be like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f11cac7",
   "metadata": {},
   "source": [
    "```python\n",
    "# train_dl_zenml.py file\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import pickle\n",
    "from zenml.steps import step, Output\n",
    "from zenml.pipelines import pipeline\n",
    "from zenml.artifacts import DataArtifact\n",
    "from zenml.io import fileio\n",
    "from zenml.materializers.base_materializer import BaseMaterializer\n",
    "from zenml.integrations.mlflow.mlflow_step_decorator import enable_mlflow\n",
    "from zenml.config.docker_configuration import DockerConfiguration\n",
    "from zenml.steps import step, ResourceConfiguration\n",
    "from typing import Type\n",
    "\n",
    "nltk.download('stopwords')\n",
    "DATA_PATH = \"data/Womens Clothing E-Commerce Reviews.csv\"\n",
    "\n",
    "\n",
    "# materializer for TF tokenizer (custom) inputs and outputs\n",
    "class TokenizerMaterializer(BaseMaterializer):\n",
    "    ASSOCIATED_TYPES = (Tokenizer,)\n",
    "    ASSOCIATED_ARTIFACT_TYPES = (DataArtifact,)\n",
    "\n",
    "    def handle_input(self, data_type: Type[Tokenizer]) -> Tokenizer:\n",
    "        \"\"\"Read from artifact store\"\"\"\n",
    "        super().handle_input(data_type)\n",
    "        with fileio.open(os.path.join(self.artifact.uri, 'tokenizer.pickle'), 'rb') as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        return tokenizer\n",
    "\n",
    "    def handle_return(self, tokenizer: Tokenizer) -> None:\n",
    "        \"\"\"Write to artifact store\"\"\"\n",
    "        super().handle_return(tokenizer)\n",
    "        with fileio.open(os.path.join(self.artifact.uri, 'tokenizer.pickle'), 'wb') as f:\n",
    "            pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "## data loading\n",
    "@step\n",
    "def read_data() -> pd.DataFrame:\n",
    "    data = pd.read_csv(DATA_PATH, index_col =[0])\n",
    "    print(\"Data loaded.\\n\\n\")\n",
    "    return data\n",
    "\n",
    "## preprocess text\n",
    "@step\n",
    "def preprocess_data(\n",
    "    data: pd.DataFrame,\n",
    "    ) -> Output(corpus=np.ndarray, y=np.ndarray):\n",
    "    data = data[~data['Review Text'].isnull()]  #Dropping columns which don't have any review\n",
    "    X = data[['Review Text']]\n",
    "    X.index = np.arange(len(X))\n",
    "\n",
    "    y = data['Recommended IND'].values\n",
    "\n",
    "    corpus =[]\n",
    "    for i in range(len(X)):\n",
    "        review = re.sub('[^a-zA-z]',' ',X['Review Text'][i])\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "        ps = PorterStemmer()\n",
    "        review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]\n",
    "        review =' '.join(review)\n",
    "        corpus.append(review)\n",
    "\n",
    "    print(\"Data preprocessed.\\n\\n\")\n",
    "\n",
    "    return np.array(corpus), y\n",
    "\n",
    "# tokenization and dataset creation\n",
    "@step\n",
    "def create_dataset(\n",
    "    corpus: np.ndarray, \n",
    "    y: np.ndarray\n",
    "    ) -> Output(X_train=np.ndarray, X_test=np.ndarray, y_train=np.ndarray, y_test=np.ndarray, tokenizer=Tokenizer):\n",
    "\n",
    "    tokenizer = Tokenizer(num_words = 3000)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(corpus)\n",
    "    padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(padded, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    print(\"Dataset created.\\n\\n\")\n",
    "    return X_train, X_test, y_train, y_test, tokenizer\n",
    "\n",
    "@enable_mlflow\n",
    "@step\n",
    "def train_model(\n",
    "    X_train: np.ndarray, \n",
    "    y_train: np.ndarray, \n",
    "    X_test: np.ndarray, \n",
    "    y_test: np.ndarray, \n",
    "    tokenizer: Tokenizer\n",
    "    ) -> None:\n",
    "    \n",
    "    mlflow.tensorflow.autolog()\n",
    "    \n",
    "    embedding_dim = 32 \n",
    "    batch_size = 64 \n",
    "\n",
    "    # model definition\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(3000, embedding_dim),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(6, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    ## training\n",
    "    num_epochs = 2\n",
    "    callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=0,\n",
    "        patience=2,\n",
    "        verbose=0,\n",
    "        mode=\"auto\",\n",
    "        baseline=None,\n",
    "        restore_best_weights=False,\n",
    "    )\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    mlflow.set_tag(\"developer\", \"Isaac\")\n",
    "    mlflow.set_tag(\"algorithm\", \"Deep Learning\")\n",
    "    mlflow.log_param(\"train-data\", \"Womens Clothing E-Commerce Reviews\")\n",
    "    mlflow.log_param(\"embedding-dim\", embedding_dim)\n",
    "\n",
    "    print(\"Fit model on training data\")\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        callbacks=callback,\n",
    "        # We pass some validation for\n",
    "        # monitoring validation loss and metrics\n",
    "        # at the end of each epoch\n",
    "        validation_data=(X_test, y_test),\n",
    "    )\n",
    "\n",
    "    ## save model and tokenizer\n",
    "    mlflow.keras.log_model(model, 'models/model_dl')\n",
    "\n",
    "    with open('tf_tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    mlflow.log_artifact(local_path=\"tf_tokenizer.pickle\", artifact_path=\"tokenizer_pickle\")\n",
    "\n",
    "    print(\"Model training completed.\\n\\n\")\n",
    "\n",
    "@pipeline\n",
    "def training_pipeline(\n",
    "    reading_data,\n",
    "    preprocessing_data,\n",
    "    creating_dataset,\n",
    "    training_model,\n",
    "):\n",
    "    data = reading_data()\n",
    "    corpus, y = preprocessing_data(data)\n",
    "    X_train, X_test, y_train, y_test, tokenizer = creating_dataset(corpus, y)\n",
    "    training_model(X_train, y_train, X_test, y_test, tokenizer)\n",
    "\n",
    "if __name__ == '__main__': \n",
    "\n",
    "    training_pipeline(\n",
    "        reading_data=read_data(),\n",
    "        preprocessing_data=preprocess_data(),\n",
    "        creating_dataset=create_dataset().with_return_materializers({\"tokenizer\": TokenizerMaterializer}),\n",
    "        training_model=train_model(),\n",
    "    ).run()\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527a4d9",
   "metadata": {},
   "source": [
    "You can then run the code using the following command, which runs the pipeline on the local stack:\n",
    "\n",
    "```bash\n",
    "python train_dl_zenml.py\n",
    "```\n",
    " \n",
    "\n",
    "It will run the pipeline on the local `Default` stack. You can run MLflow UI using the following command:\n",
    "\n",
    "```bash\n",
    "mlflow ui --backend-store-uri sqlite:///mlflow.db\n",
    "```\n",
    "\n",
    "Here is the result of training for different set of hyperparameters on MLflow UI:\n",
    "\n",
    "![](images/workflow-orchestration/7.png)\n",
    "\n",
    "\n",
    "Running the pipeline on the local machine is easy. But what if we want to run our pipeline on the cloud to be able to use powerful computation there? What if our training step needs a powerful GPU? We can easily create a ZenML stack on a cloud environment like Google Cloud and switch the stack to run the pipeline without changing our code and by just using a single command. That's amazing. Isn't it?!! Let's see how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59673f5f",
   "metadata": {},
   "source": [
    "## ZenML on GCP\n",
    "\n",
    "Now let's see how to deploy this pipeline on Cloud. I will do it on GCP here, but other clouds are almost the same.\n",
    "\n",
    "You can have a simple MLOps stack ready for running your machine learning workloads on GCP. To do this, you can follow the steps [here](https://github.com/zenml-io/mlops-stacks/tree/main/vertex-ai). It uses [Terraform](https://www.terraform.io/) and sets up the following resources:\n",
    "\n",
    "- A Vertex AI enabled workspace as an [orchestrator](https://docs.zenml.io/mlops-stacks/orchestrators) to which you can submit your pipelines.\n",
    "- A service account with all the necessary permissions to execute your pipelines.\n",
    "- A GCS bucket as an [artifact store](https://docs.zenml.io/mlops-stacks/artifact-stores), which can be used to store all your ML artifacts like the model, checkpoints, etc. \n",
    "- A CloudSQL instance as a [metadata store](https://docs.zenml.io/mlops-stacks/metadata-stores) that is essential to track all your metadata and its location in your artifact store.  \n",
    "- A Container Registry repository as [container registry](https://docs.zenml.io/mlops-stacks/container-registries) for hosting your docker images.\n",
    "- A [secrets manager](https://docs.zenml.io/mlops-stacks/secrets-managers) enabled for storing your secrets. \n",
    "- An optional MLflow Tracking server deployed on a GKE cluster as an [experiment tracker](https://docs.zenml.io/mlops-stacks/experiment-trackers). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf700d9",
   "metadata": {},
   "source": [
    "Note that the terraform will create the required service account and enable required APIs on GCP.\n",
    "\n",
    "Follow the steps below to build the infrastructure for running your pipeline on GCP using vertex-ai:\n",
    "\n",
    "- Create a new project on GCP\n",
    "\n",
    "- Set project in gcloud cli \n",
    "\n",
    "```bash\n",
    "gcloud config set project <new project id>\n",
    "```\n",
    "\n",
    "- Pull zenml vertex-ai recipe  \n",
    "\n",
    "```bash\n",
    "zenml stack recipe pull vertex-ai\n",
    "```\n",
    "\n",
    "- Customize your deployment by editing the default values in the `locals.tf` file like `project_id`, `region`, `gcs` name, `prefix`, etc.\n",
    "\n",
    "- Add your secret information like keys and passwords into the `values.tfvars.json` file, which is not committed and only exists locally.\n",
    "\n",
    "- Deploy using zenml \n",
    "\n",
    "```bash\n",
    "zenml stack recipe deploy vertex-ai\n",
    "```\n",
    "\n",
    "- Create the ZenML stack. You may need to install gcp integration using `zenml integration install gcp` too.\n",
    "\n",
    "```bash\n",
    "zenml stack import vertex-ai zenml_stack_recipes/vertex-ai/vertex_stack_2022-08-26T07_02.yml\n",
    "```\n",
    "\n",
    "- You should now create a secret for the CloudSQL instance that will allow ZenML to connect to it. Use the following command:\n",
    "\n",
    "```bash\n",
    "zenml secrets-manager secret register gcp_mysql_secret --schema=mysql --user=<DB_USER> --password=<PWD> \\\n",
    "  --ssl_ca=@</PATH/TO/DOWNLOADED/SERVER-CERT> \\\n",
    "  --ssl_cert=@</PATH/TO/DOWNLOADED/CLIENT-CERT> \\\n",
    "  --ssl_key=@</PATH/TO/DOWNLOADED/CLIENT-KEY>\n",
    "```\n",
    "\n",
    "The values for the username and password can be obtained by running the following commands inside your recipe directory. (run them inside the folder with `.tf` files)\n",
    "\n",
    "```bash\n",
    "terraform output metadata-db-username\n",
    "terraform output metadata-db-password\n",
    "```\n",
    "\n",
    "For the certificates, visit the Google Cloud Console to [create a certificate and download the files](https://cloud.google.com/sql/docs/mysql/configure-ssl-instance#:~:text=Cloud%20SQL%20Instances-,To%20open%20the%20Overview%20page%20of%20an%20instance%2C%20click%20the,Click%20Create%20client%20certificate.) to your system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb5d9d",
   "metadata": {},
   "source": [
    "- You can destroy the deployed infrastructure using:\n",
    "\n",
    "\n",
    "```bash\n",
    "zenml stack recipe destroy vertex-ai\n",
    "zenml stack recipe clean\n",
    "```\n",
    "\n",
    "- You can also delete the stack using:\n",
    "\n",
    "\n",
    "```bash\n",
    "zenml stack delete <STACK_NAME>\n",
    "```\n",
    "\n",
    "- You can also delete components by:\n",
    "\n",
    "```bash\n",
    "zenml <STACK_COMPONENT> delete <STACK_COMPONENT_NAME>\n",
    "```\n",
    "\n",
    "\n",
    "In our case:\n",
    "\n",
    "```bash\n",
    "zenml artifact-store delete gcs_artifact_store\n",
    "zenml container-registry delete gcr_container_registry\n",
    "zenml metadata-store delete cloudsql_metadata_store\n",
    "zenml orchestrator delete vertex_ai_orchestrator\n",
    "zenml secrets-manager delete gcp_secrets_manager\n",
    "zenml experiment-tracker delete gke_mlflow_experiment_tracker\n",
    "```\n",
    "\n",
    "**Note**: If you provisioned infrastructure related to the stack, make sure to deprovision it using `zenml stack down --force` before unregistering the stack.\n",
    "\n",
    "\n",
    "Our stack, named `vertex-ai`, would be as follows:\n",
    "\n",
    "![](images/workflow-orchestration/zenml-cloud.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d2c518",
   "metadata": {},
   "source": [
    "The current MLflow integration in ZenML seems to use an SQLite database hosted on the VM where MLflow runs, and there is no remote backend store, only a remote artifact store. If you check your VM instances on Google Cloud Console, you will see a VM running all the time. This can be replaced with a serverless service like Cloud Run, and a SQL database can be added for the backend store for MLflow. They are working on it and hope to see these additions in the future releases of ZenML.\n",
    "\n",
    "Then you can use `zenml stack list` to see the list of stacks. If the newly created stack (`vertex-ai`) is not active, use `zenml stack set vertex-ai` to activate it.\n",
    "\n",
    "![](images/workflow-orchestration/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c9c9b6",
   "metadata": {},
   "source": [
    "You then should be able to run your pipeline on the new stack and on GCP. Note that you may need to do `python <path-to-python-env>\\Scripts\\pywin32_postinstall.py -install` if you see any error related to that. Also, make sure you have docker installed and running. \n",
    "\n",
    "The other point you need to take care of if you want to run your pipeline on the cloud is the package requirements for your code. One way is to create a `requirements.txt` file in your repo and set the path to it in your code. The other way is to pass these requirements as a list to `DockerConfiguration`. Check [here](https://docs.zenml.io/developer-guide/advanced-usage/docker#how-to-install-additional-pip-dependencies) to learn more.\n",
    "\n",
    "I do it as follows:\n",
    "\n",
    "```python\n",
    ".\n",
    ".\n",
    ".\n",
    "docker_config = DockerConfiguration(\n",
    "    requirements=[\n",
    "        \"nltk\",\n",
    "        \"tensorflow==2.9.1\",\n",
    "        \"scikit-learn\"\n",
    "    ],\n",
    "    dockerignore=\"./.dockerignore\"\n",
    ")\n",
    ".\n",
    ".\n",
    ".\n",
    "@pipeline(docker_configuration=docker_config)\n",
    ".\n",
    ".\n",
    ".\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc8a530",
   "metadata": {},
   "source": [
    "You can also config hardware requirements for each step using the `@step` decorator as follows:\n",
    "\n",
    "```python\n",
    "from zenml.steps import step, ResourceConfiguration\n",
    "\n",
    "@step(resource_configuration=ResourceConfiguration(cpu_count=8, gpu_count=2, memory=\"16GB\"))\n",
    "def my_step(...) -> ...:\n",
    "    ...\n",
    "```\n",
    "\n",
    "And here are the attributes of the `ResourceConfiguration`:\n",
    "```python\n",
    "Attributes:\n",
    "    cpu_count: The amount of CPU cores that should be configured.\n",
    "    gpu_count: The amount of GPUs that should be configured.\n",
    "    memory: The amount of memory that should be configured.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3530434f",
   "metadata": {},
   "source": [
    "Our final code would be like this:\n",
    "\n",
    "```python\n",
    "# train_dl_zenml.py file\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import pickle\n",
    "from zenml.steps import step, Output\n",
    "from zenml.pipelines import pipeline\n",
    "from zenml.artifacts import DataArtifact\n",
    "from zenml.io import fileio\n",
    "from zenml.materializers.base_materializer import BaseMaterializer\n",
    "from zenml.integrations.mlflow.mlflow_step_decorator import enable_mlflow\n",
    "from zenml.config.docker_configuration import DockerConfiguration\n",
    "from zenml.steps import step, ResourceConfiguration\n",
    "from typing import Type\n",
    "\n",
    "nltk.download('stopwords')\n",
    "DATA_PATH = \"data/Womens Clothing E-Commerce Reviews.csv\"\n",
    "\n",
    "# docker config\n",
    "docker_config = DockerConfiguration(\n",
    "    requirements=[\n",
    "        \"nltk\",\n",
    "        \"tensorflow==2.9.1\",\n",
    "        \"scikit-learn\"\n",
    "    ],\n",
    "    dockerignore=\"./.dockerignore\"\n",
    "    )\n",
    "\n",
    "# materializer for TF tokenizer (custom) inputs and outputs\n",
    "class TokenizerMaterializer(BaseMaterializer):\n",
    "    ASSOCIATED_TYPES = (Tokenizer,)\n",
    "    ASSOCIATED_ARTIFACT_TYPES = (DataArtifact,)\n",
    "\n",
    "    def handle_input(self, data_type: Type[Tokenizer]) -> Tokenizer:\n",
    "        \"\"\"Read from artifact store\"\"\"\n",
    "        super().handle_input(data_type)\n",
    "        with fileio.open(os.path.join(self.artifact.uri, 'tokenizer.pickle'), 'rb') as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        return tokenizer\n",
    "\n",
    "    def handle_return(self, tokenizer: Tokenizer) -> None:\n",
    "        \"\"\"Write to artifact store\"\"\"\n",
    "        super().handle_return(tokenizer)\n",
    "        with fileio.open(os.path.join(self.artifact.uri, 'tokenizer.pickle'), 'wb') as f:\n",
    "            pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "## data loading\n",
    "@step\n",
    "def read_data() -> pd.DataFrame:\n",
    "    data = pd.read_csv(DATA_PATH, index_col =[0])\n",
    "    print(\"Data loaded.\\n\\n\")\n",
    "    return data\n",
    "\n",
    "## preprocess text\n",
    "@step(resource_configuration=ResourceConfiguration(cpu_count=16, memory=\"16GB\"))\n",
    "def preprocess_data(\n",
    "    data: pd.DataFrame,\n",
    "    ) -> Output(corpus=np.ndarray, y=np.ndarray):\n",
    "    data = data[~data['Review Text'].isnull()]  #Dropping columns which don't have any review\n",
    "    X = data[['Review Text']]\n",
    "    X.index = np.arange(len(X))\n",
    "\n",
    "    y = data['Recommended IND'].values\n",
    "\n",
    "    corpus =[]\n",
    "    for i in range(len(X)):\n",
    "        review = re.sub('[^a-zA-z]',' ',X['Review Text'][i])\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "        ps = PorterStemmer()\n",
    "        review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]\n",
    "        review =' '.join(review)\n",
    "        corpus.append(review)\n",
    "\n",
    "    print(\"Data preprocessed.\\n\\n\")\n",
    "\n",
    "    return np.array(corpus), y\n",
    "\n",
    "# tokenization and dataset creation\n",
    "@step(resource_configuration=ResourceConfiguration(cpu_count=8, memory=\"16GB\"))\n",
    "def create_dataset(\n",
    "    corpus: np.ndarray, \n",
    "    y: np.ndarray\n",
    "    ) -> Output(X_train=np.ndarray, X_test=np.ndarray, y_train=np.ndarray, y_test=np.ndarray, tokenizer=Tokenizer):\n",
    "\n",
    "    tokenizer = Tokenizer(num_words = 3000)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(corpus)\n",
    "    padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(padded, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    print(\"Dataset created.\\n\\n\")\n",
    "    return X_train, X_test, y_train, y_test, tokenizer\n",
    "\n",
    "@enable_mlflow\n",
    "@step(resource_configuration=ResourceConfiguration(cpu_count=8, memory=\"16GB\"))\n",
    "def train_model(\n",
    "    X_train: np.ndarray, \n",
    "    y_train: np.ndarray, \n",
    "    X_test: np.ndarray, \n",
    "    y_test: np.ndarray, \n",
    "    tokenizer: Tokenizer\n",
    "    ) -> None:\n",
    "    \n",
    "    mlflow.tensorflow.autolog()\n",
    "    \n",
    "    embedding_dim = 32\n",
    "    batch_size = 64\n",
    "\n",
    "    # model definition\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(3000, embedding_dim),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(6, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    ## training\n",
    "    num_epochs = 2\n",
    "    callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=0,\n",
    "        patience=2,\n",
    "        verbose=0,\n",
    "        mode=\"auto\",\n",
    "        baseline=None,\n",
    "        restore_best_weights=False,\n",
    "    )\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    mlflow.set_tag(\"developer\", \"Isaac\")\n",
    "    mlflow.set_tag(\"algorithm\", \"Deep Learning\")\n",
    "    mlflow.log_param(\"train-data\", \"Womens Clothing E-Commerce Reviews\")\n",
    "    mlflow.log_param(\"embedding-dim\", embedding_dim)\n",
    "\n",
    "    print(\"Fit model on training data\")\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        callbacks=callback,\n",
    "        # We pass some validation for\n",
    "        # monitoring validation loss and metrics\n",
    "        # at the end of each epoch\n",
    "        validation_data=(X_test, y_test),\n",
    "    )\n",
    "\n",
    "    ## save model and tokenizer\n",
    "    mlflow.keras.log_model(model, 'models/model_dl')\n",
    "\n",
    "    with open('tf_tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    mlflow.log_artifact(local_path=\"tf_tokenizer.pickle\", artifact_path=\"tokenizer_pickle\")\n",
    "\n",
    "    print(\"Model training completed.\\n\\n\")\n",
    "\n",
    "@pipeline(docker_configuration=docker_config)\n",
    "def training_pipeline(\n",
    "    reading_data,\n",
    "    preprocessing_data,\n",
    "    creating_dataset,\n",
    "    training_model,\n",
    "):\n",
    "    data = reading_data()\n",
    "    corpus, y = preprocessing_data(data)\n",
    "    X_train, X_test, y_train, y_test, tokenizer = creating_dataset(corpus, y)\n",
    "    training_model(X_train, y_train, X_test, y_test, tokenizer)\n",
    "\n",
    "if __name__ == '__main__': \n",
    "\n",
    "    training_pipeline(\n",
    "        reading_data=read_data(),\n",
    "        preprocessing_data=preprocess_data(),\n",
    "        creating_dataset=create_dataset().with_return_materializers({\"tokenizer\": TokenizerMaterializer}),\n",
    "        training_model=train_model(),\n",
    "    ).run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71276b7a",
   "metadata": {},
   "source": [
    "Then you can run your pipeline:\n",
    "\n",
    "\n",
    "```bash\n",
    "python train_dl_zenml.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc675b",
   "metadata": {},
   "source": [
    "This will run the pipeline on Vertex AI, and you should be able to see all the steps executed successfully on the Vertex AI pipelines section on GCP. It will be as the following image:\n",
    "\n",
    "![](images/workflow-orchestration/9.png)\n",
    "\n",
    "\n",
    "You can do many things with steps. You can read data from different sources like GCS or BigQuery, or any other source. You can do data processing and ETL using different tools like Spark. You can train even huge deep learning models using GPU on Vertex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684976a8",
   "metadata": {},
   "source": [
    "To see the MLflow UI, you can use the `zenml experiment-tracker describe` and will see sth like this:\n",
    "\n",
    "![](images/workflow-orchestration/10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddcb24c",
   "metadata": {},
   "source": [
    "You can follow the `TRACKING_URI` and `TRACKING_USERNAME` and `TRACKING_PASSWORD` to access the MLflow UI. you can also check the Cloud Storage to see the MLflow artifacts:\n",
    "\n",
    "![](images/workflow-orchestration/11.png)\n",
    "\n",
    "\n",
    "Finally, if you want to run this pipeline on `vertex-ai` stack periodically and based on a schedule, you can containerize the code and deploy it as a Cloud Run microservice and then use [Google Cloud Scheduler](https://cloud.google.com/scheduler) to trigger it. Something like the following architecture: \n",
    "\n",
    "![](images/workflow-orchestration/8.jpg)\n",
    "\n",
    "\n",
    "The scheduler will trigger the cloud run service to start running the pipeline on the `vertex-ai` stack on the GCP-provisioned infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c310ef",
   "metadata": {},
   "source": [
    "That's it for this post. I may update this post if ZenML adds some interesting features. \n",
    "As the final thought, I must admit that I loved ZenML. It's much more than defining a machine learning pipeline and helps you integrate different MLOps tools in your stack. It makes all the steps and MLOps tools easy to use. I decided to continue using it in this project and also in SUPPLYZ.eu stack.\n",
    "\n",
    "In the next blog post, we will go for deploying the trained model. See you there.\n",
    "\n",
    "Here are some resources I used:\n",
    "\n",
    "- [vertex-ai recipe on ZenML GitHub](https://github.com/zenml-io/mlops-stacks/tree/main/vertex-ai)\n",
    "\n",
    "    \n",
    "> youtube: https://youtu.be/qgvmvexGv_c\n",
    "\n",
    "\n",
    "> youtube: https://youtu.be/lNnWaKjP3I4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290fb66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
