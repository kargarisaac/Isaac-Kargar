{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2cb6d7",
   "metadata": {},
   "source": [
    "# \"MLOps project - part 2b: Machine Learning Workflow Orchestration using ZenML\"\n",
    "> \"Machine learning workflow orchestration using ZenML.\"\n",
    "\n",
    "- toc: True\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [mlops]\n",
    "- image: images/some_folder/your_image.png\n",
    "- hide: false\n",
    "- search_exclude: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df8ecf",
   "metadata": {},
   "source": [
    "In the [previous blog post](https://kargarisaac.github.io/blog/mlops/2022/08/09/machine-learning-workflow-orchestration-prefect.html), we discussed how to use Prefect as a workflow orchestration in our MLOps project. In this blog post, we will see how ZenML can help us to do the same job and maybe other stuff too. Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c9408",
   "metadata": {},
   "source": [
    "# ZenML\n",
    "\n",
    "If you are a ML engineer or data scientist shipping models to production and jumbling a plethora of tools. It helps to do data, code, and models versioning. It also helps replicating production pipelines and monitoring models in production.\n",
    "\n",
    "![](images/workflow-orchestration/zenml.gif)\n",
    "*[source](https://docs.zenml.io/getting-started/introduction)*\n",
    "\n",
    "> ZenML is an extensible, open-source MLOps framework for creating portable, production-ready MLOps pipelines. It's built for data scientists, ML Engineers, and MLOps Developers to collaborate as they develop to production. ZenML has simple, flexible syntax, is cloud- and tool-agnostic, and has interfaces/abstractions that are catered towards ML workflows. ZenML brings together all your favorite tools in one place so you can tailor your workflow to cater your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95e7c7",
   "metadata": {},
   "source": [
    "Let's first get familiar with some of the ZenML's concepts briefly:\n",
    "\n",
    "**Pipelines and Steps**: ZenML follows a pipeline-based workflow. A pipeline consist of a series of steps, organized in any order that makes sense for your use case. You can have multiple pipelines for different purposes. For example, a training pipeline to train and evaluate models and an inference pipeline to serve the model. We can use decorators such as `@step` and `@pipeline` to define steps and pipelines in ZenML. \n",
    "\n",
    "**Stacks, Components, and Stores**: A Stack is the configuration of the underlying infrastructure and choices around how your pipeline will be run. For example, you can choose to run your pipeline locally or on the cloud by changing the stack you use.\n",
    "\n",
    "In any Stack, there must be at least three basic Stack Components -\n",
    "- *Orchestrator*: An Orchestrator is the workhorse that coordinates all the steps to run in a pipeline.  \n",
    "- *Artifact Store*: An Artifact Store is a component that houses all data that pass through the pipeline. Data in the artifact store are called artifacts.\n",
    "- *Metadata Store*: A Metadata Store keeps track of all the bits of extraneous data from a pipeline run. It allows you to fetch specific steps from your pipeline run and their output artifacts in a post-execution workflow.\n",
    "\n",
    "ZenML has also some other stack components which can help us to scale up our stack to run elsewhere, for example on a cloud with powerful GPUs for training or CPU's for deployment. You can check [here](https://docs.zenml.io/mlops-stacks/categories) for the full list of these stack components.\n",
    "\n",
    "with ZenML, we can easily switch our stack from running on a local machine to running on the cloud with a single CLI command.\n",
    "Our code (steps and pipelines) stay the same. The only change is in the stack and its components. This is amazing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa244fc",
   "metadata": {},
   "source": [
    "Let's get back to our example and apply ZenML! In the previous blog post, we saw our simple pipeline:\n",
    "\n",
    "![](images/workflow-orchestration/5.png)\n",
    "\n",
    "Your ML workflows may actually be a lot more complex. The performance of various models will need to be compared, they will need to be deployed in a production environment, and there may be extensive preprocessing that you do not want to repeat every time you train a model. In this situation, ML pipelines are useful since they let us describe our workflows as a series of interchangeable modules.\n",
    "\n",
    "[ZenML's caching function](https://docs.zenml.io/developer-guide/steps-and-pipelines/caching), which is turned on by default, is another strong feature. As long as the inputs, outputs, and parameters of steps and pipelines are tracked and versioned automatically by ZenML, the pipeline's steps won't be repeated when the pipeline is performed again. This drastically shortens the development period. You can enable/disable it for the whole pipeline or individual steps.\n",
    "\n",
    "In our ML process, all of the artifacts are automatically stored in an Artifact Store. By default, this is just a location in your local file system, but we can enable ZenML to store this information in a cloud bucket instead, such as an Amazon S3 or Google Cloud storage bucket.\n",
    "\n",
    "ZenML automatically stores Metadata, such as the location of the item in a Metadata Store, together with the artifact itself. By default, this is a SQLite database on your local computer, but we could just as easily replace it with a cloud service.\n",
    "\n",
    "Any MLOps stack's foundation is made up of artifact stores, metadata stores, and orchestrators since they allow us to save, distribute, and reproduce our work. Without them, it's simple to lose sight of the specific steps taken to develop our present ML pipelines.\n",
    "\n",
    "When invoking `pipeline.run()`, the Orchestrator specifies how and where each pipeline step will be carried out.\n",
    "\n",
    "\n",
    "## ZenML on Local Development Environment\n",
    "\n",
    "Let's now define each of the steps as a ZenML **[Pipeline Step](https://docs.zenml.io/developer-guide/steps-and-pipelines#step)** simply by having several functions with ZenML's `@step` [Python decorator](https://realpython.com/primer-on-python-decorators/) decorator.\n",
    "We can also use ZenML's `@pipeline` decorator to connect all of the steps into an ML pipeline.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa1b9f5",
   "metadata": {},
   "source": [
    "One thing that I spent some time was the input and output type definition for steps. Especially custom types like the tokenizer. We need to tell ZenML somehow about this custom type. There is a solution of rthat called [`Materializer`](https://docs.zenml.io/developer-guide/advanced-usage/materializer#using-a-custom-materializer) in ZenML. You can see the defined custom materializer for our TensorFlow Tokenizer and how to use it in the following code. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a7cc37",
   "metadata": {},
   "source": [
    "```python\n",
    "class TokenizerMaterializer(BaseMaterializer):\n",
    "    ASSOCIATED_TYPES = (Tokenizer,)\n",
    "    ASSOCIATED_ARTIFACT_TYPES = (DataArtifact,)\n",
    "\n",
    "    def handle_input(self, data_type: Type[Tokenizer]) -> Tokenizer:\n",
    "        \"\"\"Read from artifact store\"\"\"\n",
    "        super().handle_input(data_type)\n",
    "        with fileio.open(os.path.join(self.artifact.uri, 'tokenizer.pickle'), 'rb') as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        return tokenizer\n",
    "\n",
    "    def handle_return(self, tokenizer: Tokenizer) -> None:\n",
    "        \"\"\"Write to artifact store\"\"\"\n",
    "        super().handle_return(tokenizer)\n",
    "        with fileio.open(os.path.join(self.artifact.uri, 'tokenizer.pickle'), 'wb') as f:\n",
    "            pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c55af9",
   "metadata": {},
   "source": [
    "You then need to pass it to the step via `.with_return_materializers({\"tokenizer\": TokenizerMaterializer})` in the pipeline definition. You will see this in a few moment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e4dc4b",
   "metadata": {},
   "source": [
    "In addition, we need to integrate MLflow in ZenML. It is a bit different from how we did it with Prefect. First you need to install the integration:\n",
    "\n",
    "```bash\n",
    "zenml integration install mlflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beacca84",
   "metadata": {},
   "source": [
    "MLflow can handle various ML lifecycle steps, such as experiment tracking, code packaging, model deployment, and more. We will focus on the [MLflow Tracking](https://mlflow.org/docs/latest/tracking.html) component in this lesson, but we will learn about other MLflow features in later posts. \n",
    "\n",
    "First, install MLflow ZenML integration:\n",
    "\n",
    "```bash\n",
    "zenml integration install mlflow -y\n",
    "```\n",
    "\n",
    "Then, by adding an `@enable_mlflow` decorator on top of the function, ZenML then automatically initializes MLflow and we can log what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0485b851",
   "metadata": {},
   "source": [
    "to run our MLflow pipelines with ZenML, we first need to add MLflow into our ZenML MLOps stack. We first register a new experiment tracker with ZenML and then add it to our current stack. To set the `tracking_uri` for MLflow in ZenML, you need to do it as follows, which is a bit different from what we did before and how you do it with pure MLflow. Also, it seems setting the experiment name in ZenML 0.13 is not possible and it uses a default name which is the name of the function you use for training. We will see it in a few moments. In the documents, it says you can pass it to the decorator, but in the new version I get an error. I hope they add it in the future versions.\n",
    "\n",
    "\n",
    "```bash\n",
    "# Register the MLflow experiment tracker\n",
    "zenml experiment-tracker register mlflow_tracker --flavor=mlflow --tracking_uri=\"sqlite:///mlflow.db\"\n",
    "\n",
    "# Add the MLflow experiment tracker into our default stack\n",
    "zenml stack update default -e mlflow_tracker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11394053",
   "metadata": {},
   "source": [
    "and run mlflow UI using the following command:\n",
    "\n",
    "```bash\n",
    "mlflow ui --backend-store-uri sqlite:///mlflow.db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ab692",
   "metadata": {},
   "source": [
    "Note that when you use `@enable_mlflow` decorator for a step, it will be one single run and you cannot have multiple runs in that step. If you want to test different set of hyperparameters and do hyperparameter search, you need to call the step multiple times. The other problem you may face is that you cannot pass parameters to a step if it is not the result of a previous step. You can do the same trick as [this example](https://github.com/zenml-io/zenbytes/blob/main/2-1_Experiment_Tracking.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f11cac7",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "if os.path.exists('./corpora'):\n",
    "    os.environ[\"NLTK_DATA\"] = \"./corpora\"\n",
    "else:\n",
    "    nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import pickle\n",
    "from zenml.steps import step, Output\n",
    "from zenml.pipelines import pipeline\n",
    "from zenml.artifacts import DataArtifact\n",
    "from zenml.io import fileio\n",
    "from zenml.materializers.base_materializer import BaseMaterializer\n",
    "from zenml.integrations.mlflow.mlflow_step_decorator import enable_mlflow\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "DATA_PATH = \"data/Womens Clothing E-Commerce Reviews.csv\"\n",
    "\n",
    "\n",
    "class TokenizerMaterializer(BaseMaterializer):\n",
    "    ASSOCIATED_TYPES = (Tokenizer,)\n",
    "    ASSOCIATED_ARTIFACT_TYPES = (DataArtifact,)\n",
    "\n",
    "    def handle_input(self, data_type: Type[Tokenizer]) -> Tokenizer:\n",
    "        \"\"\"Read from artifact store\"\"\"\n",
    "        super().handle_input(data_type)\n",
    "        with fileio.open(os.path.join(self.artifact.uri, 'tokenizer.pickle'), 'rb') as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        return tokenizer\n",
    "\n",
    "    def handle_return(self, tokenizer: Tokenizer) -> None:\n",
    "        \"\"\"Write to artifact store\"\"\"\n",
    "        super().handle_return(tokenizer)\n",
    "        with fileio.open(os.path.join(self.artifact.uri, 'tokenizer.pickle'), 'wb') as f:\n",
    "            pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "## data loading\n",
    "@step\n",
    "def read_data() -> pd.DataFrame:\n",
    "    data = pd.read_csv(DATA_PATH, index_col =[0])\n",
    "    print(\"Data loaded.\\n\\n\")\n",
    "    return data\n",
    "\n",
    "## preprocess text\n",
    "@step\n",
    "def preprocess_data(\n",
    "    data: pd.DataFrame,\n",
    "    ) -> Output(corpus=list, y=np.ndarray):\n",
    "    #check if data/corpus is created before or not\n",
    "    if not os.path.exists('data/corpus_y.pickle'):\n",
    "        print(\"Preprocessed data not found. Creating new data. \\n\\n\")\n",
    "        data = data[~data['Review Text'].isnull()]  #Dropping columns which don't have any review\n",
    "        X = data[['Review Text']]\n",
    "        X.index = np.arange(len(X))\n",
    "\n",
    "        y = data['Recommended IND'].values\n",
    "\n",
    "        corpus =[]\n",
    "        for i in range(len(X)):\n",
    "            review = re.sub('[^a-zA-z]',' ',X['Review Text'][i])\n",
    "            review = review.lower()\n",
    "            review = review.split()\n",
    "            ps = PorterStemmer()\n",
    "            review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]\n",
    "            review =' '.join(review)\n",
    "            corpus.append(review)\n",
    "\n",
    "        with open('data/corpus_y.pickle', 'wb') as handle:\n",
    "            pickle.dump((corpus, y), handle)\n",
    "    else:\n",
    "        print(\"Preprocessed data found. Loading data. \\n\\n\")\n",
    "        with open('data/corpus_y.pickle', 'rb') as handle:\n",
    "            corpus, y = pickle.load(handle)\n",
    "\n",
    "    print(\"Data preprocessed.\\n\\n\")\n",
    "\n",
    "    return corpus, y\n",
    "\n",
    "## tokenization and dataset creation\n",
    "@step\n",
    "def create_dataset(\n",
    "    corpus: list, \n",
    "    y: np.ndarray\n",
    "    ) -> Output(X_train=np.ndarray, X_test=np.ndarray, y_train=np.ndarray, y_test=np.ndarray, tokenizer=Tokenizer):\n",
    "\n",
    "    tokenizer = Tokenizer(num_words = 3000)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(corpus)\n",
    "    padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(padded, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    print(\"Dataset created.\\n\\n\")\n",
    "    return X_train, X_test, y_train, y_test, tokenizer\n",
    "\n",
    "\n",
    "def build_dl_pipeline(hyperparams):\n",
    "    @enable_mlflow(nested=True)\n",
    "    @step(enable_cache=False)\n",
    "    def train_model(\n",
    "        X_train: np.ndarray, \n",
    "        y_train: np.ndarray, \n",
    "        X_test: np.ndarray, \n",
    "        y_test: np.ndarray, \n",
    "        tokenizer: Tokenizer\n",
    "        ) -> None:\n",
    "        \n",
    "        mlflow.tensorflow.autolog()\n",
    "        \n",
    "        embedding_dim = hyperparams['embedding_dim']\n",
    "        batch_size = hyperparams['batch_size']\n",
    "\n",
    "        # model definition\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(3000, embedding_dim),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(),\n",
    "            tf.keras.layers.Dense(6, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        ## training\n",
    "        num_epochs = 50\n",
    "        callback = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0,\n",
    "            patience=2,\n",
    "            verbose=0,\n",
    "            mode=\"auto\",\n",
    "            baseline=None,\n",
    "            restore_best_weights=False,\n",
    "        )\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "        mlflow.set_tag(\"developer\", \"Isaac\")\n",
    "        mlflow.set_tag(\"algorithm\", \"Deep Learning\")\n",
    "        mlflow.log_param(\"train-data\", \"Womens Clothing E-Commerce Reviews\")\n",
    "        mlflow.log_param(\"embedding-dim\", embedding_dim)\n",
    "\n",
    "        print(\"Fit model on training data\")\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=num_epochs,\n",
    "            callbacks=callback,\n",
    "            # We pass some validation for\n",
    "            # monitoring validation loss and metrics\n",
    "            # at the end of each epoch\n",
    "            validation_data=(X_test, y_test),\n",
    "        )\n",
    "\n",
    "        ## save model and tokenizer\n",
    "        mlflow.keras.log_model(model, 'models/model_dl')\n",
    "\n",
    "        with open('tokenizer_pickle/tf_tokenizer.pickle', 'wb') as handle:\n",
    "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        mlflow.log_artifact(local_path=\"tokenizer_pickle/tf_tokenizer.pickle\", artifact_path=\"tokenizer_pickle\")\n",
    "\n",
    "        # Evaluate the model on the test data using `evaluate`\n",
    "        print(\"Evaluate on test data\")\n",
    "        results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "        print(\"test loss, test acc:\", results)\n",
    "        mlflow.log_metric(\"loss\", results[0])\n",
    "        mlflow.log_metric(\"accuracy\", results[1])\n",
    "\n",
    "        print(\"Model training completed.\\n\\n\")\n",
    "\n",
    "    return training_pipeline(\n",
    "        reading_data=read_data(),\n",
    "        preprocessing_data=preprocess_data(),\n",
    "        creating_dataset=create_dataset().with_return_materializers({\"tokenizer\": TokenizerMaterializer}),\n",
    "        training_model=train_model(),\n",
    "    )\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def training_pipeline(\n",
    "    reading_data,\n",
    "    preprocessing_data,\n",
    "    creating_dataset,\n",
    "    training_model,\n",
    "):\n",
    "    data = reading_data()\n",
    "    corpus, y = preprocessing_data(data)\n",
    "    X_train, X_test, y_train, y_test, tokenizer = creating_dataset(corpus, y)\n",
    "    training_model(X_train, y_train, X_test, y_test, tokenizer)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    for embedding_dim, batch_size in zip([32, 64, 128], [32, 64, 128]):\n",
    "        hyperparams = {\n",
    "            'embedding_dim': embedding_dim,\n",
    "            'batch_size': batch_size\n",
    "        }\n",
    "        print(hyperparams)\n",
    "        build_dl_pipeline(hyperparams=hyperparams).run()    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59673f5f",
   "metadata": {},
   "source": [
    "The above will run the training for different set of hyperparameters and do the experiment tracking using MLflow locally. \n",
    "\n",
    "\n",
    "## ZenML on GCP\n",
    "\n",
    "Now let's see how to deploy this pipeline on Cloud. I will do it on GCP here, but other clouds are almost the same.\n",
    "\n",
    "You can have a simple MLOps stack ready for running your machine learning workloads. To do this, you can follow the steps [here](https://github.com/zenml-io/mlops-stacks/tree/main/vertex-ai). It sets up the following resources:\n",
    "\n",
    "- A Vertex AI enabled workspace as an [orchestrator](https://docs.zenml.io/mlops-stacks/orchestrators) that you can submit your pipelines to.\n",
    "- A service account with all the necessary permissions needed to execute your pipelines.\n",
    "- A GCS bucket as an [artifact store](https://docs.zenml.io/mlops-stacks/artifact-stores), which can be used to store all your ML artifacts like the model, checkpoints, etc. \n",
    "- A CloudSQL instance as a [metadata store](https://docs.zenml.io/mlops-stacks/metadata-stores) that is essential to track all your metadata and its location in your artifact store.  \n",
    "- A Container Registry repository as [container registry](https://docs.zenml.io/mlops-stacks/container-registries) for hosting your docker images.\n",
    "- A [secrets manager](https://docs.zenml.io/mlops-stacks/secrets-managers) enabled for storing your secrets. \n",
    "- An optional MLflow Tracking server deployed on a GKE cluster as an [experiment tracker](https://docs.zenml.io/mlops-stacks/experiment-trackers). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a6de5",
   "metadata": {},
   "source": [
    "Note that the terraform will create the required service account and enable required APIs on GCP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf700d9",
   "metadata": {},
   "source": [
    "- create new project on GCP\n",
    "\n",
    "- set project in gcloud cli \n",
    "\n",
    "```bash\n",
    "gcloud config set project <new project id>\n",
    "```\n",
    "\n",
    "- pull zenml vertex-ai recipe  \n",
    "\n",
    "```bash\n",
    "zenml stack recipe pull vertex-ai\n",
    "```\n",
    "\n",
    "- Customize your deployment by editing the default values in the `locals.tf` file like `project_id`, `region`, `gcs` name, `prefix`?.\n",
    "\n",
    "- Add your secret information like keys and passwords into the `values.tfvars.json` file which is not committed and only exists locally.\n",
    "\n",
    "- deploy using zenml \n",
    "\n",
    "```bash\n",
    "zenml stack recipe deploy vertex-ai\n",
    "```\n",
    "\n",
    "- create the ZenML stack. You may need to install gcp integration using `zenml integration install gcp` too.\n",
    "\n",
    "```bash\n",
    "zenml stack import vertex-ai zenml_stack_recipes/vertex-ai/vertex_stack_2022-08-26T07_02.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb5d9d",
   "metadata": {},
   "source": [
    "- And to destroy the deployed infrastructure:\n",
    "\n",
    "\n",
    "```bash\n",
    "zenml stack recipe destroy vertex-ai\n",
    "zenml stack recipe clean\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d2c518",
   "metadata": {},
   "source": [
    "Then use `zenml stack list` to see the list of stacks. If the new created stack (`vertex-ai`) is not active, use `zenml stack set vertex-ai` to activate it.\n",
    "\n",
    "![](images/workflow-orchestration/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c9c9b6",
   "metadata": {},
   "source": [
    "You then should be able to run your pipeline. Note that you may need to do `python <path-to-python-env>\\Scripts\\pywin32_postinstall.py -install` if you see any error related to that. Make sure you have docker installed and running. \n",
    "\n",
    "The other point that you need to take care of is the requirements for your code on the cloud. One way is to create a `requirements.txt` file in your repo and set the path to it in your code. Check [here](https://docs.zenml.io/developer-guide/advanced-usage/docker#how-to-install-additional-pip-dependencies) to learn more.\n",
    "\n",
    "```python\n",
    ".\n",
    ".\n",
    ".\n",
    "docker_config = DockerConfiguration(requirements=\"./requirements.txt\")\n",
    ".\n",
    ".\n",
    ".\n",
    "@pipeline(docker_configuration=docker_config)\n",
    ".\n",
    ".\n",
    ".\n",
    "```\n",
    "\n",
    "Here is the content of the `requirements.txt` file:\n",
    "\n",
    "```\n",
    "nltk==3.7\n",
    "tensorflow==2.9.1\n",
    "mlflow==1.25.1\n",
    "zenml==0.13.0\n",
    "scikit-learn\n",
    "pickle\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a497f31",
   "metadata": {},
   "source": [
    "The whole code would be like this:\n",
    "\n",
    "```python\n",
    "# train_dl_zenml.py file\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "if os.path.exists('./corpora'):\n",
    "    os.environ[\"NLTK_DATA\"] = \"./corpora\"\n",
    "else:\n",
    "    nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import pickle\n",
    "from zenml.steps import step, Output\n",
    "from zenml.pipelines import pipeline\n",
    "from zenml.artifacts import DataArtifact\n",
    "from zenml.io import fileio\n",
    "from zenml.materializers.base_materializer import BaseMaterializer\n",
    "from zenml.integrations.mlflow.mlflow_step_decorator import enable_mlflow\n",
    "from zenml.config.docker_configuration import DockerConfiguration\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "DATA_PATH = \"data/Womens Clothing E-Commerce Reviews.csv\"\n",
    "docker_config = DockerConfiguration(requirements=\"./requirements.txt\")\n",
    "\n",
    "\n",
    "class TokenizerMaterializer(BaseMaterializer):\n",
    "    ASSOCIATED_TYPES = (Tokenizer,)\n",
    "    ASSOCIATED_ARTIFACT_TYPES = (DataArtifact,)\n",
    "\n",
    "    def handle_input(self, data_type: Type[Tokenizer]) -> Tokenizer:\n",
    "        \"\"\"Read from artifact store\"\"\"\n",
    "        super().handle_input(data_type)\n",
    "        with fileio.open(os.path.join(self.artifact.uri, 'tokenizer.pickle'), 'rb') as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        return tokenizer\n",
    "\n",
    "    def handle_return(self, tokenizer: Tokenizer) -> None:\n",
    "        \"\"\"Write to artifact store\"\"\"\n",
    "        super().handle_return(tokenizer)\n",
    "        with fileio.open(os.path.join(self.artifact.uri, 'tokenizer.pickle'), 'wb') as f:\n",
    "            pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "## data loading\n",
    "@step\n",
    "def read_data() -> pd.DataFrame:\n",
    "    data = pd.read_csv(DATA_PATH, index_col =[0])\n",
    "    print(\"Data loaded.\\n\\n\")\n",
    "    return data\n",
    "\n",
    "## preprocess text\n",
    "@step\n",
    "def preprocess_data(\n",
    "    data: pd.DataFrame,\n",
    "    ) -> Output(corpus=list, y=np.ndarray):\n",
    "    #check if data/corpus is created before or not\n",
    "    if not os.path.exists('data/corpus_y.pickle'):\n",
    "        print(\"Preprocessed data not found. Creating new data. \\n\\n\")\n",
    "        data = data[~data['Review Text'].isnull()]  #Dropping columns which don't have any review\n",
    "        X = data[['Review Text']]\n",
    "        X.index = np.arange(len(X))\n",
    "\n",
    "        y = data['Recommended IND'].values\n",
    "\n",
    "        corpus =[]\n",
    "        for i in range(len(X)):\n",
    "            review = re.sub('[^a-zA-z]',' ',X['Review Text'][i])\n",
    "            review = review.lower()\n",
    "            review = review.split()\n",
    "            ps = PorterStemmer()\n",
    "            review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]\n",
    "            review =' '.join(review)\n",
    "            corpus.append(review)\n",
    "\n",
    "        with open('data/corpus_y.pickle', 'wb') as handle:\n",
    "            pickle.dump((corpus, y), handle)\n",
    "    else:\n",
    "        print(\"Preprocessed data found. Loading data. \\n\\n\")\n",
    "        with open('data/corpus_y.pickle', 'rb') as handle:\n",
    "            corpus, y = pickle.load(handle)\n",
    "\n",
    "    print(\"Data preprocessed.\\n\\n\")\n",
    "\n",
    "    return corpus, y\n",
    "\n",
    "## tokenization and dataset creation\n",
    "@step\n",
    "def create_dataset(\n",
    "    corpus: list, \n",
    "    y: np.ndarray\n",
    "    ) -> Output(X_train=np.ndarray, X_test=np.ndarray, y_train=np.ndarray, y_test=np.ndarray, tokenizer=Tokenizer):\n",
    "\n",
    "    tokenizer = Tokenizer(num_words = 3000)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(corpus)\n",
    "    padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(padded, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    print(\"Dataset created.\\n\\n\")\n",
    "    # return tokenizer\n",
    "    return X_train, X_test, y_train, y_test, tokenizer\n",
    "\n",
    "\n",
    "def build_dl_pipeline(hyperparams):\n",
    "    @enable_mlflow(\n",
    "        nested=True,\n",
    "        # experiment_name=\"customer-sentiment-analysis\"\n",
    "        )\n",
    "    @step(enable_cache=False)\n",
    "    def train_model(\n",
    "        X_train: np.ndarray, \n",
    "        y_train: np.ndarray, \n",
    "        X_test: np.ndarray, \n",
    "        y_test: np.ndarray, \n",
    "        tokenizer: Tokenizer\n",
    "        ) -> None:\n",
    "        \n",
    "        mlflow.tensorflow.autolog()\n",
    "        \n",
    "        embedding_dim = hyperparams['embedding_dim']\n",
    "        batch_size = hyperparams['batch_size']\n",
    "\n",
    "        # model definition\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(3000, embedding_dim),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(),\n",
    "            tf.keras.layers.Dense(6, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        ## training\n",
    "        num_epochs = 50\n",
    "        callback = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0,\n",
    "            patience=2,\n",
    "            verbose=0,\n",
    "            mode=\"auto\",\n",
    "            baseline=None,\n",
    "            restore_best_weights=False,\n",
    "        )\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "        mlflow.set_tag(\"developer\", \"Isaac\")\n",
    "        mlflow.set_tag(\"algorithm\", \"Deep Learning\")\n",
    "        mlflow.log_param(\"train-data\", \"Womens Clothing E-Commerce Reviews\")\n",
    "        mlflow.log_param(\"embedding-dim\", embedding_dim)\n",
    "\n",
    "        print(\"Fit model on training data\")\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=num_epochs,\n",
    "            callbacks=callback,\n",
    "            # We pass some validation for\n",
    "            # monitoring validation loss and metrics\n",
    "            # at the end of each epoch\n",
    "            validation_data=(X_test, y_test),\n",
    "        )\n",
    "\n",
    "        ## save model and tokenizer\n",
    "        mlflow.keras.log_model(model, 'models/model_dl')\n",
    "\n",
    "        with open('tokenizer_pickle/tf_tokenizer.pickle', 'wb') as handle:\n",
    "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        mlflow.log_artifact(local_path=\"tokenizer_pickle/tf_tokenizer.pickle\", artifact_path=\"tokenizer_pickle\")\n",
    "\n",
    "        # Evaluate the model on the test data using `evaluate`\n",
    "        print(\"Evaluate on test data\")\n",
    "        results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "        print(\"test loss, test acc:\", results)\n",
    "        mlflow.log_metric(\"loss\", results[0])\n",
    "        mlflow.log_metric(\"accuracy\", results[1])\n",
    "\n",
    "        print(\"Model training completed.\\n\\n\")\n",
    "\n",
    "    return training_pipeline(\n",
    "        reading_data=read_data(),\n",
    "        preprocessing_data=preprocess_data(),\n",
    "        creating_dataset=create_dataset().with_return_materializers({\"tokenizer\": TokenizerMaterializer}),\n",
    "        training_model=train_model(),\n",
    "    )\n",
    "\n",
    "\n",
    "@pipeline(docker_configuration=docker_config)\n",
    "def training_pipeline(\n",
    "    reading_data,\n",
    "    preprocessing_data,\n",
    "    creating_dataset,\n",
    "    training_model,\n",
    "):\n",
    "    data = reading_data()\n",
    "    corpus, y = preprocessing_data(data)\n",
    "    X_train, X_test, y_train, y_test, tokenizer = creating_dataset(corpus, y)\n",
    "    training_model(X_train, y_train, X_test, y_test, tokenizer)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    for embedding_dim, batch_size in zip([32, 64, 128], [32, 64, 128]):\n",
    "        hyperparams = {\n",
    "            'embedding_dim': embedding_dim,\n",
    "            'batch_size': batch_size\n",
    "        }\n",
    "        build_dl_pipeline(hyperparams=hyperparams).run()    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6086ec6",
   "metadata": {},
   "source": [
    "Then you can run your pipeline:\n",
    "\n",
    "\n",
    "```bash\n",
    "python train_dl_zenml.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc675b",
   "metadata": {},
   "source": [
    "If you see an error and problem, here are some tips:\n",
    "\n",
    "- there would be a folder in `C:\\Users\\<user>\\AppData\\Roaming\\zenml` if you are on windows. You can use `zenml clean` yo remove it. If it needs permission, you can remove it manualy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14861912",
   "metadata": {},
   "source": [
    "To see the MLflow UI, you can ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d85be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d563d4b8",
   "metadata": {},
   "source": [
    "Here are some resources I used:\n",
    "\n",
    "https://github.com/zenml-io/mlops-stacks/tree/main/vertex-ai\n",
    "\n",
    "https://github.com/zenml-io/zenml/blob/main/examples/vertex_ai_orchestration/run.py\n",
    "    \n",
    "> youtube: https://www.youtube.com/watch?v=qgvmvexGv_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8baf3a",
   "metadata": {},
   "source": [
    "As the final thought, I must admit that I loved ZenML. It's much more than defining a machine learning pipeline and helps you integrate different MLOps tools in your stack. It makes all the steps and MLOps tools easy to use. I decided to continue using it in this project and also in SUPPLYZ.eu stack."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
