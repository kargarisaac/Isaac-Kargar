{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Data Engineering - Week 1\"\n",
    "> \"This is week 1 from the Data Engineering Zoomcamp course.\"\n",
    "\n",
    "- toc: True\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [data engineering, jupyter]\n",
    "- image: images/some_folder/your_image.png\n",
    "- hide: false\n",
    "- search_exclude: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the video of this week:\n",
    "\n",
    "> youtube: https://youtu.be/bkJZDmreIpA\n",
    "\n",
    "github repo: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/dataset.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Platform\n",
    "\n",
    "A very short introduction of Google cloud services:\n",
    "\n",
    "> youtube: https://youtu.be/18jIzE41fJ4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> youtube: https://youtu.be/EYNwNlOrpr0\n",
    "\n",
    "\n",
    "The main goal is to get data (in csv format for example) and process it and then push it into postgres database:\n",
    "\n",
    "![](images/data-engineering-w1/data-pipeline.png)\n",
    "\n",
    "Let's write a Dockerfile and build an image to run a simple python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.py\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "print(sys.argv)\n",
    "day = sys.argv[1]\n",
    "# some fancy stuff with pandas\n",
    "\n",
    "print(f'job finished successfully for day = {day}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dockerfile\n",
    "\n",
    "FROM python:3.9.1 #the base image to start from\n",
    "\n",
    "RUN pip install pandas #run a command to install python packages\n",
    "\n",
    "WORKDIR /app #change the working directory - it's like cd command in linux \n",
    "COPY pipeline.py pipeline.py # copy the file from current folder in the host machine to the working directory\n",
    "\n",
    "ENTRYPOINT [ \"python\", \"pipeline.py\" ] # run the python pipeline.py command when we use docker run command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the following command to build the image from Dockerfile in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker build -t test:pandas ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostgreSQL\n",
    "\n",
    "> youtube: https://youtu.be/2JM-ziJt0WI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we can run a PostgreSQL database with docker and push some data into that.\n",
    "\n",
    "Run *postgres:13* image database with some environment commands (specified by -e), mapping local folder from host machine to a path in docker container (using -v flag), and on port 5432 which will be used for connecting to the database from outside (our python code for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -it \\\n",
    "> -e POSTGRES_USER=\"root\" \\\n",
    "> -e POSTGRES_PASSWORD=\"root\" \\\n",
    "> -e POSTGRES_DB=\"ny_taxi\" \\\n",
    "> -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n",
    "> -p 5432:5432 \\\n",
    "> postgres:13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from [here]( https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) and under `2021 > January > Yellow Taxi Trip Records`. The file name is *yellow_tripdata_2021-01.csv*.\n",
    "\n",
    "Using the following codes you can load and visualize and import data to postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# create engine and set the root as postgresql://user:password@host:port/database\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n",
    "\n",
    "df_iter = pd.read_csv('yellow_tripdata_2021-01.csv', iterator=True, chunksize=100000)\n",
    "\n",
    "while True: #iterate and read chunks of data and append it to the table\n",
    "    df = next(df_iter)\n",
    "    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "    df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "    df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to connect to the postgres database. `pgcli` is a python package that we can use it for connecting to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pgcli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgcli -h localhost -p 5432 -u root -d ny_taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then using `\\dt` command, we can list tables of the database.\n",
    "\n",
    "Use `\\d yellow_taxi_data` command to see the imported data schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+-----------------------+-----------------------------+-----------+\n",
    "| Column                | Type                        | Modifiers |\n",
    "|-----------------------+-----------------------------+-----------|\n",
    "| index                 | bigint                      |           |\n",
    "| VendorID              | bigint                      |           |\n",
    "| tpep_pickup_datetime  | timestamp without time zone |           |\n",
    "| tpep_dropoff_datetime | timestamp without time zone |           |\n",
    "| passenger_count       | bigint                      |           |\n",
    "| trip_distance         | double precision            |           |\n",
    "| RatecodeID            | bigint                      |           |\n",
    "| store_and_fwd_flag    | text                        |           |\n",
    "| PULocationID          | bigint                      |           |\n",
    "| DOLocationID          | bigint                      |           |\n",
    "| payment_type          | bigint                      |           |\n",
    "| fare_amount           | double precision            |           |\n",
    "| extra                 | double precision            |           |\n",
    "| mta_tax               | double precision            |           |\n",
    "| tip_amount            | double precision            |           |\n",
    "| tolls_amount          | double precision            |           |\n",
    "| improvement_surcharge | double precision            |           |\n",
    "| total_amount          | double precision            |           |\n",
    "| congestion_surcharge  | double precision            |           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write any query on imported tables in the database. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root@localhost:ny_taxi> SELECT max(tpep_pickup_datetime), min(tpep_pickup_datetime), max(total_amount\n",
    " ) FROM yellow_taxi_data;                                                                            \n",
    "+---------------------+---------------------+---------+\n",
    "| max                 | min                 | max     |\n",
    "|---------------------+---------------------+---------|\n",
    "| 2021-02-22 16:52:16 | 2008-12-31 23:05:14 | 7661.28 |\n",
    "+---------------------+---------------------+---------+\n",
    "SELECT 1\n",
    "Time: 0.204s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write the whole pipeline to read data, process it, and push it into postgres. \n",
    "\n",
    "We will have one docker container for data pipeline and one for postgres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PgAdmin\n",
    "\n",
    "It is a tool that can help to connect to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
