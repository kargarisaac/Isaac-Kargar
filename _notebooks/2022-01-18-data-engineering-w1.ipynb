{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Data Engineering - Week 1\"\n",
    "> \"This is week 1 from the Data Engineering Zoomcamp course.\"\n",
    "\n",
    "- toc: True\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [data engineering, jupyter]\n",
    "- image: images/some_folder/your_image.png\n",
    "- hide: false\n",
    "- search_exclude: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this course is to build a data pipeline around a dataset like *TLC Trip Record Data* which is about pickups and drop offs in New York City.\n",
    "\n",
    "Here is the architecture of what we want to do in this course:\n",
    "\n",
    "![](images/data-engineering-w1/1.png)\n",
    "\n",
    "![](images/data-engineering-w1/2.png)\n",
    "\n",
    "\n",
    "We will take this data, process it, then upload it into Google cloud storage, and then from there we will upload it to Google BigQuery, and then we will use BigQuery to do Analytics engineering and building transformation using spark and so on. In the other part, we will pretend that this data is coming in real time and we will build a system around processing this data using kafka.\n",
    "\n",
    "The course will be around 10 weeks:\n",
    "\n",
    "![](images/data-engineering-w1/3.png)\n",
    "\n",
    "\n",
    "Here is the video of this week which will introduce teachers of the course and overview what you can expect from the course in each week:\n",
    "\n",
    "> youtube: https://youtu.be/bkJZDmreIpA\n",
    "\n",
    "**Note**: You can also find the github repo for the course [here](https://github.com/DataTalksClub/data-engineering-zoomcamp). \n",
    "\n",
    "**Note**: You can also find the playlist of videos of the course [here](https://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb).\n",
    "\n",
    "In the first week, we will cover the following topics:\n",
    "\n",
    "![](images/data-engineering-w1/4.png)\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Platform\n",
    "\n",
    "We will use some services from Google Cloud Platform (GCP). Here is a very short introduction of its services:\n",
    "\n",
    "> youtube: https://youtu.be/18jIzE41fJ4\n",
    "\n",
    "First you need to create a new account [google cloud](https://cloud.google.com/). You can also use your account if you have one, but with a new account, you will get 300 dollars credit for free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> youtube: https://youtu.be/EYNwNlOrpr0\n",
    "\n",
    "[Docker](https://www.docker.com/) is a set of platform as a service products that use OS-level virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries, and configuration files; they can communicate with each other through well-defined channels [wikipedia].\n",
    "\n",
    "The main goal is to get data (in csv format for example) and process it and then push it into postgres database:\n",
    "\n",
    "![](images/data-engineering-w1/data-pipeline.png)\n",
    "\n",
    "Let's write a Dockerfile and build an image to run a simple python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.py\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "print(sys.argv)\n",
    "day = sys.argv[1]\n",
    "# some fancy stuff with pandas\n",
    "\n",
    "print(f'job finished successfully for day = {day}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dockerfile\n",
    "\n",
    "FROM python:3.9.1 #the base image to start from\n",
    "\n",
    "RUN pip install pandas #run a command to install python packages\n",
    "\n",
    "WORKDIR /app #change the working directory - it's like cd command in linux \n",
    "COPY pipeline.py pipeline.py # copy the file from current folder in the host machine to the working directory\n",
    "\n",
    "ENTRYPOINT [ \"python\", \"pipeline.py\" ] # run the python pipeline.py command when we use docker run command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the following command to build the image from Dockerfile in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker build -t test:pandas ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostgreSQL\n",
    "\n",
    "> youtube: https://youtu.be/2JM-ziJt0WI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PostgreSQL, also known as Postgres, is a free and open-source relational database management system emphasizing extensibility and SQL compliance [wikipedia].\n",
    "\n",
    "Now let's see how we can run a PostgreSQL database with docker and push some data into that.\n",
    "\n",
    "Run *postgres:13* image database with some environment commands (specified by -e), mapping local folder from host machine to a path in docker container (using -v flag), and on port 5432 which will be used for connecting to the database from outside (our python code for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -it \\\n",
    " -e POSTGRES_USER=\"root\" \\\n",
    " -e POSTGRES_PASSWORD=\"root\" \\\n",
    " -e POSTGRES_DB=\"ny_taxi\" \\\n",
    " -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n",
    " -p 5432:5432 \\\n",
    " postgres:13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from [here]( https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) and under `2021 > January > Yellow Taxi Trip Records`. The file name is *yellow_tripdata_2021-01.csv*.\n",
    "\n",
    "Using the following codes you can load and visualize and import data to postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# create engine and set the root as postgresql://user:password@host:port/database\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n",
    "\n",
    "df_iter = pd.read_csv('yellow_tripdata_2021-01.csv', iterator=True, chunksize=100000)\n",
    "\n",
    "while True: #iterate and read chunks of data and append it to the table\n",
    "    df = next(df_iter)\n",
    "    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "    df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "    df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to connect to the postgres database. `pgcli` is a python package and a command line interface to quickly look at data and we can use it for connecting to the database and do whatever we want with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pgcli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgcli -h localhost -p 5432 -u root -d ny_taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then using `\\dt` command, we can list tables of the database.\n",
    "\n",
    "Use `\\d yellow_taxi_data` command to see the imported data schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+-----------------------+-----------------------------+-----------+\n",
    "| Column                | Type                        | Modifiers |\n",
    "|-----------------------+-----------------------------+-----------|\n",
    "| index                 | bigint                      |           |\n",
    "| VendorID              | bigint                      |           |\n",
    "| tpep_pickup_datetime  | timestamp without time zone |           |\n",
    "| tpep_dropoff_datetime | timestamp without time zone |           |\n",
    "| passenger_count       | bigint                      |           |\n",
    "| trip_distance         | double precision            |           |\n",
    "| RatecodeID            | bigint                      |           |\n",
    "| store_and_fwd_flag    | text                        |           |\n",
    "| PULocationID          | bigint                      |           |\n",
    "| DOLocationID          | bigint                      |           |\n",
    "| payment_type          | bigint                      |           |\n",
    "| fare_amount           | double precision            |           |\n",
    "| extra                 | double precision            |           |\n",
    "| mta_tax               | double precision            |           |\n",
    "| tip_amount            | double precision            |           |\n",
    "| tolls_amount          | double precision            |           |\n",
    "| improvement_surcharge | double precision            |           |\n",
    "| total_amount          | double precision            |           |\n",
    "| congestion_surcharge  | double precision            |           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write any query on imported tables in the database. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root@localhost:ny_taxi> SELECT max(tpep_pickup_datetime), min(tpep_pickup_datetime), max(total_amount\n",
    " ) FROM yellow_taxi_data;                                                                            \n",
    "+---------------------+---------------------+---------+\n",
    "| max                 | min                 | max     |\n",
    "|---------------------+---------------------+---------|\n",
    "| 2021-02-22 16:52:16 | 2008-12-31 23:05:14 | 7661.28 |\n",
    "+---------------------+---------------------+---------+\n",
    "SELECT 1\n",
    "Time: 0.204s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PgAdmin\n",
    "\n",
    "pgAdmin is the leading Open Source management tool, the world’s most advanced Open Source database. The pgAdmin package is a free and open-source graphical user interface (GUI) administration tool for PostgreSQL, which is supported on many computer platforms.\n",
    "pgAdmin 4 is designed to meet the needs of both novice and experienced Postgres users alike, providing a powerful graphical interface that simplifies the creation, maintenance and use of database objects [wikipedia, pgAdmin documentation]. [Here](https://www.pgadmin.org/docs/pgadmin4/latest/index.html) is the documentation.\n",
    "\n",
    "Instead of pgcli which was a command line interface, we can use pgAdmin which is GUI-based and more convenient to work with the database.\n",
    "\n",
    "We can use a docker image that contains both postgres and pgadmin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -it \\\n",
    "-e PGADMIN_DEFAULT_EMAIL=admin@admin.com \\\n",
    "-e PGADMIN_DEFAULT_PASSWORD=root \\\n",
    "-p 8080:80 \\\n",
    "dpage/pgadmin4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then go to this address in your browser: `http://localhost:8080/` and use the email and password you used above to log in.\n",
    "\n",
    "Then right click on `Servers` in the left side of the page and then `create > server...`. Then in the `General` tab set a name and in the `connection` tab use `localhost` and `root` and `root` for host, user, and password. But it doesn't work and it cannot find postgres in its localhost (it cannot see it because there is no connection between these two containers).\n",
    "\n",
    "If we want to connect two containers of postgres and pgAdmin to see each other, we need to put them in one network. Then pgAdmin will be able to connect to postgress. We can create a nettwork using `docker network create <some-name  for example pg-network>` and then when we want to run each container, we need to tell it that this container needs to be run on this network using `--network=pg-network` flag. Also we need to set the `--name=<some-name for example pg-database/pgadmin>` for the postgres/pgadmin container so that the pgAdmin/postgres can find it by name. \n",
    "\n",
    "Then again log in to pgadmin and use `pg-database` name in the `Host name/address` in the connection tab of creating server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker network create pg-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terminal1\n",
    "docker run -it \\\n",
    "  -e POSTGRES_USER=\"root\" \\\n",
    "  -e POSTGRES_PASSWORD=\"root\" \\\n",
    "  -e POSTGRES_DB=\"ny_taxi\" \\\n",
    "  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n",
    "  -p 5432:5432 \\\n",
    "  --network=pg-network \\\n",
    "  --name pg-database \\\n",
    "  postgres:13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#terminal2\n",
    "docker run -it \\\n",
    "  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n",
    "  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n",
    "  -p 8080:80 \\\n",
    "  --network=pg-network \\\n",
    "  --name pgadmin \\\n",
    "  dpage/pgadmin4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use docker-compose and put everything in one yaml file instead of creating network and run two containers in two terminals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terraform and GCP\n",
    "\n",
    "In this section we will go for an introduction to Terraform and how to setup GCP infrastructure using Terraform.\n",
    "\n",
    "Terraform is an open-source infrastructure as code software tool created by HashiCorp. Users define and provide data center infrastructure using a declarative configuration language known as HashiCorp Configuration Language, or optionally JSON [wikipedia].\n",
    "\n",
    "![](images/data-engineering-w1/5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first install Terraform client from [this link](https://www.terraform.io/downloads) based on your OS type.\n",
    "\n",
    "After setting up your GCP account, you need to create a project (follow the video). Then you need to create a service account from `IAM & Admin > Services > Create service account` and then fill the name and other stuff to create it. Then grant `Viewer` role to begin with.\n",
    "\n",
    "A service account is a special kind of account used by an application or compute workload, such as a Compute Engine virtual machine (VM) instance, rather than a person. Applications use service accounts to make authorized API calls, authorized as either the service account itself, or as Google Workspace or Cloud Identity users through domain-wide delegation.\n",
    "For example, a service account can be attached to a Compute Engine VM, so that applications running on that VM can authenticate as the service account. In addition, the service account can be granted IAM roles that let it access resources. The service account is used as the identity of the application, and the service account's roles control which resources the application can access.\n",
    "A service account is identified by its email address, which is unique to the account [[GCP docs](https://cloud.google.com/iam/docs/service-accounts)].\n",
    "\n",
    "After creating the service account, click on three dots in front of it and then `manage keys > add key> create a new key > json`. You can download and save the key on your machine.\n",
    "\n",
    "Then you need Google SDK which is a CLI tool for you to interact with google cloud services. Cloud SDK is a set of tools that you can use to manage resources and applications hosted on Google Cloud. These tools include the gcloud, gsutil, and bq command-line tools [[gcp docs](https://cloud.google.com/sdk/docs)].\n",
    "\n",
    "You can install the SDK following the instructions [here](https://cloud.google.com/sdk/docs/install#deb).\n",
    "\n",
    "To test if it is installed correctly, you can use the `gcloud -v` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isaac@isaac ~ $ gcloud -v\n",
    "\n",
    "Google Cloud SDK 369.0.0\n",
    "alpha 2022.01.14\n",
    "beta 2022.01.14\n",
    "bq 2.0.72\n",
    "core 2022.01.14\n",
    "gsutil 5.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then set environment variable to point to your downloaded GCP keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export GOOGLE_APPLICATION_CREDENTIALS=<path/to/your/service-account-authkeys>.json\n",
    "\n",
    "# Refresh token, and verify authentication\n",
    "gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create the infrastructure for our project in GCP:\n",
    "\n",
    "- Google Cloud Storage (GCS): Data Lake\n",
    "- BigQuery: Data Warehouse\n",
    "\n",
    "A data lake is a centralized repository designed to store, process, and secure large amounts of structured, semistructured, and unstructured data. It can store data in its native format and process any variety of it, ignoring size limits [[gcp docs](https://cloud.google.com/learn/what-is-a-data-lake#section-1)].\n",
    "\n",
    "A data lake provides a scalable and secure platform that allows enterprises to: ingest any data from any system at any speed—even if the data comes from on-premises, cloud, or edge-computing systems; store any type or volume of data in full fidelity; process data in real time or batch mode; and analyze data using SQL, Python, R, or any other language, third-party data, or analytics application [[gcp docs](https://cloud.google.com/learn/what-is-a-data-lake#section-1)].\n",
    "\n",
    "Today’s enterprises rely on the effective collection, storage, and integration of data from disparate sources for analysis and insights. These data analytics activities have moved to the heart of revenue generation, cost containment, and profit optimization. As such, it’s no surprise that the amounts of data generated and analyzed, as well as the number and types of data sources, have exploded.\n",
    "Data-driven companies require robust solutions for managing and analyzing large quantities of data across their organizations. These systems must be scalable, reliable, and secure enough for regulated industries, as well as flexible enough to support a wide variety of data types and use cases. The requirements go way beyond the capabilities of any traditional database. That’s where the data warehouse comes in.\n",
    "BigQuery is the Google Cloud’s modern and serverless data warehousing solution [[gcp docs](https://cloud.google.com/learn/what-is-a-data-warehouse)].\n",
    "\n",
    "A data warehouse is an enterprise system used for the analysis and reporting of structured and semi-structured data from multiple sources, such as point-of-sale transactions, marketing automation, customer relationship management, and more. A data warehouse is suited for ad hoc analysis as well custom reporting. A data warehouse can store both current and historical data in one place and is designed to give a long-range view of data over time, making it a primary component of business intelligence [[gcp docs](https://cloud.google.com/learn/what-is-a-data-warehouse)].\n",
    "\n",
    "More explanation about GCS and BigQuery will come in next lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add permissions for our service account. From `IAM & Admin > IAM` in the `Permissions` tab select the permission we created and edit it. In order to give Terraform access to go and create buckets and objects in GCS, we need to add two new roles called `storage admin` and `storage object admin`. Note that in production you may want to create custom rules to limit the access and not use Admin version which gives full access.\n",
    "The ideal case would be to create one service account for Terrafrom and assign its permissions, and then a different service for data pipeline with its own permissions. \n",
    "\n",
    "In addition to the above two roles, we need to add `BigQuery admin` too for BigQuery to be able to interact with GCS.\n",
    "\n",
    "We also need to enable APIs. The idea is that when the local environment interacts with the cloud environment, it doesn't interact directly with the resource. These APIs are the enablers of this communication. We need to enable these APIs for our project:\n",
    "\n",
    "- [Identity and Access Management (IAM) API](https://console.cloud.google.com/apis/library/iam.googleapis.com?project=caramel-aria-318622)\n",
    "- [IAM Service Account Credentials API](https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com?project=caramel-aria-318622)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write the whole pipeline to read data, process it, and push it into postgres. \n",
    "\n",
    "We will have one docker container for data pipeline and one for postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
