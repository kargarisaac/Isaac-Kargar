<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>MLOps project - part 3a: Machine Learning Model Deployment | Isaac Kargar</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="MLOps project - part 3a: Machine Learning Model Deployment" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Deploying machine learning models in production." />
<meta property="og:description" content="Deploying machine learning models in production." />
<link rel="canonical" href="https://kargarisaac.github.io/blog/mlops/2022/08/28/machine-learning-model-deployment.html" />
<meta property="og:url" content="https://kargarisaac.github.io/blog/mlops/2022/08/28/machine-learning-model-deployment.html" />
<meta property="og:site_name" content="Isaac Kargar" />
<meta property="og:image" content="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-08-28T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Deploying machine learning models in production.","@type":"BlogPosting","headline":"MLOps project - part 3a: Machine Learning Model Deployment","dateModified":"2022-08-28T00:00:00-05:00","datePublished":"2022-08-28T00:00:00-05:00","url":"https://kargarisaac.github.io/blog/mlops/2022/08/28/machine-learning-model-deployment.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://kargarisaac.github.io/blog/mlops/2022/08/28/machine-learning-model-deployment.html"},"image":"https://kargarisaac.github.io/blog/images/some_folder/your_image.png","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://kargarisaac.github.io/blog/feed.xml" title="Isaac Kargar" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','G-7C8WW0BBJ4','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>MLOps project - part 3a: Machine Learning Model Deployment | Isaac Kargar</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="MLOps project - part 3a: Machine Learning Model Deployment" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Deploying machine learning models in production." />
<meta property="og:description" content="Deploying machine learning models in production." />
<link rel="canonical" href="https://kargarisaac.github.io/blog/mlops/2022/08/28/machine-learning-model-deployment.html" />
<meta property="og:url" content="https://kargarisaac.github.io/blog/mlops/2022/08/28/machine-learning-model-deployment.html" />
<meta property="og:site_name" content="Isaac Kargar" />
<meta property="og:image" content="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-08-28T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Deploying machine learning models in production.","@type":"BlogPosting","headline":"MLOps project - part 3a: Machine Learning Model Deployment","dateModified":"2022-08-28T00:00:00-05:00","datePublished":"2022-08-28T00:00:00-05:00","url":"https://kargarisaac.github.io/blog/mlops/2022/08/28/machine-learning-model-deployment.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://kargarisaac.github.io/blog/mlops/2022/08/28/machine-learning-model-deployment.html"},"image":"https://kargarisaac.github.io/blog/images/some_folder/your_image.png","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://kargarisaac.github.io/blog/feed.xml" title="Isaac Kargar" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','G-7C8WW0BBJ4','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Isaac Kargar</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">MLOps project - part 3a: Machine Learning Model Deployment</h1><p class="page-description">Deploying machine learning models in production.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-08-28T00:00:00-05:00" itemprop="datePublished">
        Aug 28, 2022
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      12 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/kargarisaac/blog/tree/master/_notebooks/2022-08-28-machine-learning-model-deployment.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/kargarisaac/blog/master?filepath=_notebooks%2F2022-08-28-machine-learning-model-deployment.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/kargarisaac/blog/blob/master/_notebooks/2022-08-28-machine-learning-model-deployment.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#Machine-Learning-Model-Deployment">Machine Learning Model Deployment </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Batch-or-Offline-Mode">Batch or Offline Mode </a></li>
<li class="toc-entry toc-h2"><a href="#Online-Deployment">Online Deployment </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Web-Service">Web Service </a></li>
<li class="toc-entry toc-h3"><a href="#Streaming">Streaming </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Model-Deployment-Tools">Model Deployment Tools </a></li>
<li class="toc-entry toc-h2"><a href="#Cloud-Run">Cloud Run </a></li>
<li class="toc-entry toc-h2"><a href="#Vertex-AI">Vertex AI </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#KServe-with-ZenML">KServe with ZenML </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-08-28-machine-learning-model-deployment.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So far in this series of blog posts, we saw how to do experiment tracking and creating a machine learning pipeline for model training. But what should we do now with the trained mode?! That's right. We have to deploy the model into production. so people can use it and make inference.</p>
<p>In this blog post we will see what is machine learning model deployment and what are the options that can help us to do it. In the next post, we will deploy our own trained model for customer sentiment analysis in production.</p>
<p>Let's get started.</p>
<h1 id="Machine-Learning-Model-Deployment">
<a class="anchor" href="#Machine-Learning-Model-Deployment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Machine Learning Model Deployment<a class="anchor-link" href="#Machine-Learning-Model-Deployment"> </a>
</h1>
<p>There are multiple options for model deployment. First, we need to ask if we want the predictions to be done immediately or if they can wait for an hour, a day, etc.</p>
<ul>
<li>In case we can wait a bit, we can go for <strong>batch or offline</strong> deployment. In this case, the model doen't need to be running all the time and we can call it with a batch of data regularly in a time interval.</li>
<li>In the other case, we need the model predictions as soon as possible and the model should be running all the time. This is called <strong>online</strong> deployment. Online deployment has multiple variants as well:<ul>
<li>
<em>Web service</em>: In this case, we deploy our model as a web service and we can send HTTPS requests and get the prediction from that.</li>
<li>
<em>Streaming</em>: In this case, there is a stream of events and model is listening to events and reacting to them. </li>
</ul>
</li>
</ul>
<p><img src="/blog/images/copied_from_nb/images/model-deployment/1.jpg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Batch-or-Offline-Mode">
<a class="anchor" href="#Batch-or-Offline-Mode" aria-hidden="true"><span class="octicon octicon-link"></span></a>Batch or Offline Mode<a class="anchor-link" href="#Batch-or-Offline-Mode"> </a>
</h2>
<p>In this case, we need to apply the model to a batch of data in a time interval. It can be every 10 minutes, every half an hour, every day, every week, etc.</p>
<p>Usually we have a database with data in it and a job which has the model to pull the data from database and apply the model to the data. It can then save the results of the prediction into another database, so other jobs can use the data for other purposes like a report or dashboard.</p>
<p><img src="/blog/images/copied_from_nb/images/model-deployment/2.jpg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Online-Deployment">
<a class="anchor" href="#Online-Deployment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Online Deployment<a class="anchor-link" href="#Online-Deployment"> </a>
</h2>
<h3 id="Web-Service">
<a class="anchor" href="#Web-Service" aria-hidden="true"><span class="octicon octicon-link"></span></a>Web Service<a class="anchor-link" href="#Web-Service"> </a>
</h3>
<p>The other common way of deploying models is Web Service which is in the category of Online deployment. In this case you have a web service which has the machine learning model. This service needs to be up and running all the time. It is also possible to use serverless microservices like a service deployed using Cloud Run. There would be a very small delay which needs to be taken into consideration and if it is not acceptable, you need to go for more real-time architectures. This case is more like a one-to-one relationship between client and server.</p>
<p><img src="/blog/images/copied_from_nb/images/model-deployment/3.jpg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Streaming">
<a class="anchor" href="#Streaming" aria-hidden="true"><span class="octicon octicon-link"></span></a>Streaming<a class="anchor-link" href="#Streaming"> </a>
</h3>
<p>In this case, we have producers and consumers. Producers will push some events into an event stream and multiple services or consumers would read from the stream and react to the events. This more like a one-to-many or many-to-many relationship between producer(s) and consumers. For example, the producer can be a user with an app which interacts with the backend and produces some events. Then this events will go to an stream event and multiple services can do different jobs on those events. The difference with the web service option is that there is no explicit connection between the procuser and consumers here. The producer just pushes an event and some services will process it. The result of these consumers may go to another event stream to be used by some other consumers and services. There is no limit there.</p>
<p><img src="/blog/images/copied_from_nb/images/model-deployment/4.jpg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Check the following video to learn more about different deployment options:

</p>
<center>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/JMGe4yIoBRA" frameborder="0" allowfullscreen=""></iframe>
</center>


</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this blog post, we will deploy our app as a web server. To do this, we will use three approaches: Cloud Run, Vertex AI, and an open source MLOps tool.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Model-Deployment-Tools">
<a class="anchor" href="#Model-Deployment-Tools" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model Deployment Tools<a class="anchor-link" href="#Model-Deployment-Tools"> </a>
</h2>
<p>There are many ways to deploy your model into production. <a href="https://getindata.com/blog/machine-learning-model-serving-tools-comaprison-kserve-seldon-core-bentoml/">This blog post</a> does a nice comparison of three popular tools: <a href="https://www.seldon.io/solutions/open-source-projects/core">Seldon Core</a>, <a href="https://kserve.github.io/website/0.9/">KServe</a>, and <a href="https://www.bentoml.com/">BentoML</a>.</p>
<p>The comparison is focused on 9 main areas of model serving tools:</p>
<ul>
<li>ability to serve models from standard frameworks, including Scikit-Learn, PyTorch, Tensorflow and XGBoost</li>
<li>ability to serve custom models / models from niche frameworks</li>
<li>ability to pre-process/post-process data</li>
<li>impact on the development workflow and existing codebase</li>
<li>availability of the documentation</li>
<li>DevOps operability</li>
<li>autoscaling capabilities</li>
<li>available interfaces for obtaining predictions</li>
<li>infrastructure management<blockquote>
<p><strong>KServe</strong>:KServe (previously, before the 0.7 version was named KFServing) is an open-source, Kubernetes-based tool providing custom abstraction (Kubernetes Custom Resource Definition) to define Machine Learning model serving capabilities. Itâ€™s main focus is to hide the underlying complexity of such deployments so that itâ€™s users only need to  focus on the ML-related parts. It supports many advanced features such as autoscaling, scaling-to-zero, canary deployments, automatic request batching as well as many popular ML frameworks out-of-the-box.&gt; <strong>Seldon Core</strong>:Seldon Core is an open source tool developed by Seldon Technologies Ltd, as a building block of the larger (paid) Seldon Deploy solution. Itâ€™s similar to KServe in terms of the approach - it provides high level Kubernetes CRD and supports canary deployments, A/B testing as well as Multi-Armed-Bandit deployments. &gt; <strong>BentoML</strong>:BentoML is a Python framework for wrapping the machine learning models into deployable services. It provides a simple object-oriented interface for packaging ML models and creating HTTP(s) services for them. BentoML offers in-depth integration with popular ML frameworks, so that all of the complexity related to packaging the models and their dependencies is hidden. BentoML-packaged models can be deployed in many runtimes, which include plain Kubernetes Clusters, Seldon Core, KServe, Knative as well as cloud-managed, serverless solutions like AWS Lambda, Azure Functions or Google Cloud Run.
It's really informative and I highly recommend check it out.</p>
</blockquote>
</li>
</ul>
<p>From the above tools, I will use KServe in a ZenML stack to deploy our model. But first, let's see how to deploy the model as an endpoint using Cloud Run and Vertex AI.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Cloud-Run">
<a class="anchor" href="#Cloud-Run" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cloud Run<a class="anchor-link" href="#Cloud-Run"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the previous posts, we trained the model on Vertex AI on GCP and used MLflow for experiment tracking. MLflow will save model artifacts on GCS. Here we just use those artifacts in our endpoint for prediction. I use a solution here to download the model based on experiment ID and run ID and pass these values as environment variable in the docker file. If you use Cloud Run for deployment, you can also set these variables using cloud commands. Check <a href="https://cloud.google.com/run/docs/configuring/environment-variables#console">here</a> for more information.</p>
<p>We develop a single endpoint for prediction using FastAPI. To access GCS, one way is to use service account. You can create the service account from Google Console and give it the required access to storage. I give it storage admin role here, but in production, you can give more limited access. Then you need to generate the JSON key file and same it with the name of <code>key.json</code> it in the root folder. Be carefull to add this file to your <code>.gitignore</code> file as you don't want to push it into Github.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here is the code:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fastapi</span> <span class="kn">import</span> <span class="n">FastAPI</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">from</span> <span class="nn">google.cloud</span> <span class="kn">import</span> <span class="n">storage</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">nltk.stem.porter</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>

<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">'stopwords'</span><span class="p">)</span>
<span class="n">experiment_id</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"EXPERIMENT_ID"</span><span class="p">))</span>
<span class="n">run_id</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"RUN_ID"</span><span class="p">))</span>

<span class="n">path_on_gcs</span> <span class="o">=</span> <span class="n">experiment_id</span> <span class="o">+</span> <span class="s2">"/"</span> <span class="o">+</span> <span class="n">run_id</span>
<span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">'english'</span><span class="p">))</span>


<span class="c1"># fastapi app</span>
<span class="n">app</span> <span class="o">=</span> <span class="n">FastAPI</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"Fashion Sentment Analysis REST API"</span><span class="p">,</span>
    <span class="n">description</span><span class="o">=</span><span class="s2">"This API analyses the review for a fashion product."</span><span class="p">,</span>
    <span class="n">version</span><span class="o">=</span><span class="s2">"0.0.1"</span><span class="p">,</span>
    <span class="n">contact</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"Isaac Kargar"</span><span class="p">,</span>
        <span class="s2">"email"</span><span class="p">:</span> <span class="s2">"Isaac@email.com"</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">review</span><span class="p">):</span>
    <span class="n">review_processed</span> <span class="o">=</span> <span class="n">review</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">review_processed</span> <span class="o">=</span> <span class="n">review_processed</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="n">ps</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
    <span class="n">review_processed</span> <span class="o">=</span><span class="p">[</span><span class="n">ps</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">review_processed</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">'english'</span><span class="p">))]</span>
    <span class="n">review_processed</span> <span class="o">=</span><span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">review_processed</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">review_processed</span>

<span class="k">def</span> <span class="nf">download_mlflow_artifacts</span><span class="p">():</span>
    <span class="c1"># create storage client</span>
    <span class="n">storage_client</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">Client</span><span class="o">.</span><span class="n">from_service_account_json</span><span class="p">(</span><span class="s1">'key.json'</span><span class="p">)</span>
    <span class="c1"># storage_client = storage.Client()</span>
    <span class="c1"># get bucket with name</span>
    <span class="n">bucket</span> <span class="o">=</span> <span class="n">storage_client</span><span class="o">.</span><span class="n">get_bucket</span><span class="p">(</span><span class="s1">'mlflow'</span><span class="p">)</span>
    <span class="c1"># get bucket data as blob</span>
    <span class="n">blobs</span> <span class="o">=</span> <span class="n">bucket</span><span class="o">.</span><span class="n">list_blobs</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="n">path_on_gcs</span><span class="p">)</span> 

    <span class="k">for</span> <span class="n">blob</span> <span class="ow">in</span> <span class="n">blobs</span><span class="p">:</span>
        <span class="n">blob_name</span> <span class="o">=</span> <span class="n">blob</span><span class="o">.</span><span class="n">name</span> 
        <span class="n">file_path</span> <span class="o">=</span> <span class="s2">"models/"</span> <span class="o">+</span> <span class="n">blob_name</span>
        <span class="n">folder_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">folder_path</span><span class="p">)</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">folder_path</span><span class="p">)</span>

        <span class="n">blob</span><span class="o">.</span><span class="n">download_to_filename</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Artifacts downloaded.</span><span class="se">\n\n</span><span class="s2">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">load_model_tokenizer</span><span class="p">():</span>
    <span class="n">artifact_folder</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"models/</span><span class="si">{</span><span class="n">path_on_gcs</span><span class="si">}</span><span class="s2">/artifacts"</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">artifact_folder</span><span class="si">}</span><span class="s2">/models/model_dl"</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">artifact_folder</span><span class="si">}</span><span class="s2">/tokenizer_pickle/tf_tokenizer.pickle"</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">handle</span><span class="p">:</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Model and tokenizer loaded.</span><span class="se">\n\n</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span>


<span class="nd">@app</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"/"</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">root</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">"message"</span><span class="p">:</span> <span class="s2">"Customer review sentiment analysis"</span><span class="p">}</span>


<span class="nd">@app</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"/predict"</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">get_predict</span><span class="p">(</span><span class="n">review</span><span class="p">:</span> <span class="nb">str</span><span class="p">,):</span>
    <span class="sd">"""</span>
<span class="sd">    Reads the list of sensors from the database</span>
<span class="sd">    """</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">review_processed</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">review</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">review_processed</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">"message"</span><span class="p">:</span> <span class="s2">"Please enter a valid review - It seems there is no valubale review in the text."</span><span class="p">}</span>

        <span class="n">download_mlflow_artifacts</span><span class="p">()</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">load_model_tokenizer</span><span class="p">()</span>
        <span class="n">review_processed</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">review_processed</span><span class="p">)</span>
        <span class="n">review_processed</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">review_processed</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'post'</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">review_processed</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">"prediction"</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
            <span class="p">}</span>

    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">"error"</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)}</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">uvicorn</span>

    <span class="n">uvicorn</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">app</span><span class="p">,</span> <span class="n">host</span><span class="o">=</span><span class="s2">"0.0.0.0"</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="mi">8000</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We then need to create a <code>Dockerfile</code>. I use <code>pipenv</code> in this project to be able to have isolated virtual environment for each project. You can learn more about it <a href="https://pipenv.pypa.io/en/latest/">here</a>. Our Dockerfile would be as follows:</p>
<div class="highlight"><pre><span></span><span class="k">FROM</span> <span class="s">python:3.9-slim</span>

<span class="k">RUN</span> pip install -U pip
<span class="k">RUN</span> pip install pipenv 

<span class="k">EXPOSE</span><span class="s"> 8080</span>
<span class="k">ENV</span> EXPERIMENT_ID &lt;mlflow experiment id&gt;
<span class="k">ENV</span> RUN_ID &lt;mlflow run id&gt;

<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="k">COPY</span> . .

<span class="k">RUN</span> pipenv install --system --deploy

<span class="k">CMD</span> <span class="p">[</span><span class="s2">"uvicorn"</span><span class="p">,</span> <span class="s2">"main:app"</span><span class="p">,</span> <span class="s2">"--host"</span><span class="p">,</span> <span class="s2">"0.0.0.0"</span><span class="p">,</span> <span class="s2">"--port"</span><span class="p">,</span> <span class="s2">"8080"</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We use <code>uvicorn</code> web server for FastAPI here. You can also see that I set <code>EXPERIMENT_ID</code> and <code>RUN_ID</code> as environment variables to be used in my python code.</p>
<p>Then we can build the image and test locally first. Follow the steps below to do so:</p>
<ul>
<li>
<p>Build the image</p>
<div class="highlight"><pre><span></span>docker build -t sentiment:v1 .
</pre></div>
</li>
<li>
<p>Run the econtainer</p>
<div class="highlight"><pre><span></span>docker run -it --rm -p <span class="m">8080</span>:8080 sentiment:v1
</pre></div>
</li>
<li>
<p>Then visit <a href="http://localhost:8080/docs">http://localhost:8080/docs</a> to view your fastapi app. You will see the following page which you can play with the API interactively:</p>
</li>
</ul>
<p><img src="/blog/images/copied_from_nb/images/model-deployment/5.jpg" alt=""></p>
<ul>
<li>You can then interact with it and test:</li>
</ul>
<p><img src="/blog/images/copied_from_nb/images/model-deployment/6.jpg" alt=""></p>
<p>If everything works fine, you can deploy the app on Google Cloud using the following commands:</p>
<ul>
<li>Set project ID variable:</li>
</ul>
<div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">PROJECT_ID</span><span class="o">=</span>&lt;your gcp project id&gt;
</pre></div>
<ul>
<li>
<p>Build the image:</p>
<div class="highlight"><pre><span></span>gcloud builds submit --tag gcr.io/<span class="nv">$PROJECT_ID</span>/sentiment
</pre></div>
</li>
<li>
<p>Deploy on cloud run:</p>
<div class="highlight"><pre><span></span>gcloud run deploy sentiment --image gcr.io/<span class="nv">$PROJECT_ID</span>/sentiment --platform managed --allow-unauthenticated
</pre></div>
</li>
</ul>
<p>You then should be able to use the endpoint link and see the same page as before.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We also developed a simple UI using streamlit package to get user review about a product and analyze it. The code for UI is as follows:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">streamlit</span> <span class="k">as</span> <span class="nn">st</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="n">st</span><span class="o">.</span><span class="n">set_page_config</span><span class="p">(</span>
    <span class="n">page_title</span><span class="o">=</span><span class="s2">"Fashion Sentiment Analysis"</span><span class="p">,</span>
    <span class="n">page_icon</span><span class="o">=</span><span class="s2">"ðŸ“ˆ"</span><span class="p">,</span>
    <span class="n">layout</span><span class="o">=</span><span class="s2">"wide"</span>
<span class="p">)</span>

<span class="n">st</span><span class="o">.</span><span class="n">header</span><span class="p">(</span><span class="s2">"Fashion Sentiment Analysis UI"</span><span class="p">)</span>

<span class="n">st</span><span class="o">.</span><span class="n">image</span><span class="p">(</span><span class="s2">"https://krm-stc-ms.azureedge.net/-/media/Images/Ecco/Products/MENS/BROWN/ECCO-SOFT-7-M/470364-02053-main.webp?Crop=1&amp;Size=ProductDetailsMedium1x"</span><span class="p">)</span>

<span class="n">review_text</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">text_input</span><span class="p">(</span><span class="s1">'User review about the product:'</span><span class="p">,</span> <span class="s1">'Input your review here'</span><span class="p">)</span>

<span class="k">if</span> <span class="n">st</span><span class="o">.</span><span class="n">button</span><span class="p">(</span><span class="s2">"Analyze"</span><span class="p">):</span>
    <span class="n">endpoint</span> <span class="o">=</span> <span class="s2">"&lt;url for the deployed model on cloud run&gt;"</span>
    <span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">endpoint</span><span class="si">}</span><span class="s1">/predict?review=</span><span class="si">{</span><span class="n">review_text</span><span class="si">}</span><span class="s1">'</span>

    <span class="n">prediction</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
    <span class="n">st</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">"Prediction:"</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
</pre></div>
<p>And the Docker file would be as follows:</p>
<div class="highlight"><pre><span></span><span class="k">FROM</span> <span class="s">python:3.9-slim</span>

<span class="k">RUN</span> pip install -U pip
<span class="k">RUN</span> pip install pipenv 

<span class="k">EXPOSE</span><span class="s"> 8080</span>

<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="k">COPY</span> . .

<span class="k">RUN</span> pipenv install --system --deploy

<span class="k">CMD</span> streamlit run --server.port <span class="m">8080</span> --server.enableCORS<span class="o">=</span><span class="nb">false</span> app.py
</pre></div>
<p>You can again dockerize this and deploy it on Cloud Run as another microservice by following the steps below:</p>
<ul>
<li>
<p>Set project ID variable:</p>
<div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">PROJECT_ID</span><span class="o">=</span><span class="k">$(</span>gcloud config get-value core/project<span class="k">)</span>
</pre></div>
</li>
<li>
<p>build the image:</p>
<div class="highlight"><pre><span></span>gcloud builds submit --tag gcr.io/<span class="nv">$PROJECT_ID</span>/sentiment_ui
</pre></div>
</li>
<li>
<p>deploy on cloud run:</p>
<div class="highlight"><pre><span></span>gcloud run deploy sentimentui --image gcr.io/<span class="nv">$PROJECT_ID</span>/sentiment_ui --platform managed --allow-unauthenticated
</pre></div>
</li>
</ul>
<p>By following the url for the <code>sentimentui</code> endpoint, you would see a UI wo as follows:</p>
<p><img src="/blog/images/copied_from_nb/images/model-deployment/7.jpg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Great. We deployed our model as an endpoint and developed a simple UI for people to write their comment on the product, and we then call the endpoint and make the inference.</p>
<p>But, what if the model needs GPU for inference? For those models, we can go for Vertex AI. We can also use Vertex AI for models with CPU, but there are some points that we need to be aware of. 
The absence of a downscale to zero is one of the main drawbacks of Vertex AI Endpoints. There is always at least one endpoint node active.
It canÂ costÂ more than 100$ per month for a single endpoint . Even if you receive no requests for predictions, you still have to pay the fee. It is absurd. This is acceptable if you just have one model deployed, but it adds up over time as you deploy more and more models. Additionally, keep in mind that prices increase with machine size.
Consider deploying your model to Cloud Run rather than Vertex AI Endpoints if your model does not require the use of a GPU. Cloud Run is capable of scaling down to zero. On the other hand, keep in mind that if you use Cloud Run instead of Vertex AI, you lose some of the capabilities like Explainable AI or Model Monitoring. Check <a href="https://medium.com/google-cloud/how-to-reduce-your-ml-model-inference-costs-on-google-cloud-e3d5e043980f">this</a> blog post for more detailed comparison.</p>
<p>Anyway, let's see how we can use Vertex AI to deploy our model as an endpoint. You can decide in each project if you want to use it or not.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Vertex-AI">
<a class="anchor" href="#Vertex-AI" aria-hidden="true"><span class="octicon octicon-link"></span></a>Vertex AI<a class="anchor-link" href="#Vertex-AI"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Deploying a model as an endpoint on Vertex is really easy. You can check <a href="https://medium.com/nlplanet/deploy-a-pytorch-model-with-flask-on-gcp-vertex-ai-8e81f25e605f">this blog post</a> to learn how to do it using Google Cloud Console. I use <code>gcloud</code> here to do the same. Follow the steps below:</p>
<ul>
<li>Import model to Vertex AI model registry:</li>
</ul>
<div class="highlight"><pre><span></span>gcloud ai models upload <span class="se">\</span>
  --region<span class="o">=</span>europe-west1 <span class="se">\</span>
  --display-name<span class="o">=</span>sentiment2 <span class="se">\</span>
  --container-image-uri<span class="o">=</span>gcr.io/<span class="nv">$PROJECT_ID</span>/sentiment_ui
</pre></div>
<ul>
<li>Create an endpoint:</li>
</ul>
<div class="highlight"><pre><span></span>gcloud ai endpoints create <span class="se">\</span>
  --region<span class="o">=</span>europe-west1 <span class="se">\</span>
  --display-name<span class="o">=</span>sentiment2
</pre></div>
<ul>
<li>Get the ID for model and endpoint:</li>
</ul>
<div class="highlight"><pre><span></span>gcloud ai endpoints list --region<span class="o">=</span>europe-west1
gcloud ai models list --region<span class="o">=</span>europe-west1
</pre></div>
<ul>
<li>Deploy the model to the endpoint using the above IDs:</li>
</ul>
<div class="highlight"><pre><span></span>gcloud ai endpoints deploy-model &lt;ENDPOINT_ID&gt; <span class="se">\</span>
  --region<span class="o">=</span>europe-west1 <span class="se">\</span>
  --model<span class="o">=</span>&lt;MODEL_ID&gt; <span class="se">\</span>
  --display-name<span class="o">=</span>sentiment2

gcloud ai endpoints deploy-model <span class="m">3509588339302858752</span> <span class="se">\</span>
  --region<span class="o">=</span>europe-west1 <span class="se">\</span>
  --model<span class="o">=</span><span class="m">4628381002983538688</span> <span class="se">\</span>
  --display-name<span class="o">=</span>sentiment2
</pre></div>
<p>You can use the <code>--accelerator</code> flag if you want to use GPU. Check <a href="https://cloud.google.com/sdk/gcloud/reference/ai/endpoints/deploy-model">here</a> to learn more.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="KServe-with-ZenML">
<a class="anchor" href="#KServe-with-ZenML" aria-hidden="true"><span class="octicon octicon-link"></span></a>KServe with ZenML<a class="anchor-link" href="#KServe-with-ZenML"> </a>
</h1>
<blockquote>
<p><a href="https://github.com/kserve/kserve">KServe</a> provides a Kubernetes Custom Resource Definition for serving machine learning (ML) models on arbitrary frameworks. It aims to solve production model serving use cases by providing performant, high abstraction interfaces for common ML frameworks like Tensorflow, XGBoost, ScikitLearn, PyTorch, and ONNX. It encapsulates the complexity of autoscaling, networking, health checking, and server configuration to bring cutting edge serving features like GPU Autoscaling, Scale to Zero, and Canary Rollouts to your ML deployments. It enables a simple, pluggable, and complete story for Production ML Serving including prediction, pre-processing, post-processing and explainability. KServe is being used across various organizations.</p>
</blockquote>
<p><img src="/blog/images/copied_from_nb/images/model-deployment/kserve.png" alt="">
<em><a href="https://github.com/kserve/kserve">source</a></em></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">blog</span><span class="o">.</span><span class="n">zenml</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">deploy</span><span class="o">-</span><span class="k">with</span><span class="o">-</span><span class="n">kserve</span><span class="o">/</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="kargarisaac/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/mlops/2022/08/28/machine-learning-model-deployment.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My posts about Machine Learning</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/kargarisaac" target="_blank" title="kargarisaac"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/eshagh-kargar" target="_blank" title="eshagh-kargar"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/kargarisaac" target="_blank" title="kargarisaac"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
