<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://kargarisaac.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kargarisaac.github.io/blog/" rel="alternate" type="text/html" /><updated>2020-10-26T12:45:35-05:00</updated><id>https://kargarisaac.github.io/blog/feed.xml</id><title type="html">Isaac Kargar</title><subtitle>My posts about Machine Learning</subtitle><entry><title type="html">Model Based Reinforcement Learning (MBRL)</title><link href="https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html" rel="alternate" type="text/html" title="Model Based Reinforcement Learning (MBRL)" /><published>2020-10-26T00:00:00-05:00</published><updated>2020-10-26T00:00:00-05:00</updated><id>https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl</id><content type="html" xml:base="https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-10-26-mbrl.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This post is a summary of the model-based RL tutorial at ICML-2020. You can find the videos &lt;a href=&quot;https://sites.google.com/view/mbrl-tutorial&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Introduction-and-Motivation&quot;&gt;Introduction and Motivation&lt;a class=&quot;anchor-link&quot; href=&quot;#Introduction-and-Motivation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Having access to a model of the world and using it for decision making is a powerful idea. 
There are a lot of applications of MBRL in different areas like robotics (manipulation- what will happen by doing an action), 
self-driving cars (having a model of other agents decisions and future motions and act accordingly),
games (AlphaGo- search over different possibilities), Science ( chemical usecases),
and peration research and energy applications (how to allocate renewable energy in different points in time to meet the demand).&lt;/p&gt;
&lt;h2 id=&quot;Problem-Statement&quot;&gt;Problem Statement&lt;a class=&quot;anchor-link&quot; href=&quot;#Problem-Statement&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In sequential decision making, the agent will interact with the world by doing action $a$ and getting the next state $s$ and reward $r$.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/rl.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We can write this problem as a Markov Decision Process (MDP) as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;States $S \epsilon R^{d_S}$&lt;/li&gt;
&lt;li&gt;Actions $A \epsilon R^{d_A}$&lt;/li&gt;
&lt;li&gt;Reward function $R: S \times A \rightarrow R$&lt;/li&gt;
&lt;li&gt;Transition function $T: S \times A \rightarrow S$&lt;/li&gt;
&lt;li&gt;Discount $\gamma \epsilon (0,1)$&lt;/li&gt;
&lt;li&gt;Policy $\pi: S \rightarrow A$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The goal is to find a policy which maximizes the sum of discounted future rewards:
$$
argmax_{\pi} \sum_{t=0}^\inf \gamma^t R(s_t, a_t)
$$
subject to
$$
a_t = \pi(s_t) , s_{t+1}=T(s_t, a_t)
$$&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;How to solve this optimization problem?!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Collect data $D= \{ s_t, a_t, r_{t+1}, s_{t+1} \}_{t=0}^T$.&lt;/li&gt;
&lt;li&gt;Model-free: learn policy directly from data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$ D \rightarrow \pi$ e.g. Q-learning, policy gradient&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model-based: learn model, then use it to &lt;strong&gt;learn&lt;/strong&gt; or &lt;strong&gt;improve&lt;/strong&gt; a policy &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$ D \rightarrow f \rightarrow \pi$&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-is-a-model?&quot;&gt;What is a model?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-a-model?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;a model is a representation that explicitly encodes knowledge about the structure of the environment and task.&lt;/p&gt;
&lt;p&gt;This model can take a lot of different forms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A transition/dynamic model: $s_{t+1} = f_s(s_t, a_t)$&lt;/li&gt;
&lt;li&gt;A model of rewards: $r_{t+1} = f_r(s_t, a_t)$&lt;/li&gt;
&lt;li&gt;An inverse transition/dynamics model (which tells you what is the action to take and go from one state to the next state): $a_t = f_s^{-1}(s_t, s_{t+1})$&lt;/li&gt;
&lt;li&gt;A model of distance of two states: $d_{ij} = f_d(s_i, s_j)$&lt;/li&gt;
&lt;li&gt;A model of future returns: $G_t = Q(s_t, a_t)$ or $G_t = V(s_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Typically when someone says MBRL, he/she means the firs two items.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/model.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Sometimes we know the ground truth dynamics and rewards. Might as well use them! Like game environments or simulators like Mujoco, Carla, and so on.&lt;/p&gt;
&lt;p&gt;But we don't have access to the model in all cases, so we need to learn the model. In cases like in robots, complex physical dynamics, and interaction with humans.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;How-to-use-model?&quot;&gt;How to use model?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-use-model?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In model-free RL agent we have a policy and learning algorithm like the figure below:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/rl2.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In model-based RL we can use the model in three different ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;simulating the environment: replacing the environment with model and use it to generate data and use it to update the policy.&lt;/li&gt;
&lt;li&gt;Assisting the learning algorithm: modify the learning algorithm to use the model to interpret the data it is getting in a different way. &lt;/li&gt;
&lt;li&gt;Strengthening the policy: allow the agent at test time to use the model to try out different actions before it commits to one of them (taking the action in the real world).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In general, to compare model-free and model-based:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl_vs_mfrl.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;How-to-learn-a-model?&quot;&gt;How to learn a model?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-learn-a-model?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;There are two different dimensions that are useful to pay attention to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;representation of the features for the states that the model is being learned over them&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;representation of the transition between states&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/learn_model.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In continue we take a look at different transition models.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;state-transition-models&quot;&gt;state-transition models&lt;a class=&quot;anchor-link&quot; href=&quot;#state-transition-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In some cases, we know equations of motion and dynamics but we don't know the exact parameters like mass. We can use system identification to estimate unknown parameters like mass. But these sort of cases require having a lot of domain knowledge about how exactly the system works.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/learn_model2.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/learn_model3.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In some cases that we don't know the dynamics of motion, we can simply use an MLP to get a concatenation of $s_t, a_t$ and output the next state $s_{t+1}$.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/learn_model4.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In cases that we have some, not perfect, domain knowledge about the environment, we can use graph neural networks (GNNs) to model the agent (robot). For example in Mujoco we can model a robot (agent) with nodes as its body parts and edges as joint and learn the physics engine.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/learn_model5.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;observation-transition-models&quot;&gt;observation-transition models&lt;a class=&quot;anchor-link&quot; href=&quot;#observation-transition-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In this cases, we don't have access to states (low level states like joint angles), but we have access to images. The MDP for this cases would be like this:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/learn_model6.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;So what can we do with this?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Directly predict transitions between observations (observation-transition models)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/learn_model7.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reconstruct observation at every timestep: Using sth like LSTMs. Here we need to reconstruct the whole observation in each timestep. The images can be blurry in these cases.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/learn_model8.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/learn_model88.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;latent-state-transition-models&quot;&gt;latent state-transition models&lt;a class=&quot;anchor-link&quot; href=&quot;#latent-state-transition-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Another option when we have just access to observation is to instead of making transition between observations we can infere a latent state and then make transitions in that latent space (latent state-transition models) not in the observation space. It would be much faster than reconstructing the observation on every timestep. We take our initial observation or perhaps the last couple of observations and embed them into the latent state and then unroll it in time and do predictions in $z$ instead of $o$.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/learn_model9.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Usually we use the observation and reconstruct it during training but at test time we can unroll it very quickly. we can also reconstruct observation at each timestep we want (not necessarily in all timesteps).&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/learn_model10.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" /><media:content medium="image" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Offline Reinforcement Learning</title><link href="https://kargarisaac.github.io/blog/rl/2020/10/22/offline-rl.html" rel="alternate" type="text/html" title="Offline Reinforcement Learning" /><published>2020-10-22T00:00:00-05:00</published><updated>2020-10-22T00:00:00-05:00</updated><id>https://kargarisaac.github.io/blog/rl/2020/10/22/offline-rl</id><content type="html" xml:base="https://kargarisaac.github.io/blog/rl/2020/10/22/offline-rl.html">&lt;h1 id=&quot;offline-reinforcement-learning&quot;&gt;Offline Reinforcement Learning&lt;/h1&gt;

&lt;p&gt;Here I will summarize some offline RL algorithms which sound interesting to me.&lt;/p&gt;</content><author><name></name></author><summary type="html">Offline Reinforcement Learning</summary></entry><entry><title type="html">Online Reinforcement Learning</title><link href="https://kargarisaac.github.io/blog/rl/2020/06/15/online-rl.html" rel="alternate" type="text/html" title="Online Reinforcement Learning" /><published>2020-06-15T00:00:00-05:00</published><updated>2020-06-15T00:00:00-05:00</updated><id>https://kargarisaac.github.io/blog/rl/2020/06/15/online-rl</id><content type="html" xml:base="https://kargarisaac.github.io/blog/rl/2020/06/15/online-rl.html">&lt;h1 id=&quot;online-reinforcement-learning&quot;&gt;Online Reinforcement Learning&lt;/h1&gt;

&lt;p&gt;In this post I will overview different single and multi-agent online Reinforcement Learning (RL) algorithms. By &lt;strong&gt;online&lt;/strong&gt; I mean the algorithms that can interact with an environment and collect data, in contrast to offline RL. I will update this post and add algorithms periodically.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/rl-diagram.png&quot; alt=&quot;RL diagram&quot; /&gt; &lt;em&gt;RL diagram&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here are some resources to learn more about RL!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;David Silver’s &lt;a href=&quot;https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&quot;&gt;course&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CS287 at UC Berkeley - Advanced Robotics &lt;a href=&quot;https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNBPJdt8WamRAt4XKc639wF&quot;&gt;course&lt;/a&gt; - Instructor: Pieter Abbeel&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CS 285 at UC Berkeley - Deep Reinforcement Learning &lt;a href=&quot;http://rail.eecs.berkeley.edu/deeprlcourse/&quot;&gt;course&lt;/a&gt; - Instructor: Sergey Levine&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CS234 at Stanford - Reinforcement Learning &lt;a href=&quot;http://web.stanford.edu/class/cs234/index.html&quot;&gt;course&lt;/a&gt; - Instructor: Emma Brunskill&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CS885 at University of Waterloo - Reinforcement Learning &lt;a href=&quot;https://www.youtube.com/playlist?list=PLdAoL1zKcqTXFJniO3Tqqn6xMBBL07EDc&quot;&gt;course&lt;/a&gt; - Instructor: Pascal Poupart&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Arthur Juliani’s &lt;a href=&quot;https://medium.com/@awjuliani&quot;&gt;posts&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Jonathan Hui’s &lt;a href=&quot;https://medium.com/@jonathan_hui/rl-deep-reinforcement-learning-series-833319a95530&quot;&gt;posts&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A Free &lt;a href=&quot;https://simoninithomas.github.io/deep-rl-course/&quot;&gt;course&lt;/a&gt; in Deep Reinforcement Learning from beginner to expert by Thomas Simonni&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;single-agent&quot;&gt;Single agent&lt;/h1&gt;

&lt;h3 id=&quot;dqn&quot;&gt;DQN&lt;/h3&gt;

&lt;p&gt;We will take a look at DQN with experience replay buffer and the target network.&lt;/p&gt;

&lt;p&gt;DQN is a value-based method. It means that we try to learn a value function and then use it to achieve the policy. In DQN we use a neural network as a function approximator for our value function. It gets the state as input and outputs the value for different actions in that state. These values are not limited to be between zero and one, like probabilities, and can have other values based on the environment and the reward function we define.&lt;/p&gt;

&lt;p&gt;DQN is an off-policy method which means that we are using data from old policies, the data that we gather in every interaction with the environment and save it in the experience replay buffer, to sample from it later and train the network. The size of the replay buffer should be large enough to reduce the $i.i.d$ property between data that we sample from it.&lt;/p&gt;

&lt;p&gt;To use DQN, the action should be discrete. We can use it for continuous action spaces by discretizing the action space, but it’s better to use other techniques that can handle continuous action spaces such as Policy Gradients.
First, let’s see the algorithm’s sudo code:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/dqn.png&quot; alt=&quot;DQN algorithm&quot; title=&quot;DQN algorithm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this algorithm, we have experience replay buffer and a target network with a different set of parameters that will be updated every $C$ steps. These tricks help to get a better and more stable method rather than pure DQN. There are a lot of improvements for DQN and we will see some of them in the next posts too.&lt;/p&gt;

&lt;p&gt;First, we initialize the weights of both networks and then start from the initial state s and take action a with epsilon-greedy policy. In the epsilon-greedy policy, we select an action a randomly or using the Q-network. Then we execute the selected action and get the next state, reward, and the done values from the environment and save them in our replay buffer. Then we sample a random batch from the replay buffer and calculate target based on the Bellman equation in the above picture and use MSE loss and gradient descent to update the network weights. We will update the weights of our target network every $C$ steps.&lt;/p&gt;

&lt;p&gt;In the training procedure, we use epsilon decay. It means that we consider a big value for epsilon, such as $1$. Then during the training procedure, as we go forward, we reduce its value to something like $0.02$ or $0.05$, based on the environment. It will help the agent to do more exploration in the first steps and learn more about the environment. It’s better to have some exploration always. That’s a trade-off between exploration-exploitation.
In test time, we have to use a greedy policy. It means we have to select the action with the highest value, not randomly anymore (set epsilon to zero actually).&lt;/p&gt;

&lt;h3 id=&quot;reinforce&quot;&gt;REINFORCE&lt;/h3&gt;

&lt;p&gt;REINFORCE is a Monte-Carlo Policy Gradient (PG) method. In PGs, we try to find a policy to map the state into action directly.&lt;/p&gt;

&lt;p&gt;In value-based methods, we find a value function and use it to find the optimal policy. Policy gradient methods can be used for stochastic policies and continuous action spaces. If you want to use DQN for continuous action spaces, you have to discretize your action space. This will reduce the performance and if the number of actions is high, it will be difficult and impossible. But REINFORCE algorithms can be used for discrete or continuous action spaces. They are on-policy because they use the samples gathered from the current policy.&lt;/p&gt;

&lt;p&gt;There are different versions of REINFORCE. The first one is without a baseline. It is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/reinforce.png&quot; alt=&quot;reinforce algorithm&quot; title=&quot;from Sutton Barto book: Introduction to Reinforcement Learning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this version, we consider a policy (here a neural network) and initialize it with some random weights. Then we play for one episode and after that, we calculate discounted reward from each time step towards the end of the episode. This discounted reward (G in the above sudo code) will be multiplied by the gradient. This G is different based on the environment and the reward function we define. For example, consider that we have three actions. The first action is a bad action and the other two actions are some good actions that will cause more future discounted rewards. If we have three positive G values for three different actions, we are pushing the network towards all of them. Actually, we push the network towards action number one slightly and towards others more. Now consider we have one negative G value for the first action and two G values for the other two actions. Here we are pushing the network far from the first action and towards the other two actions. You see?! the value of G and its sign is important. It guides our gradient direction and its step size. To solve such problems, one way is to use baseline. This will reduce the variance and accelerate the learning procedure. For example, subtract the value of the state from it, or normalize it with the mean and variance of the discounted reward of the current episode. You can see the sudo code for REINFORCE with baseline in the following picture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/reinforce2.png&quot; alt=&quot;reinforce algorithm&quot; title=&quot;from Sutton Barto book: Introduction to Reinforcement Learning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this version, first, we initialize the policy and value networks. It is possible to use two separate networks or a multi-head network with a shared part. Then we play an episode and calculate the discounted reward from every step until the end of the episode (reward to go). Then subtract the value (from the learned neural net) for that state from the discounted reward (REINFORCE with baseline) and use it to update the weights of value and policy networks. Then generate another episode and repeat the loop.&lt;/p&gt;

&lt;p&gt;In the Sutton&amp;amp;Barto book, they do not consider the above algorithm as actor-critic (another RL algorithm that we will see in the next posts). It learns the value function but it is not used as a critic! I think it is because we do not use the learned value function (critic) in the first term of the policy gradient rescaler (for bootstrapping) to tell us how good is our policy or action in every step or in a batch of actions (in A2C and A3C we do the update every t_max step). In REINFORCE we update the network at the end of each episode.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“The REINFORCE method follows directly from the policy gradient theorem. Adding a state-value function as a baseline reduces REINFORCE’s variance without introducing bias. Using the state-value function for bootstrapping introduces bias but is often desirable for the same reason that bootstrapping TD methods are often superior to Monte Carlo methods (substantially reduced variance). The state-value function assigns credit to — critizes — the policy’s action selections, and accordingly the former is termed the critic and the latter the actor, and these overall methods are termed actor–critic methods.
Actor–critic methods are sometimes referred to as advantage actor–critic (“A2C”) methods in the literature.”&lt;/em&gt;
[Sutton&amp;amp;Barto — second edition]&lt;/p&gt;

&lt;p&gt;I think Monte-Carlo policy gradient and Actor-Critic policy gradient are good names as I saw in the slides of David Silver course.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/reinforce3.png&quot; alt=&quot;reinforce algorithm&quot; title=&quot;source: https://www.youtube.com/watch?v=KHZVXao4qXs&amp;amp;list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&amp;amp;index=7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I also saw the following slide from the Deep Reinforcement Learning and Control course (CMU 10703) at Carnegie Mellon University:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/reinforce4.png&quot; alt=&quot;reinforce algorithm&quot; title=&quot;source: https://www.andrew.cmu.edu/course//10-703/slides/Lecture_PG-NatGrad-10-8-2018.pdf&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here they consider every method that uses value function (V or Q) as actor-critic and if you just consider reward to go in the policy gradient rescaler, it is REINFORCE. The policy evaluation by the value function can be TD or MC.&lt;/p&gt;

&lt;p&gt;Summary of the categorization:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Vanilla REINFORCE or Policy gradient → we use G as gradient rescaler.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;REINFORCE with baseline → we use $\frac{G-mean(G)}{std(G)}$ or $(G-V)$ as gradient rescaler. We do not use $V$ in $G$. $G$ is only the reward to go for every step in the episode → $G_t = r_t + \gamma r_{t+1} + … $&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Actor-Critic → we use $V$ in the first term of gradient rescaler and call it Advantage ($A$):&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$A_t = Q(s_t, a_t) - V(s_t)$&lt;/p&gt;

&lt;p&gt;$A_t = r_t + \gamma V_{s_{t+1}} - V_{s_t}$ → for one-step&lt;/p&gt;

&lt;p&gt;$A_t = r_t + \gamma r_{t+1} + \gamma^2 V_{s_{t+2}} - V_{s_t}$ → for 2-step&lt;/p&gt;

&lt;p&gt;and so on.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In Actor-Critics you can do the update each $N$ step based on your task. This $N$ can be less than an episode.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anyway, let’s continue.&lt;/p&gt;

&lt;p&gt;This algorithm can be used for either discrete or continuous action spaces. In discrete action spaces, it will output a probability distribution over action, which means that the activation function of the output layer is a softmax. For exploration-exploitation, it samples from the actions based on their probabilities. Actions with higher probabilities have more chances to be selected.&lt;/p&gt;

&lt;p&gt;In continuous action spaces, the output will not have any softmax. Because the output is a mean for a normal distribution. We consider one neuron for each action and it can have any value. In fact, the policy is a normal distribution and we calculate its mean by a neural network. The variance can be fixed or decrease over time or can be learned. You can consider it as a function of the input state, or define it as a parameter that can be learned by gradient descent. If you want to learn the sigma too, you have to consider the number of actions. For example, if we want to map the front view image of a self-driving car into steering and throttle-brake, we have two continuous actions. So we have to have two mean and two variance for these two actions. During training, we sample from this normal distribution for exploration of the environment, but in the test, we only use the mean as action.&lt;/p&gt;

&lt;h3 id=&quot;a2c&quot;&gt;A2C&lt;/h3&gt;

&lt;h3 id=&quot;a3c&quot;&gt;A3C&lt;/h3&gt;

&lt;h3 id=&quot;ppo&quot;&gt;PPO&lt;/h3&gt;

&lt;h3 id=&quot;ddpg&quot;&gt;DDPG&lt;/h3&gt;

&lt;p&gt;This algorithm is from the &lt;em&gt;“Continuous Control with Deep Reinforcement Learning”&lt;/em&gt; &lt;a href=&quot;https://arxiv.org/pdf/1509.02971.pdf&quot;&gt;paper&lt;/a&gt; and uses the ideas from deep q-learning in the continuous action domain and is a model-free method based on the deterministic policy gradient.&lt;/p&gt;

&lt;p&gt;In Deterministic Policy Gradient (DPG), for each state, we have one clearly defined action to take (the output of policy is one value for action and for exploration we add a noise, normal noise for example, to the action). But in Stochastic Gradient Descent, we have a distribution over actions (the output of policy is mean and variance of a normal distribution) and sample from that distribution to get the action, for exploration. In another term, in stochastic policy gradient, we have a distribution with mean and variance and we draw a sample from that as an action. When we reduce the variance to zero, the policy will be deterministic.&lt;/p&gt;

&lt;p&gt;When the action space is discrete, such as q-learning, we get the max over q-values of all actions and select the best action. But in continuous action spaces, you cannot apply q-learning directly, because in continuous spaces finding the greedy policy requires optimization of $a_t$ at every time-step and would be too slow for large networks and continuous action spaces. Based on the proposed equation in the reference paper, here we approximate &lt;em&gt;max Q(s, a)&lt;/em&gt; over actions with &lt;em&gt;Q(a, µ(s))&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In DDPG, they used function approximators, neural nets, for both action-value function $Q$ and deterministic policy function $\mu$. In addition, DDPG uses some techniques for stabilizing training, such as updating the target networks using soft updating for both $\mu$ and $Q$. It also uses batch normalization layers, noise for exploration, and a replay buffer to break temporal correlations.&lt;/p&gt;

&lt;p&gt;This algorithm is an actor-critic method and the network structure is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/ddpg_post/ddpg_diagram.jpg&quot; alt=&quot;DDPG diagram&quot; title=&quot;DDPG diagram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;First, the policy network gets the state and outputs the action mean vector. This will be a vector of mean values for different actions. For example, in a self-driving car, there are two continuous actions: steering and acceleration&amp;amp;braking (one continuous value between $-x$ to $x$, the negative values are for braking and positive values are for acceleration). So we will have two mean for these two actions. To consider exploration, we can use Ornstein-Uhlenbeck or normal noise and add it to the action mean vector in the training phase. In the test phase, we can use the mean vector directly without any added noise. Then this action vector will be concatenated with observation and fed into the $Q$ network. The output of the $Q$ network will be one single value as a state-action value. In DQN, because it had discrete action space, we had multiple state-action values for each action, but here because the action space is continuous, we feed the actions into the $Q$ network and get one single value as the state-action value.&lt;/p&gt;

&lt;p&gt;Finally, the sudo code for DDPG is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/ddpg_post/ddpg_algorithm.jpg&quot; alt=&quot;DDPG algorithm&quot; title=&quot;DDPG algorithm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To understand the algorithm better, it’s good to try to implement it and play with its parameters and test it in different environments. Here is a good implementation in PyTorch that you can start with &lt;a href=&quot;https://github.com/higgsfield/RL-Adventure-2/blob/master/5.ddpg.ipynb&quot;&gt;this&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also found the Spinningup implementation of DDPG very clear and understandable too. You can find it &lt;a href=&quot;https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ddpg/ddpg.py&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For POMDP problems, it is possible to use LSTMs or any other RNN layers to get a sequence of observations. It needs a different type of replay buffer for sequential data.&lt;/p&gt;

&lt;h3 id=&quot;sac&quot;&gt;SAC&lt;/h3&gt;

&lt;h3 id=&quot;ape-x&quot;&gt;Ape-X&lt;/h3&gt;

&lt;h3 id=&quot;r2d2&quot;&gt;R2D2&lt;/h3&gt;

&lt;h3 id=&quot;impala&quot;&gt;IMPALA&lt;/h3&gt;

&lt;h3 id=&quot;never-give-up&quot;&gt;Never Give-Up&lt;/h3&gt;

&lt;h3 id=&quot;agent57&quot;&gt;Agent57&lt;/h3&gt;

&lt;h1 id=&quot;multi-agent&quot;&gt;Multi-Agent&lt;/h1&gt;

&lt;p&gt;In Multi-Agent Reinforcement Learning (MARL)problems, there are several agents who usually have their own private observation and want to take an action based on that observation. This observation is local and different from the full state of the environment in that time-step. The other problem that we face in such environments is the non-stationary problem because all agents are learning and their behavior would be different during training as they learn to act differently.&lt;/p&gt;

&lt;p&gt;To solve this problem, the most naive approach is to use single-agent RL algorithms for each agent and treat other agents as part of the environment. Some methods like Independent Q-Learning (IQL) work fine in some multi-agent RL problems in practice but there is no guarantee for them to converge. In IQL, each agent has one separate action-value function that gets the agent’s local observation to select its action based on that. It is also possible to use additional inputs like previous actions as input. Usually, in partially observable environments, we use RNNs to consider a history of several sequential observation-actions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/marl1.png&quot; alt=&quot;MARL&quot; title=&quot; source: https://arxiv.org/pdf/1706.05296.pdf &quot; /&gt;&lt;/p&gt;

&lt;p&gt;The other approach is to have a fully centralised method to learn and act in a centralised fashion. We can consider this type as a big single-agent problem. This approach is also valid in some problems that you don’t need decentralised execution. For example for traffic management or traffic light management, it is possible to use such approaches.&lt;/p&gt;

&lt;p&gt;There is one more case that is somewhere between the previous two ones: centralised training and decentralised execution. Usually in the training procedure, because we train agents in a simulation environment or in a lab, we have access to the full state and information in the training phase. So it is better to use this knowledge. On the other hand, the learned policy should be decentralised in some environments and agents cannot have access to the full state during the execution phase. So having algorithms to use the available knowledge in the training phase and learn a policy that is not dependent on the full state in the execution time is necessary. Here we focus on the last case.&lt;/p&gt;

&lt;p&gt;There are several works that try to propose such an algorithm and can be divided into two groups:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;value-based methods like Value Decomposition Networks (VDN) and QMIX&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;actor-critic methods like MADDPG and COMA&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;value-based-methods&quot;&gt;VALUE-BASED METHODS&lt;/h2&gt;

&lt;p&gt;These approaches try to propose a way to be able to use value-based methods like Q-learning and train them in a centralised way and use them for decentralised execution.&lt;/p&gt;

&lt;h3 id=&quot;vdn&quot;&gt;VDN&lt;/h3&gt;

&lt;p&gt;This work proposes a way to have separate action-value functions for multiple agents and learn them by just one shared team reward signal. The joint action-value function is a linear summation of all action-value functions of all agents. Actually, by using a single shared reward signal, it tries to learn decomposed value functions for each agent and use it for decentralised execution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/vdn1.png&quot; alt=&quot;VDN&quot; title=&quot; source: https://arxiv.org/pdf/1706.05296.pdf &quot; /&gt;&lt;/p&gt;

&lt;p&gt;Consider a case with 2 agents, the reward would be:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/vdn2.png&quot; alt=&quot;VDN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then the total $Q$ function is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/vdn3.png&quot; alt=&quot;VDN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is using the same Bellman equation to standard q-learning approach and just replaces $Q$ in that equation with the new $Q$ value.&lt;/p&gt;

&lt;h3 id=&quot;qmix&quot;&gt;QMIX&lt;/h3&gt;

&lt;p&gt;QMIX is somehow an extension to value decomposition networks (VDN) but tries to mix the Q-value of different agents in a nonlinear way. They use global state $s_t$ as input to hypernetworks to generate weights and biases of the mixing network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/qmix1.png&quot; alt=&quot;QMIX&quot; title=&quot; source: https://arxiv.org/pdf/1803.11485.pdf&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here again, the equation to update the weights is the standard Bellman equation in which the $Q$ is replaced with $Q_tot$ in the above figure.&lt;/p&gt;

&lt;h2 id=&quot;actor-critic-based-methods&quot;&gt;ACTOR-CRITIC BASED METHODS&lt;/h2&gt;

&lt;p&gt;This group of methods tries to use actor-critic architecture to do centralised training and decentralised execution. Usually, they use the full state and additional information which are available in the training phase in the critic network to generate a richer signal for the actor.&lt;/p&gt;

&lt;h3 id=&quot;maddpg&quot;&gt;MADDPG&lt;/h3&gt;

&lt;p&gt;Multi-Agent DDPG (MADDPG) is a method to use separate actors and critics for each agent and train the critic in a centralised way and use the actor in execution. So each agent has one actor and one critic. The actor has access to its own action-observation data and is trained by them and the critic has access to observation and action of all agents and is trained by all of them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/maddpg.png&quot; alt=&quot;MADDPG&quot; title=&quot; source: https://arxiv.org/pdf/1706.02275.pdf &quot; /&gt;&lt;/p&gt;

&lt;p&gt;The centralised action-value function for each agent can be written as:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/maddpg2.png&quot; alt=&quot;MADDPG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And the gradient can be written as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/maddpg3.png&quot; alt=&quot;MADDPG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you see, the policy is conditioned on the observation of the agent itself, o_i, and the critic is conditioned on the full state and actions of all agents.
This separate critic for each agent allows us to have agents with different rewards, cooperative or competitive behaviors.&lt;/p&gt;

&lt;h3 id=&quot;coma&quot;&gt;COMA&lt;/h3&gt;

&lt;p&gt;The talk can be found &lt;a href=&quot;https://www.youtube.com/watch?v=3OVvjE5B9LU&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Counterfactual Multi-Agent (COMA) policy gradient is a method for cooperative multi-agent systems and uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents’ policies. In addition, to address the problem of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent’s action, while keeping the other agents’ actions fixed. The idea comes from difference rewards, in which each agent learns from a shaped reward $D_a = r(s, u) − r(s,(u^{-a}, c_a))$ that compares the global reward to the reward received when the action of agent $a$ is replaced with a default action $c_a$.&lt;/p&gt;

&lt;p&gt;COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/coma.png&quot; alt=&quot;COMA&quot; title=&quot; source: https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/17193 &quot; /&gt;&lt;/p&gt;

&lt;p&gt;For each agent $a$, we can then compute an advantage function that compares the Q-value for the current action $u^a$ to a counterfactual baseline that marginalizes out $u^a$, while keeping the other agents’ actions $u^{-a}$ fixed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/coma2.png&quot; alt=&quot;COMA&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In contrast to MADDPG, COMA is an on-policy approach and has only one critic network.&lt;/p&gt;</content><author><name></name></author><summary type="html">Online Reinforcement Learning</summary></entry><entry><title type="html">AlphaGo - Mastering the game of Go with deep neural networks and tree search</title><link href="https://kargarisaac.github.io/blog/jupyter/2020/04/12/AlphaGo.html" rel="alternate" type="text/html" title="AlphaGo - Mastering the game of Go with deep neural networks and tree search" /><published>2020-04-12T00:00:00-05:00</published><updated>2020-04-12T00:00:00-05:00</updated><id>https://kargarisaac.github.io/blog/jupyter/2020/04/12/AlphaGo</id><content type="html" xml:base="https://kargarisaac.github.io/blog/jupyter/2020/04/12/AlphaGo.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-12-AlphaGo.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_0.png?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_1.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_2.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_3.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_4.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_5.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_6.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_7.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_8.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;That’s it for the first one. In the next post, I will review the AlphaGo Zero paper.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Isaac Kargar</name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kargarisaac.github.io/blog/_notebooks/my_icons/alphago/alphago_0.png" /><media:content medium="image" url="https://kargarisaac.github.io/blog/_notebooks/my_icons/alphago/alphago_0.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>