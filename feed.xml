<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://kargarisaac.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kargarisaac.github.io/blog/" rel="alternate" type="text/html" /><updated>2020-11-15T10:14:21-06:00</updated><id>https://kargarisaac.github.io/blog/feed.xml</id><title type="html">Isaac Kargar</title><subtitle>My posts about Machine Learning</subtitle><entry><title type="html">Bayesian Deep Learning and Probabilistic Model Construction</title><link href="https://kargarisaac.github.io/blog/deep%20learning/bayesian/jupyter/2020/11/15/bayesian-dl.html" rel="alternate" type="text/html" title="Bayesian Deep Learning and Probabilistic Model Construction" /><published>2020-11-15T00:00:00-06:00</published><updated>2020-11-15T00:00:00-06:00</updated><id>https://kargarisaac.github.io/blog/deep%20learning/bayesian/jupyter/2020/11/15/bayesian-dl</id><content type="html" xml:base="https://kargarisaac.github.io/blog/deep%20learning/bayesian/jupyter/2020/11/15/bayesian-dl.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-11-15-bayesian-dl.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This is a summary of the Bayesian Deep Learning in ICML 2020. You can find the talk &lt;a href=&quot;https://www.youtube.com/watch?v=E1qhGw8QxqY&amp;amp;t=123s&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This tutorial will have four parts:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-009.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Model-Selection&quot;&gt;Model Selection&lt;a class=&quot;anchor-link&quot; href=&quot;#Model-Selection&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;This is the icml 2020 tutorial on bayesian deep learning. 
We'll start with a pattern recognition problem where we have airline passenger numbers indexed by time we'll consider three different modeling choices:&lt;/p&gt;
&lt;p&gt;1- a linear function&lt;/p&gt;
&lt;p&gt;2- a cubic polynomial and choice&lt;/p&gt;
&lt;p&gt;3- a ten thousandth order polynomial&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-002.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;which choice you would make in order to provide a good description of the data? most people go with choices one or two. In this tutorial we'll argue for choice three, because the real world is a complicated place and there'll be some setting of the coefficients in choice three the $w_j$s which provide a better description of reality than could be possibly managed by choices one or two which are just special cases of choice three.&lt;/p&gt;
&lt;p&gt;In practice we're often making something like choice three and using neural nets that have tens of millions of parameters to follow problems with just tens of thousands of data points and finding we often get very good generalization. In part two of this talk we'll actually consider models with an infinite number of parameters that at the same time have very simple inductive dot biases and provide great generalization even on problems with a small number of data points.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To begin understanding the motivation for choice three. It's helpful to think about modeling from a function space perspective let's consider choice one, the linear function.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-003.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;$f(x)=w_0 + w_1 x$ and we'll just put a standard normal distribution over $w_0$ and $w_1$ and this will induce a distribution over functions which we can visualize by sampling from this distribution over the parameters and looking at the different straight lines with different slopes and intercepts that we get. We'll get different $w_0$ and $w_1$s. Different straight lines the gray shade here shows a 95 credible set containing 95 of these functions and the solid blue curve here shows the expectation of this distribution over functions.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Model-construction-and-generalization&quot;&gt;Model construction and generalization&lt;a class=&quot;anchor-link&quot; href=&quot;#Model-construction-and-generalization&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;In this diagram we have a conceptualization of all possible data sets on the horizontal axis and the marginal likelihood or evidence on the vertical axis which is the
probability that we would generate a
data set
if we were to randomly sample from the
parameters of our model.
The model which we were considering on
the last slide can just generate
straight lines with different slopes and
intercepts not very many data sets, 
but because the marginal likelihood is a
proper normalizable probability density, 
it's going to give a lot of mass to
those data sets.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-004.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We could alternatively
consider a model like a multi-layer
perceptron which might have a lot of
hidden units and layers and we have a
broad distribution over the parameters
and it can generate a wide range of
different data sets.
But it won't give any one of those data
sets very much probability.
We could alternatively consider a third
type of model like a convolutional
neural net which is very flexible it can
describe a wide array of different data
sets.
But at the same time it has very
particular inductive biases like
translation equivariance which says that
if we translate an image we don't want
to change the class label
and this means that these types of
models are going to give a reasonable
amount of mass also to structured image
data sets and provide
good generalization on those problems.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;How-do-we-learn?&quot;&gt;How do we learn?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-do-we-learn?&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;In order to construct models with good
generalization, we'll argue that we want
large support. The support being which
solutions are a priority possible
and reasonably calibrated inductive
biases. The inductive biases being the
distribution of support which solutions
are a priori likely.
So we can consider the support to be the
flexibility and we want a lot of
flexibility.
At the same time we should be careful
not to conflate flexibility and
complexity.
In fact in part two as we've mentioned
we'll be considering models with
infinitely many parameters that are
extraordinarily flexible
and at the same time provide very good
generalization even given very small
data sets.
By the same token, we should not treat
parameter counting as a proxy for
complexity.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-005.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In this figure on the left panel we have
the same diagram we had on the previous
slide. In the other panels
we see what happens as these different
models are exposed to a given data set.
In green here we see in the second panel
the model with
large support is able to contain a good
ground truth description of reality
but it has well-calibrated inductive
biases so it efficiently collapses down
onto that good description of reality. 
In the next panel in blue we can hardly
describe
many data sets at all just straight
lines with different slopes and
intercepts and so
while the model becomes quickly
constrained by the available data it
becomes erroneously constrained and it
starts to collapse down onto a bad
solution
, and in the last panel we have a flexible
model which cap
casts wide enough a net to provide a
good description of reality
but it doesn't have it spreads its
support too thinly to have good
inductive biases and reasonable
contraction
around that good description of reality.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;What-is-bayesian-learning?&quot;&gt;What is bayesian learning?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-bayesian-learning?&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;In this tutorial we'll argue that the
key distinguishing property of a
bayesian approach
is marginalization rather than
optimization.
That is instead of use a single setting
of parameters $w$,
we want to use all possible settings of
parameters and weight them by their
posterior probabilities in what's called
a bayesian model average,
and we'll argue that this bayesian model
average will be especially relevant
in deep learning.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-006.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Why-bayesian-deep-learning?&quot;&gt;Why bayesian deep learning?&lt;a class=&quot;anchor-link&quot; href=&quot;#Why-bayesian-deep-learning?&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;We might
characterize bayesian methods as trying
to avoid optimization at all costs,
don't just bet everything on a single
setting of parameters, use all possible
settings of parameters.
The argument was being made that mini
batch SGD would converge to flat regions
of the loss which would provide
better generalization in deep learning than full batch gradient methods.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-007.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In this diagram we have a
conceptualization of parameters on the
horizontal axis
and the value of the loss on the
vertical axis training loss in black
testing loss in red.
We can see that a
flat solution at train
has reasonably low loss and test whereas
a sharp solution has pretty high loss
after there's this horizontal shift
between the training and the test loss.
Which will typically happen because our
model won't be
completely determined by a finite sample.
when we evaluate the loss on different
sets of points even if they're drawn
from the same distribution,
we should get a different optimal
setting of parameters and the shape of
the losses should be relatively similar
because the training loss is still not a
horrible proxy for generalization.
Now if this argument were true and it
meant something
then to mean something the different
parameters in the flat region would have
to correspond to different functions
which provide
compelling and complementary
explanations for the data otherwise we
could just
contrive flatness to re-parametrization
and it wouldn't really
mean anything then this was an
extraordinary argument for following a
bayesian approach and doing
marginalization or integration basically
integrating a flipped version
of this curve where we want to consider
all of those good solutions and weight
them by their posterior probabilities.
In a sense it might just be a bit
arbitrary if everything on just one good
solution, but
we know that there are many.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;So there are many reasons to be
excited about bayesian deep learning.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-008.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Why?&lt;/p&gt;
&lt;p&gt;Neural nets can represent a variety of
complementary explanations for the data
and we'll be seeing this particularly in
part three
and this will lead to better uncertainty
representation which is crucial for
decision making.
We could think from a practical
perspective if our model could never
influence
a decision conceivably then it might not
have much of a practical impact but it
will also have a big effect
on the accuracy of our point predictions
which is
perhaps an under appreciated aspect of
the benefits of bayesian marginalization
in deep learning in particular because
there are all these different
complementary
solutions. We can form a rich ensemble
of high performing and diverse solutions
and by doing that, we'll often get
much better accuracy if we can do this
marginalization effectively
bayesian neural nets were also a gold
standard for a wide variety of problems
in the second wave of neural nets
led in many ways by bradford neil's
hamiltonian monte carlo approaches
which don't scale to modern
architectures but we know that
in a sense there's treasure buried in
some direction and we just need to
build the right tools as a community to
extract that treasure
as we started to see. 
Neural nets are
also much less mysterious
when viewed through the lens of
probability theory
over parametrization double descent,
model construction,
and many other properties like being
able to fit random labels
become very understandable when we think
about things from a probabilistic
perspective.&lt;/p&gt;
&lt;p&gt;Why not?&lt;/p&gt;
&lt;p&gt;These models can be
computationally intractable and can
involve a lot of moving parts design
decisions approximate inference
procedures and so on but they don't have
to and in the last year there's been
really extraordinary empirical progress
for bayesian deep learning
where we now have several methods often
providing better practical results than
classical training
without significant overhead on quite a
wide variety of problems.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This tutorial will have four parts:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-009.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Part-1&quot;&gt;Part 1&lt;a class=&quot;anchor-link&quot; href=&quot;#Part-1&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;Statistics-from-scratch&quot;&gt;Statistics from scratch&lt;a class=&quot;anchor-link&quot; href=&quot;#Statistics-from-scratch&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-011.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We have these $n$ training points or
targets observations
$y$ and they're indexed by $x$'s $x_1, ..., x_n$.
Generally the x's could be like time
spatial locations images
and we want to make a prediction at some
arbitrary test input
$x_*$. In this case could be like the
airline passenger number is in 1961.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-012.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We can
start by thinking about the functional
forms that we are familiar with like
sines, cosines, exponentials,
polynomials, and then create a
functional form that would be a reasonable
description of data
and it might have some free parameters
and we would specify an error function
which could be like the square distance
between the outputs of
of our function and the training
observations
and minimize that error function
with respect to the function parameters to learn
those parameters.
But this approach would involve a lot of
ad hoc design decisions like y-squared
error and not
absolute error for instance.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-013.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We can instead follow a probabilistic
approach, where we
suppose that our observations are drawn
from a noise-free function
$f(x, w)$ plus for example additive gaussian
noise with noise variance sigma squared
and we can then use this observation
model to form
a likelihood and then we can maximize
that likelihood with respect to our
parameters and learn those parameters
and then use our
conditional predictive distribution
given those parameters to make our
predictions.
We can see by taking logs of the
likelihood that if we follow this
approach
we'll get exactly the same point
predictions as we had using the approach
on the previous slide, where we just
specified the squared error function.
However in this approach the design
decisions are a bit more interpretable.
We probably have some intuitions for
example about
whether we want to use gaussian noise
perhaps
if we thought there were outliers in our
problem we might use a heavy-tailed
noise model like a laplace distribution
and that would lead to an absolute value
error function.
So we can make different design
decisions here and derive different loss
functions. 
If we believe our model, $f(x, w)$ to some
extent, we can also get
an estimate of the noise variance in the
data.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Either of those approaches could lead to
what's called overfitting,
where we get very low training loss but
we get
very bad testing error.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-014.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In order to combat overfitting it's
quite popular to introduce what's called
a regularizer where we add
some kind of complexity penalty like we
want to penalize the the magnitude of
the weights in our model.
But this also involves all sorts of
heuristic design decisions like
how do we know whether we want large
weights or small weights. It would
totally depend on the parametrization of
our model.
What is complexity? how much should we
penalize it? we could use some kind of
lambda parameter maybe it determines
your cross validation, but
what would be our validation sets and if
we had several
lambda parameters then we'd have a
cursive dimensionality in estimating
those parameters.
We can gain some interpretability by
thinking about maximizing a log
posterior which would equal a log
likelihood plus a log
prior and the log prior could be
interpreted as a regularizer
but this really isn't a bayesian
approach.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Bayesian-inference&quot;&gt;Bayesian inference&lt;a class=&quot;anchor-link&quot; href=&quot;#Bayesian-inference&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;There isn't much you need to know in
order to use a bayesian approach for
your own research.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-015.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We have bayes rule which is often
expressed as a posterior being
proportional to a likelihood times of
prior over
the normalization constant which is the
marginal likelihood what we were
considering on the vertical axis of that
plot
we had all possible data sets on the
horizontal axis.&lt;/p&gt;
&lt;p&gt;The sum rule says the marginal
distribution of p of x is equal to the
sum
over the joint distribution of p of x
and y summing out y.&lt;/p&gt;
&lt;p&gt;The product rule says that the joint
distribution over x and y
is equal to the conditional distribution
of p of x given y times p of y
or the conditional distribution of y
given x times p of x and we can derive
the
bayes rule from the product rule.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Bayesian-predictive-distribution&quot;&gt;Bayesian predictive distribution&lt;a class=&quot;anchor-link&quot; href=&quot;#Bayesian-predictive-distribution&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-016.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Now ultimately we want to compute the
unconditional predictive distribution p
of y
given our data bulb y but not given
parameters,
and so the sum and product rules give us
this integral in equation 11. The integral of p of y given the
parameters times the posterior over
those parameters given the data y.
So this is called marginalization
because we see w
doesn't appear on the left side it does
on the right.
In words, this integral is saying let's
not just use one setting of parameters
let's use all possible settings of
parameters weighted by their posterior
probabilities,
and this isn't a controversial
expression. It's a direct consequence of
the sum and product rules of probability.
This model average represents what's
called epistemic uncertainty over which
function
fits the data. There are many different
functions corresponding to different
settings of the parameters
and we're not sure given a finite sample
which is the right description of the
data.
By representing episomic uncertainty we
can have some robustness against
against overfitting.
We can also view classical training as a
special case of this approach where we
have an approximate posterior q of w
given our data y
equal to just a point mass, a delta
function, centered on the map the
regularized maximum likelihood
solution of parameters. We can see that
if we substitute this in
we're just going to get our conditional
predictive distribution given those
maximum likelihood or map parameters.
We can also see then that bayesian and
classical approaches will be
similar when the posterior is highly
concentrated around a setting of
parameters, which of course is exactly
not the case
in deep learning where we have neural
nets that are
very diffuse in their posteriors and
also the posteriors capture a variety of
different models corresponding to
complementary explanations
of the data.
So we're going to especially want to do
this integral in deep learning.
We can also see that we can probably do
a lot better than classical training in
terms of estimating this integral
by using some fairly simple posteriors
which
might not be good descriptions of the
exact posterior but are still a lot
better than a point mass.
So we can definitely improve our
estimates without needing to have
an exact approximation of this
integral or exact
representation of this integral.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Bayesian-model-averaging-is-not-model-combination&quot;&gt;Bayesian model averaging is not model combination&lt;a class=&quot;anchor-link&quot; href=&quot;#Bayesian-model-averaging-is-not-model-combination&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;There are fundamental differences
between bayesian model averaging and
some types of model combination.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-017.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In
particular the bayesian model average is
meant to represent
a statistical inability to distinguish
between hypotheses given limited
information but the assumption
is that one of those hypotheses, one
setting of those parameters, is the
correct setting of parameters
and as we get more and more data our
posterior over our hypotheses or
parameters
will collapse onto a particular setting
and we'll recover the maximum likelihood
solution.
This is different than some approaches
to ensembling and model combination
which work by enriching the hypothesis
space and assuming for example the
combination models might be a correct
description
of reality.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Now let's exemplify some of these ideas
with a few applications
suppose we flip a bias coin with the
probability lambda of landing tails. Think about the following three questions:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-018.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-019.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;So the likelihood of our data is just a
product of bernoulli distributions two
possible outcomes here we have $y=1$ if $y_i$ is tails and $y=0$ if y is
heads. If we don't care about ordering
and we observe $m$ tails then our
likelihood is
a binomial distribution.
$m$ tails here probability of getting
tails is $\lambda$.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-021.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We can easily maximize this
likelihood you could try taking
logs of this expression then derivatives
with respect to $\lambda$ setting those
derivatives to equal to zero and so on
we'll get the solution that the maximum
likelihood setting of $\lambda$ is $m$ over $n$
where we have $m$ tails and $n$ total flips
which in a sense
is kind of intuitive, but on the other
hand is kind of problematic.
Why do you think this estimate might be
problematic and in considering this
question
think about the third part of the
problem. What's the probability
that we would get tails on the next flip
assuming we've done one flip and we've
just observed tails
using this estimator.
So if we substitute in $m=1$ and $n=1$,
we're saying there's a one hundred
percent chance that the next flip was
tails.
Do you believe that? of course not.
When we arrive at a clearly unbelievable
prediction,
it's usually because some part of our modeling procedure
has not honestly represented our beliefs.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-023.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Let's think about a bayesian approach
this problem. If we choose a prior:&lt;/p&gt;
$$
p(\lambda) \propto \lambda^\alpha (1 - \lambda)^\beta
$$&lt;p&gt;then the posterior after we multiply the
likelihood against the prior,
will have the same functional form as
the priors. This is called a conjugate
prior.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-024.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;A beta distribution has this functional
form,
the gamma functions here for
normalization.
We can analytically compute the moments
of the beta distribution.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-025.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Here we have visualizations of the beta
distribution corresponding to different
settings of its parameters, a
and b. We can see in the top right panel
if we if we want to express the belief
that we don't know what the bias is then
we can use a uniform distribution
setting a and b equal to one and this
means lambda
is equally probable for any value
between zero and one a priori.
So we can express
even the idea that we really just don't
know using a prior distribution. We don't
need to have
an informative prior. However we might
want to consider
a prior that says we think that the
bias is probably close to a half but
we're not going to say it's
definitely a half. Just choose whichever
prior
is an honest reflection of your beliefs
even if your belief is
&quot;I don't know&quot;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-026.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;So we can
multiply our prior with our likelihood
to get our unnormalized posterior.
This is a beta distribution. We can
compute its moments and
use the posterior expectation over
lambda for our predictions.
We can see in equation 27 it's $m$ plus $a$
over
$n$ plus $a$ plus $b$.&lt;/p&gt;
&lt;p&gt;Now let's consider
a few questions it's good to do some
sanity checks here.
What's the probability the next flip is
tails? let's suppose that
a and b are one we can see that it's not
going to be
a hundred percent if $m$ is one and $n$ is
one. So that's good.&lt;/p&gt;
&lt;p&gt;What happens when we make $a$ and $b$ really
large? well the prior starts to dominate.
That gives us a strong prior
if we make the data really large then
both $n$ and $m$ will be large
and $m$ over $n$ will dominate. In this
estimate it will recover the maximum
likelihood solution
which is what we want that's a good
sanity check.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-027.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Now consider the
fourth question which is conceptually
very important.
Does the map estimate what we get when
we
take the argmax of the log posterior over
$\lambda$,
which is equal to the argmax of the log
likelihood plus the log prior,
with a uniform prior over lambda,
give the same answer as bayesian
marginalization?
to find the probability that the next
flip is tails?&lt;/p&gt;
&lt;p&gt;So if we have a uniform prior over
lambda
then log p of lambda won't affect our
optimization in equation 31 and we'll
just get the maximum likelihood solution.
And we just saw that when we do
marginalization we get a different
answer than the maximum likelihood
solution even with a uniform prior.
So we can't emphasize enough that we
should not interpret
bayesian methods as regularizers in
optimization.
There is a conceptually very important
difference
between marginalization and regularized
maximum likelihood
optimization, and that difference will be
practically
crucial when we're thinking about
bayesian methods
in deep learning.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-028.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Let's consider one more example. Suppose
we have observations $y_1, ...,y_n$
drawn from an unknown density $p(y)$.
We'll start by specifying an observation
model we'll suppose that the points are
drawn from a mixture of gaussians and in
order to estimate this unknown density
we'll
learn the parameters of this mixture of
two gaussians.
Parameters here are the weights, the
means and the variances.
So we can use the observation model to
form a likelihood.&lt;/p&gt;
&lt;p&gt;Think about
choosing a setting of parameters which
will provide
a lot of likelihood without
having to take derivatives and stuff
like that. You can just kind of look at
this expression
and play with a few settings and find
something and just think about the
the means and the variances, don't worry
about the weights.
So if we make for example the mean of
the first component equal to one of the
data points,
then the first $exp$ will disappear. It'll just be
1 and we get the normalization constant
$\frac{w_1}{\sqrt{2\pi \sigma_1^2}}$,
and then we can make $\sigma_1$, the
variance or the standard deviation, very
small for the first component
and then the first term will blow up and the
other term we can use to assign density
to all the points. So we're not
multiplying again zeros and the
likelihood goes to infinity.&lt;/p&gt;
&lt;p&gt;Now do we believe this solution? of
course not. We typically wouldn't believe
our data are comprised of point masses
and when we reach an unbelievable
solution it's typically because we
haven't
fully represented our beliefs in our
modeling procedure.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-029.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We could introduce a regularizer or a
prior which would go to zero faster than
the likelihood goes to infinity
as the variance parameters go to zero,
but we might want to include the point
mass solution
as long as it's one of an uncountably
infinite number of solutions,
which we can do through full bayesian
marginalization, in which case we can use
extremely flexible models
even infinite mixtures of gaussians
corresponding to dirichlet processed
mixture models
and achieve good generalization even
with a small number of points.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-030.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Now ultimately as we've been saying we
wish to compute a bayesian model average
corresponding to our unconditional
predictive distribution,
p of y given data $p(y|x_*, D)$, rather than our
conditional predictive distribution,
p of y given parameters w $p(y|x_*, w)$. This
unconditional predictive distribution is
equal to the integral
of our conditional predictive
distribution times our posterior,
p of w given data $p(w|D)$. This is just an
expression of the sum and product rules
of probability,
rather than use a single setting of
parameters, we want to use all possible
settings of parameters
weighted by their posterior
probabilities, which is going to be
especially impactful in deep learning
where we have
highly diffused posteriors containing
different settings of parameters that
correspond to a variety of compelling
and different solutions to a given
problem.
For most models including bayesian
neural nets this integral is not
analytic.&lt;/p&gt;
&lt;p&gt;It's common to use what's called a
simple monte carlo approximation,
where we take an average of the
conditional predictive distributions
for different settings of parameters
sampled from an approximate posterior q
of w given data $q(w|D)$.&lt;/p&gt;
&lt;p&gt;We find these samples typically through
one of two approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Deterministic methods approximate the
posterior distribution with some
convenient distribution $q$
although the integral can't be computed
in closed form
typically we can represent the
unnormalized posterior analytically.
It's just the likelihood times the prior
$q$ our approximate posterior is chosen
for convenience often so that it's easy
to sample from,
like a gaussian distribution in which
case its parameters would be its mean
vector and its covariance matrix
which we choose typically to make $q$
close to $p$
in some sense. For example variational
methods find these parameters by
minimizing the kl divergence between
$q$ and $p$. As we mentioned earlier,
classical training is a special case of
approximate inference,
where our approximate posterior is just
a point mass centered at the maximum
likelihood
or regularized maximum likelihood map
setting of parameters.
The laplace approximation is another
popular deterministic method, which we'll
discuss further in part three.
Expectation propagation is another
popular approach and there are several
others.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We could alternatively consider markov
chain monte carlo which forms a markov
chain of approximate but asymptotically
exact samples from our posterior.
Metropolis hastings is a popular mcmc
approach,
hamiltonian monte carlo uses gradient
information and was very
successfully developed by radford neal
in the mid-90s for bayesian neural nets.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Recently stochastic gradient mcmc
methods have been very
up-and-coming and exciting approaches in
bayesian deep learning because they
algorithmically resemble SGD which means
they can be applied in
a wide variety of variety of
applications where you might otherwise
use classical training but often with
better results stochastic gradient
longitude dynamics and
sarcastic radiant hamiltonian monte
carlo or stochastic gradient mcmc
approaches.&lt;/p&gt;
&lt;p&gt;We'll also argue that we may sometimes
want to avoid the simple monte carlo
perspective in equation 33 really what
we're most interested in
ultimately is estimating this
unconditional predictive distribution in
equation 32
under computational constraints, and from
this perspective it's helpful to think
of estimating that integral as an
active learning problem under
constraints in which case the deep
ensembles method can be very
compelling as an approximate bayesian
method which we'll discuss
in part four.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Part-2&quot;&gt;Part 2&lt;a class=&quot;anchor-link&quot; href=&quot;#Part-2&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;in this part
we'll be considering a function space
perspective
of machine learning.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/bayesdl/bayesdlicml2020-032.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;we'll cover gaussian processes infinite
neural nets
how training a neural net is like
learning a kernel bayesian
non-parametric deep learning
and several other topics
from the function space perspective the
parameters in isolation
are entirely divorced from the
statistical properties of a model
in part one we considered how in
regularization whether or not we want
large weights or small weights entirely
depends
on the parametrization of the function
that we're using
yet we focus most of our modeling
efforts on learning parameters w
what we really care about are how those
parameters w combine with the functional
form
f x w ideally we want to perform
inference
directly in function space
let's return to the example that we
considered in part one
where we had a distribution over
functions induced by a distribution over
parameters in a linear model we'll
consider f of x w
equals w naught plus w one x and we'll
place a standard normal distribution
over w naught and w one we can sample
from that distribution
to get different values of w naught and
w1
corresponding to different straight
lines with different slopes and
intercepts
the gray shade here corresponds to a 95
credible set
95 of these functions are in that shade
and the solid blue line corresponds to
our expectation
of this induced distribution over
functions
we'll now consider the more general
model class where we have an inner
product of a vector of weights w
with a vector of basis functions phi
the entries of phi for example could be
one x x squared x cubed if we're
considering a polynomial basis we could
alternatively consider a fourier basis
this model class is quite general and
we'll place a gaussian distribution
over w with mean zero for convenience
and covariance matrix sigma w
now i'd like you to pause the video for
a second and see if you can derive the
moments of the induced distribution
over functions given by equations 38 and
39
now all of the randomness in this model
is coming from the distribution over
parameters w
so we can pull w out of we can pull phi
out of the expectation it's just
deterministic
and we can evaluate the expectation over
w
and by definition in this case it's just
zero so the mean of this induced
distribution over functions is zero
now f of x i and f of x j
are two different random variables
corresponding to querying
our random function at two different
input points
x i and x j so the inputs could be time
spatial locations whatever is
indexing our random function
we can use the definition of covariance
and derive the covariance function
also known as the kernel function in
this case as the inner product
of phi of x i with phi of x j
under sigma w now i'd like you to
consider whether there are any
higher moments in this induced
distribution over functions
in this case there are no higher moments
because we just have a sum of gaussian
random variables
in equation 40 and gaussians are closed
under addition
in fact f of x is a gaussian process
with a mean function m of x and a
covariance function or a kernel
k x x prime for two arbitrary inputs x
and x prime
in this case we can actually do
inference directly over f
x instead of over parameters w
formerly a gaussian process is a
collection of random variables
any finite number of which have a joint
gaussian distribution
we can use a gaussian process to define
a prior over functions
this notation here f x is distributed as
a gp
means that any collection of function
values queried at any collection of
inputs
x1 up to xn these could be time spatial
locations images etc
has a joint multivariate normal
distribution with a mean vector mu
defined by the mean function of the
gaussian process
and a covariance matrix k defined by the
covariance function or kernel of the
gaussian process
created all possible pairs of inputs x1
up to xn
in the bottom left panel here we have
samples from our induced distribution
over functions to create this sample
to create these samples i chose a bunch
of input points
a bunch of x's and then i formed my
multivariate normal distribution
by building up my mean vector mu
evaluating my mean function in this case
zero at all the input points that i've
chosen
and forming my covariance matrix by
querying my kernel at all possible pairs
of the inputs
i then sampled from that multivariate
normal distribution
to get the black dots the random
function values
and then i sampled from exactly that
same distribution two more times to get
the purple curve and the green curve
here i've just joined the dots together
the gray shade again corresponds to our
95
credible set and the blue curve here
corresponds to our prior mean function
in this case zero for convenience
in the right panel i've conditioned on
some data denoted by crosses
and i've sampled several posterior
functions in black
purple and green we also have the
posterior mean
function in solid blue which we can use
to make point predictions
and a 95 credible set in gray shape
very importantly all of the statistical
properties of these functions
are controlled by our kernel
and in this case we're using what's
called an rbf kernel
also known as a squared exponential or a
gaussian kernel
this kernel has the functional form a
squared
times the exponential of the negative
euclidean distance
between our two inputs x and x prime
over two
l squared our length scale
it incorporates the very intuitive
inductive bias the functions that are
close together in the input space
should be more correlated than functions
that are far away in the input space
for example airline passenger numbers in
1951 and 1952
should be more similar than airline
passenger numbers in 1951 and 1960.
so this is a very natural inductive bias
the extent of the correlations is
controlled by the length scale
hyperparameter
l if l is very small
the function values become uncorrelated
we just have a white noise process
if l is very large then the function
values become
all very correlated
a is the amplitude hyperparameter if a
is large then the amplitude of these
functions will be large
in the top panel here we have
visualization of the entries of our
covariance matrix when we have 1d
ordered inputs we can see that there's
this strong diagonal band where we have
highest covariance
and then the covariances decrease away
from that band as we increase distance
in the input space
so let's consider the distribution over
functions that we get
when we have particular settings of our
length scale
and our amplitude parameters l naught
and a naught
here we have our sample prior functions
and our sample posterior functions
pause the video for a moment and think
about whether these sample functions
look strange to you in some way
and it might be a little bit subtle
to me these functions look a bit too
wiggly they're varying a little bit too
quickly they're a bit too complex
the mean shoots back down to the prior
mean
a bit too fast
so to me that's that that seems like the
length scale is too small
now let's see what happens is we
increase the length scale
perhaps here we've increased the length
scale a bit too much we see now that the
functions look
too slowly varying too simple they're
over smoothing the data
let's consider again the marginal
likelihood that we considered
in part one of this tutorial we can form
the marginal likelihood
in this case by integrating away our
gaussian process
p of f the marginal likelihood again
is the probability that we would
generate a data set if we were to
sample from this distribution over
functions
now if we have a very small length scale
we just have a white noise process
so we're going to be able to generate
all sorts of different data sets but
they're all going to be pretty different
from one another so if we keep sampling
from that distribution over functions we
won't see the same thing again
if we have a very large length scale
then this induced distribution over
functions is pretty simple as we saw
everything looks sort of like a straight
line everything is is pretty similar
and so we're not really generating that
much many data sets with very much
probability
using that large length scale in red
for a given data set the marginal
likelihood will have an occam's razor
property
where it will automatically favor a
model of appropriate complexity
this is described very eloquently by
david mackay
both in his phd thesis and in his book
on information theory it's also
described
in the gaussian process for machine
learning book by rasmussen and williams
if we optimize our marginal likelihood
with respect to length scale
we get the green fit here which
intuitively has an appropriate level of
complexity
if we had to choose between one of these
curves we'd probably choose the green
curve
now we opened this part of the tutorial
by deriving
covariance functions from classical
weight space models
and then we said well let's just use a
gaussian process with an rbf kernel
now let's derive where that rbf kernel
comes from
we'll again consider an inner product of
weights
and basis functions in this case not
expressed using vector notation
we'll put a gaussian distribution over
our weights and
we'll use as our basis functions
gaussian bumps
centered at points ci as drawn in this
diagram
we can grind through the algebra revert
to the definition
of our covariance function and get our
covariance function in equation 55.
now let's consider what happens as we
let the number of basis functions
j go to infinity
we'll want these basis functions to
cover the whole real line
so we'll let cj the center of the j
spaces function be
log j and c1 the center of the first
basis function
be minus log j
if these basis functions are equally
distributed
then the difference between the i plus
first basis function and i face's
function will be
2 log j over j so we see that that goes
to 0 as j
goes to infinity which is why i used log
j for the endpoints i could have used
root j for instance but not not j
and the expression for the kernel in
equation 57
becomes a riemann sum an integral
with limits c naught and c infinity in
this case minus infinity
and infinity we can substitute in our
expression
for the basis functions and evaluate
this analytic this integral
in closed form and get something that's
proportional to the rbf kernel
now this is a very extraordinary result
i'd like you to
pause the video and just consider what
we've done for a moment
we've actually shown that by thinking
from a function space perspective
we can use a model with an infinite
number of basis functions
using a finite amount of computational
resources
so in part one of the talk i open with
this question
about how we might model airline
passenger numbers and i said i'd like to
use
the ten thousandth order polynomial from
the three choices that we were
considering
you might have wondered well carrying
this argument to its limit are we even
going to be able to store these models
in memory assuming that we want to use
them in fact what we're seeing here is
we can use something like an infinite
order polynomial we can use a model
that is extraordinarily flexible in fact
this is a universal approximator and we
can see that by looking at this
derivation if we collapse these basis
functions onto point masses we can have
densely dispersed point masses with
different different heights
but we've also seen that this model can
generalize very well it has very
intuitive inductive biases
so we should be very careful not to
conflate flexibility with the complexity
of our model class
and we should also be very careful not
to do parameter
counting as a proxy for complexity here
we're seeing
that we can actually use models with an
infinite number of parameters
that in some sense have fairly simple
inductive biases
and provide very good generalization
in this diagram where we have all
possible data sets on the horizontal
axis
we're saying we want to have heavy tails
we want to support all sorts of
different data sets
but we also want to have mass in
reasonable places
and we can get that in this case through
the inductive biases
of the kernel function
i'd like to add a brief note about the
mean function
in this tutorial so far and in many
texts about gaussian processes
the mean function m of x is often taken
to be zero for notational convenience
but in fact we can use any deterministic
mean function without fundamentally
changing
the modeling procedure and it's usually
pretty standard to subtract off the
empirical mean and then use the
zero mean gp or subtract some kind of
deterministic mean function
and then add it back later
also typically the covariance function
or the kernel is really the
the key object of interest there are
often degeneracies between specifying
the mean
and covariance function in which case
often it's typically preferred therefore
to do the modeling
in the covariance function for example
the kernel function shows up in
the aqua factor term we get in the
marginal likelihood a log determinant
but not the mean function that said
a mean function can be a great way of
incorporating scientific
inductive biases into a model there's
sometimes a tension
between using an interpretable
scientifically motivated model for
example a model with physically
interpretable
parameters and some black box function
approximator
in actuality this is often a false
choice we can often use both in the
sense
we can use as our mean function that
scientifically motivated
model that parametric approach which has
interpretable parameters
and then we can at the same time use our
gaussian process with an rbf kernel to
allow for a non-parametric
relaxation around that mean function to
account for
inevitable model misspecification
so we can really have both we can we can
use that mean function of whatever other
model we would have used
to calibrate the inductive biases of our
approach and we can concentrate
our distribution of functions as closely
or
as loosely as we like around that that
scientific mean function
interest in gaussian processes in the
machine learning community
was triggered by work that bradford neil
was doing on bayesian neural nets
he really embraces this idea that we
should build very flexible models and
accordingly
was pursuing the limits of very large
bayesian neural nets
and he showed that as we we let the
number of hidden units
in a bayesian neural net go to infinity
we get a gaussian process
here let's consider the simple neural
net f of x equals a bias
plus a sum of a bunch of hidden units
with hidden unit weights ui
with fairly general assumptions one can
show
using a central limit theorem argument
and not even you know requiring
for example that the parameters have
gaussian distributions
that will get a gaussian process in the
limit as j goes to infinity
and we can derive the moments of the
gaussian process just as we already have
been doing in other examples
we can make particular design choices
like we can let our activation functions
be error functions
and we can use a gaussian prior over our
hidden unit weights
and derive a neural network kernel in
equation 69.
we have samples from a gp with this
neural net kernel we can see that
they're non-stationary that the
at a high order the statistical
properties in the center are different
than
in other regions unlike say the rbf
kernel which is translation in variance
it's a very flexible model it could look
different in different parts of space
but
in a high order the statistical
properties the functions in in those
types of kernels will be similar in
different regions of space
so this result that an infinite neural
net converts to a gaussian process
was very exciting in the machine
learning community and
in some cases it drove the idea that
we ought to just use gaussian processes
instead of neural nets
at the time people were becoming very
frustrated by all sorts of design
decisions associated with working with
neural nets like
how many hidden units do we want how
many hidden layers
what are the activation functions what
optimization procedure are we using what
what learning rate schedule are we using
with that optimization procedure
and the lack of a principle framework to
answer these types of questions
gaussian processes by contrast were very
simple and interpretable
and flexible and you could write the
same 10 lines of code and get the same
answer
anywhere in the world
now david mackay was a big supporter of
gaussian process research but
at the same time he was a bit of a
contrarian he wrote this essay
in an edited book on neural nets where
he has this quote
how can gaussian processes possibly
replace neural nets have we thrown the
baby out with the bathwater
and what he meant was that when neural
nets were being developed they were
envisaged as becoming intelligent agents
that could
discover interesting hidden
representations in data
and while gaussian processes have all
these nice statistical properties
they're also basically just smoothing
devices
in which case are we throwing the baby
out with the bath water and treating gps
as
replacements for neural nets
the answer to this question is to build
more expressive kernel functions
which can discover interesting hidden
representations in data
although kernel methods and gaussian
processes and neural nets are somehow
treated as competing approaches they're
often actually quite complementary
what we get with the neural net is a way
to create
highly adaptive basis functions which
have very good inductive biases for a
number of different application domains
such as image recognition
what we get through a kernel method in a
gaussian process
is a way to have an infinite number of
basis functions
using a finite amount of computation
we can combine both of these both of
these properties together
into approaches like deep kernel
learning where we have a neural net
transforming the inputs of a base kernel
to create a deep kernel which is then
jointly trained through the marginal
likelihood of the gaussian process as
one model
so this gives us an infinite number of
highly adaptive
basis functions importantly this is
quite different than what we would get
if we were to just
use a neural net as a feature extractor
train the neural net
and then apply a gp to the result
rather deep kernel learning here is a
one-step end-to-end procedure it's a
single
model and it's trained through the
marginal likelihood objective
we can use gaussian processes with deep
kernels to do representation learning
in this example here we have a gaussian
process with a deep kernel applied to
the all of eddie faces problem
where we're considering faces with
different orientation angles and we're
trying to predict the orientation angle
for new faces
we can see projecting what the model
learns into two dimensions
that it discovers that faces with
similar orientation angles
here each face is given by a line
segment and the orientation angle is
given by the slope of that line segment
are clustered together are similar in
some way
we can also see this by visualizing
the learned covariance matrices that we
get ordering the faces by orientation
angle
in the left two panels where we're using
deep kernels we see a pronounced
diagonal band
which shows that the model learns that
faces with similar orientation angles
should be more correlated this is
non-euclidean
metric learning in a sense we can
describe what a neural net
is doing as learning a flexible
non-euclidean
similarity metric for the data in this
sense
training a neural net is a lot like
learning a kernel
in the far right panel we have the
entries of the trained rbf kernel matrix
which we see are quite diffuse
euclidean distance is just not a good
proxy for similarity in this application
looking at euclidean distance of vectors
of pixel intensities
is not going to describe what we need to
describe in order to solve this kind
of representation learning problem very
effectively
in this example we're considering a
discontinuous step function
and we see that a gp with a deep kernel
in green here the green shows the 95
credible set describes that data very
effectively
a gp with a spectral mixture kernel a
different type of kernel is shown in red
it still fits the data all right but
it's sort of over smooth
we see a gp with an rbf kernel fit in
blue
from this perspective of a gp with an
rbf kernel even though this is a
flexible model
um this kind of data is a very unlikely
draw from the prior we would have to
sample for a very very long time to see
anything like the step function and so
since we can learn the noise here the
model is just saying well
it's it's quite probable if we you know
believe the gpu with the rbf kernel that
this this data is basically just noise
and will have very simple fit and a lot
of
uncertainty so this example shows that
gaussian processes for example don't
necessarily
over smooth the data or have trouble
fitting discontinuous data
it totally depends on the kernel and
with the deep kernel we can learn
this kind of discontinuous step function
data
here we've applied a deep kernel
structured with an lstm network
to an autonomous driving application
where decision making
with predictive distributions is very
important we don't want to just know
where lane boundaries might be but we
want to know you know
error bars where these lean boundaries
could be in making decisions
and we can see in the bottom row here
that
the predictive distributions do a good
job of capturing the ground
truth and the point predictions are also
better than the point predictions that
what we get
if we use the classic lstm neural net
conventional conventionally gaussian
processes have suffered from
computational constraints we need to
solve a linear system
involving our n by n covariance matrix
for n training points and also
compute log determinants and derivatives
of log determinants
which incurs naively an n cube
computational cost and
exact gaussian processes have often been
intractable for problems with more than
a few thousand points
recently by embracing advances in
hardware krylov subspace methods have
been developed that enable
very scalable exact gaussian processes
through
gpu acceleration and parallelization
and so in a sense deep learning has
progressed not only through
advances in methodological design but
also
by building algorithms that can really
benefit
from hardware acceleration by thinking
about how to benefit from systems
and gpi torch this package here
really is inspired by that approach in
order to scale gaussian processes
with exact inference to very large
problems
and when we do that we can see that gps
with deep kernels
often outperform standalone deep neural
nets
also we can see that when we scale an
exact gaussian process
to large problems we can really realize
the benefit of a non-parametric
representation
as we add more and more data points we
can see that the error decreases
very substantially with a non-parametric
method that has an infinite number of
basis functions
the capacity of the model scales
automatically with the amount of
available information
unlike a parametric model which is
entirely determined by a finite number
of parameters
many approximate scalable inference
techniques for gaussian processes
introduce low rank kernel matrices so
this would this would apply to inducing
point methods stochastic variational
inducing point methods
as well as random feature expansions
which basically turns the model into a
parametric method
and we can see that on really large data
sets the gap between the
scalable exact gps and the approximate
gps is especially
large there are several other
popular gaussian process libraries such
as gp flow
g pi bow torch for bayesian optimization
that's
black box optimization automatic ml
hyper parameter tuning a b
testing and these libraries have
different algorithmic foundations and
use cases
there's also a lot of additional work on
combining gaussian processes
and neural nets in various different
ways for example gaussian process
regression networks and deep gaussian
processes
build hierarchical models replacing
neurons in neural nets with gaussian
processes
several other recent works have extended
bradford niels limits to
multi-layer neural nets and other
architectures like convolutional neural
nets
and those limits are also very related
to neural tangent kernels which have
been a very
hot topic in deep learning recently
where we take infinite
neural net limits and derive kernels
which have recently achieved fairly
promising empirical
results now most of these kernels from
infinite neural net limits
have fixed covariance structure and we
described how training a neural net
in many ways is like learning a kernel
that's what's doing the representation
learning
and so bridging this gap and developing
these infinite limits
so that we can also do kernel learning i
think is a very exciting direction for
future work in this space
now there are many ways to realize
bayesian principles in deep learning in
addition to the standard approach of
margin line
marginalizing distributions over
parameters in a neural net
in this part we've really been
considering how we can use neural nets
to provide inductive biases for gaussian
processes
for a bayesian non-parametric approach
to deep learning
in general combining principles of
bayesian non-parametrics
with deep learning is a very exciting
area for future work
that's the end of part two in part three
we'll be considering practical methods
for
bayesian deep learning hi i'm andrew
wilson at new york university
and welcome back to the icml 2020
tutorial on bayesian deep learning
this is part three methods for practical
bayesian deep learning
in this part we'll be using several
modern approaches for highly practical
bayesian deep learning
to exemplify many of the foundational
concepts that we've introduced in other
parts of the tutorial
while i've long been interested in
bayesian machine learning
i ironically became quite interested in
bayesian deep learning
after listening to a talk about
optimization the irony is that i might
consider
bayesian approaches as trying to avoid
optimization at all costs
rather than use one parameter use all
the perimeters and weight them by their
posterior probabilities
the argument being made was that mini
batch sgd
would converge to flatter regions of the
loss than
full batch gradient methods and
therefore provide better
generalization in this figure
the loss is on the vertical axis train
in black
chest and red and there's a
conceptualization of parameters on the
horizontal axis
we can see that a flat solution has
still reasonably good
testing loss but sharp solution has
pretty bad
testing loss once there's this
horizontal shift between
train and test which happens because
our model won't be entirely specified by
a finite amount of data and in fact we
would want the different
optimal solutions to change as we query
our loss on different sets of data even
if they're from the same distribution
at the same time the shape of the loss
is unchanged because
we imagine that the loss is still a
reasonable proxy for generalization
in actuality will change a bit but not
necessarily that much
now this was actually a really great
argument for bayesian integration
because
in order for this to be true
the flat regions of the loss would have
to have parameters corresponding to
very different functions which would
provide complementary explanations of
the data
otherwise we could just contrive
flatness for example is
re-parametrization
it would really mean that much and if
that's the case
that we have all of these low-loss
parameter settings that are providing
complementary explanations of the data
that we really really want to do that
integral we want to use all these
parameters and it's kind of arbitrary in
a sense to say
let's just put everything on one
hypothesis we want to integrate
a flipped version of this curve
and that's going to give a very
different answer than just using one
solution
in particular recall that the bayesian
model average here given by this
integral
says that we want to use a conditional
predictive distribution given w
weighted by the posterior distribution w
given d
and integrating over all possible values
of w that's why it's called
marginalization we're not
depending on w anymore we've
marginalized it out when we find this
conditional predictive distribution the
posterior w
given the uh often the losses taken as
the negative log posterior
for neural nets is extremely complex
containing many complementary solutions
which is why bayesian leveraging is
especially significant in deep learning
you know more significant for neural
nets than for most other model classes
in order to come up with a really good
approximation of this integral however
we're going to
really need to carefully understand the
structure of the neural net lost
landscapes
towards that end we showed that if you
retrained a neural network
twice and uh uh uh
got different architectures different
solutions different basins of attraction
you could actually find subspaces along
which these different solutions were
connected
meaning that you could walk from one
point to another
along a curve in this subspace and the
training loss wouldn't change very much
as we traverse this curve and so this is
a surprising result because if we take a
direct linear path between
the different uh solutions found by by
training sgd with different
initializations
we often occur incredibly high loss at
almost 100
uh training error and so um
you know in a sense the intuition is
that the points were isolated but in
fact there are subspaces along which
they aren't and this actually means in a
sense that
we're not even finding local optima when
we train sgd for these neural nets we're
just finding these kind of basins of
attraction and there's some directions
along which
they're extremely flat and we can walk
from one solution
to another and we can find these paths
by minimizing the loss that we found
used sort of for sgd training like cross
entropy
uniformly an expectation over the curve
so this this corresponds to something
like uh
a line integral of cross entropy
normalized by
arc length so in this visualization we
have a two two-dimensional slice
through a very high dimensional loss
surface and
each point in this plane here
corresponds to an affine composition of
three high dimensional weight vectors
and the
height of the loss the color is is is is
is
is the value of the loss of that
combination
and so these visualizations are in
collaboration with with
javier idme lost landscape.com very cool
website
here we have several other
visualizations of mode connectivity
we can see even in the two-dimensional
slices that the structure of the lost
surface is extremely
complex multimodal
why would we choose just one of these
points
so the next question to ask is whether
we can actually
recycle geometric information in the sgd
trajectory for scalable posterior
approximations
centered on flat regions of the loss so
this mode connecting result
is very significant in the sense that it
empirically verifies
this diagram that was used to motivate
sort of flat optima
it shows that in fact there are these
extraordinarily flat regions
of the lost surface and in fact as we'll
see they contain
a variety of different solutions we
really want to do this integral
and we want to see now how we can
practically kind of learn something
about the shape of the law surface just
through something that resembles kind of
basic sgd training
and so we found that if we use the
learning rate schedule that decayed to a
relatively high constant learning rate
and took an equal average of the weights
traversed by sgd with
that high constant learning rate we were
spinning around
a region of the lost surface that
contained a bunch of different
models which all had very low training
loss and so it would normally be very
hard to penetrate inside that region
but when we're spinning around we can
take an equal average of the iterates
it's very important this is an equal
average and a constant learning rate
schedule
we can we can move inside to a fairly
centered region so this is
and and that those those solutions in in
this procedure called stochastic weight
averaging often leads to much better
generalization without a lot of
additional
computational overhead and so this is
very you know importantly different than
say polyacr
rupert averaging where uh uh often we're
we're using um exponential moving
averages with decaying rates and
the idea is to get better convergence
say in convex optimization
now in addition to taking the mean
of these estimates we can also come with
a low rank
plus diagonal approximation to the
second moment the covariance matrix
and then we can use that to come up with
a gaussian approximate posterior for our
parameters
and then we can use that and sample from
it to get our simple monte carlo
estimate
of our predictive distribution and the
whole method here
uh called swag swa gaussian
because it's a gaussian approximate
posterior using
ideas kind of motivated by swa
uh uh can be written here just on you
know just just just one
one on this one slide and uh you know
that's why we call it a simple baseline
for bayesian uncertainty and deep
learning
now there's theory suggesting that if we
run sgd with modified learning rate
schedules asymptotically we're sampling
from a gaussian distribution which is a
reasonable description of the
posterior however this theory has
assumptions which are violated in deep
learning
so as a sanity check we also visualized
the lost surface in the subspaces
spanned by the principal components of
the sgd
iterates with these modified learning
rate schedules and showed that
the swag procedure came up with a pretty
good posterior approximation of the loss
surface in these subspaces
here we're considering uh the
calibration
of various models so on the vertical
axis of each of these plots
we have confidence minus accuracy
confidence corresponds to the highest
softmax
output in our comp net and accuracy is
the accuracy of using the associated
class label
and so here a horizontal curve at zero
corresponds to a perfectly calibrated
model
a curve above that horizontal curve
corresponds to
uh over confidence and below that curve
corresponds to under confidence in the
horizontal
axis here we just have bins for
different confidence levels
so green is sgd training we see that
it's almost always
overconfident and in a way that isn't
surprising because those models are
ignoring epistemic uncertainties so
they're
you know in classical training we're
betting everything on a single model
whether or not it's regularized
and um this means that we're ignoring
all the other
possible functions that are consistent
with our observations so of course in a
sense we're overconfident
we try to account for epistemic
uncertainty through model averaging that
we
get through swag for example then we see
in many cases we end up with quite good
calibration
we're also considering a number of other
approaches so swag here is in blue
there's a dropout like approach in gold
we have
k factor class in in pink temperature
scaling
procedure in brown we can also notably
see in the bottom row here we have
imagenet results with huge neural nets
like a dense net 161 or a resnet 152 and
so this is a very scalable procedure it
can be applied
virtually wherever you would just do
standard classical sgd training
and in the top right we have a transfer
learning problem which is
important because a lot of non-bayesian
alternatives like temperature scaling
where we just scale
our logits by a particular parameter um
uh the the the logics just before we
pass it pass through the softmax
uh uh a special case of flat scaling
um uh require you know a validation set
that's really representative of the
target distribution
and uh that's hard if we have covariate
shift and so
capturing epistemic uncertainty is going
to be especially robust in these kinds
of settings
compared to the alternatives but still
we see all the different approaches here
are
are you know over confident some extent
we can very naturally visualize
epistemic uncertainty and regression
problems so
here we see that with the swag method as
we move away from the data point
where we're getting a wider predictive
distribution and this is because there
are
many different types of curves that are
consistent with our observations
away from the data but not towards the
data will become constrained
and so that's really you know what
epistemic uncertainty
is trying to capture compared to say
illeatoric uncertainty which would
correspond in this case to noise in the
measurements themselves
on the other hand we see with the full
space vi
using standard variational approach for
for
approximate marginalization of neural
net parameters we get a reasonably good
mean fit point predictions but the
uncertainty is pretty homogeneous and it
doesn't grow that much as we move away
from the data so it's not doing a great
job of epidemic
uncertainty representation so in short
this
swa gaussian swag procedure provides a
simple and scalable method for bayesian
deep learning
ideas just fit the estee uterus to a low
rank plus diagonal gaussian distribution
and you know we can capture geometric
properties
of of the posterior uh in the subspace
of sgd and we can we can
improve predictions and uncertainty at
the imagenet scale
now in order to motivate this swag
procedure
we created a visualization of the
subspace spanned by the principal
components of the yes deuteros and this
led to the
question of whether it might make sense
just to try to do bayesian
marginalization in the very low
dimensional subspace directly and so
here we're saying let's construct a
subspace of a network
that has a very high dimensional
parameter space perform inference in
that subspace and then sample from the
approximate posterior
uh just using bayesian and then and then
then use those samples for bayesian
model averaging and what we found is
that we could approximate the posterior
of a wide resnet with
36 million parameters in a 5d subspace
and achieve state-of-the-art results so
the contention here
is that even though the parameter space
is very high dimensional
a lot of the functional variability
could be captured in a very low
dimensional subspace and
once we've reduced that integral that
we're trying to compute to
a very low dimensional integral then
it's going to be a lot easier to
estimate
so a lot of the challenges with bayesian
neural nets
are around the fact that we have to
compute this very high dimensional
integral typically
so in particular in this approach uh
what we did was we
collected all the weights of the drill
net and said they were equal to an
offset like the swa solution
plus a linear projection p of a very low
dimensional vector z
and then we did inference in z space so
we could even use slice sampling if
we're in a low
enough dimensional space which is a
really great mcmc method but it has a
cursive dimensionality
um and um then we can sort of use this
equation to go back into w space and do
the bayesian model average
so let's first consider the mode
connecting subspace so in the right
panel here we're traversing through
parameter space
in this mode connecting subspace and in
the left panel we're looking at the
corresponding functions in purple
and we can see first that there's a lot
of functional variability
in this very flat region of the
posterior
and if i were to ask you which
curve you preferred you know this purple
curve or that purple curse you probably
have a hard time saying and this is why
it's quite arbitrary just to bet
everything on one solution we want to
consider all of these solutions
informing our predictive distribution
when we do that we see we get
good epistemic uncertainty
representation
we can see that the spread of the
predictive distribution here showing the
95
credible set you know increases as we as
we move away from the data
decreases as we move towards the data
here we consider two other subspaces the
random subspace of the pca subspace
random subspace is what we get when we
just use
uh samples from a gaussian distribution
for for our projection matrix
independence
standard gaussian and we can see that
the
curve that here actually fits the data
pretty
well in terms of point predictions but
the uncertainty is very homogeneous so
we're not doing a good job of epidemic
uncertainty representation
the bottom row here we see the the
subspaces in parameter space so
each point is sort of a different
composition of
parameters and the colors correspond to
the values of
the loss at those points in the middle
column here we have the pca subspace
which is what we get when that
projection matrix is found
by looking at the principal components
of the sg iterates in particular as
we're traversing the lost surface with
this
high constant learning rate schedule we
can
store a matrix which has its columns the
weights that we're passing through
minus the average of those weights then
we can do a partial svd and that gives
us our projection matrix it's super
efficient it's just
a single computation and uh uh
you know uh uh once we've got that
subspace we perform inference and we can
see that we
we get a pretty good predictive
distribution here which
is doing an intuitively good job of
representing epidemic uncertainty
now in part one i emphasize that because
there's all this functional
variability uh inside the lost surface
of a neural net
uh we're really going to benefit not
just in terms of uncertainty
representation which can be
measured by say negative log likelihood
but also in terms of just the accuracy
of our point predictions
and we can really realize that here with
that subspace approach so here we have a
resnet on c400 it's getting 78.5
accuracy with classical sgd training
it's getting
80.17 with the random
projection um and we're getting eighty
point five percent then
with the pca subspace and eighty one
point two eight percent of the curve
subspace so these are really
non-trivial gains in accuracy and a pca
subspace for example is
not really much more expensive than just
classical training
and so we can really empirically
empirically realize the benefits of
bayesian marginalization
in terms of not just uncertainty
representation measured by an ll which
we see is also increasing
or decreasing but uh but also accuracy
so historically the laplace
approximation has been very
important in performing inference with
bayesian neural nets david mackay who
did a lot of the
the first work in this space um was
considering
laplace approximations and recently
they've come back with
chronic factorizations so the the basic
idea
is we approximate the posterior with a
gaussian
has its parameters theta it's it's mean
mu and it's it's covariance matrix a
inverse
and uh these parameters are determined
by a second-order taylor approximation
around the log of normalized true
posterior pw
given d and we do that taylor expansion
we see that we set
the mean equal to the map setting of the
parameter is what we get when we
maximize the posterior with respect to w
and we set a equal to the the
negative hessian of the log posterior
evaluated at w
map so for a high dimensional parameter
space
this a matrix is way too large to store
uh
so you know if it's the w is 10 million
dimensional
typically a is taken to be diagonal like
in david bukhai's work
or recently you can you can it's been
k's been expressed as a chronicler
product of much smaller matrices which
is more expressive
than the diagonal approximation and
leads to better results but it's still
scalable
and you know once we have our gaussian
approximate posterior we can sample from
it
using a simple monte carlo estimate of
our
unconditional predictive distribution so
overall the laplace approach is
compelling because
it's simple and can be relatively
efficient
perhaps as a drawback it's constrained
to unimodal approximations
uh uh it's like it's a gaussian
approximate posterior and
uh it's fairly local in its description
of the loss you know even for gaussian
so um
the curvature is entirely different
defined by the hessian evaluated at that
map setting of parameters
and so this means if for example there's
a little kink
in the the lost surface then it'll be
very concentrated around a very small
region and a
very very compact representation of the
posterior
uh swag by contrast even though it
provides a gaussian approximation
it's more global because we have sgd
with this fairly high constant learning
rate bouncing around
within some basement of attraction so if
their little kings
will still get a fairly kind of global
gaussian approximation that won't be
trapped in those those regions at least
for that basis
mc dropout is another in uh you know
very popular approach
uh for for bayesian neural nets and
really catalyzed a lot of
renewed interest in bayesian deep
learning uh in 2016
and so the idea is to run dropout during
both train and test you randomly drop
out each hidden unit
probability r at each input and this
creates you know a mask in regression
each network can be trained to output
both a mean mu and a variance sigma
square by maximizing a gaussian
likelihood
we then create an equally weighted
ensemble of the corresponding sub
networks with a different dropout mass
at test time
so this is a you know very compelling
approach it's
very easy to apply it's very scalable
and it's had great empirical results uh
representing episode uncertainty a
number of problems uh
we know in the in this case the ensemble
doesn't collapse as we get more data on
like a standard bayesian model average
so in part one we
considered how bayesian model averaging
isn't sort of a model combination
in the sense that you know it it's just
representing a statistical inability to
distinguish between hypothesis given in
limited
information um whereas uh
here uh you know we're still going to be
sampling from this bernoulli
distribution to get the dropout mass and
so this isn't something that collapses
as we get more data and you know
figure out how to maybe modify it for
that would be you know an interesting
direction for future work
so uh another historically important
approach
for bayesian deep learning is called
bayes by back prop this is where we
introduce
typically gaussian approximate posterior
for our parameters
and then we learn those parameters using
a variational method which minimizes the
kl divergence between our approximate
posterior and the true posterior
even though the we can't integrate with
respect to the true posterior we can
usually write down
the normalized true posterior exactly we
just multiply our likelihood in our
prior
and we can manipulate the kale
divergence again evidence lower bound
which can be
optimized with with sgd and back
propagation
so even though in principle we can
choose sort of different distributions
for q it is often a lot easier to use
gaussian stochastic
mcmc is a very up-and-coming
direction for bayesian deep learning so
rad for neil's mcmc methods in the mid
90s were achieving state of the
art results for all sorts of interesting
interesting problems but these
these standard hamiltonian monte carlo
methods
um wouldn't really scale to you know big
architectures
stochastic gradient longitudinal
dynamics and stochastic radiant hmc for
example
on the other hand are very broadly
applicable they uh
algorithmically resemble sgd noisy sgd
pretty closely uw here is a log
posterior and so we can really apply
these wherever we we apply std and often
achieve
better results through the bayesian
model average
now when we're um doing optimization
you know even though we sort of explore
the law surface to try to find a good
solution in the end all we care about
in principle is finding a solution with
low loss
whereas when we're sampling and we're
doing bayesian model averaging more
generally we really care about exploring
this surface and so
in this case the learning rate schedule
is particularly crucial for good
performance
and it's recently been found that a
cyclical learning rate schedule can
really
greatly enhance the ability for the
stochastic mcmc procedure to explore
complex multimodal loss surfaces and
achieve you know much better
practical performance so here we
increase the learning rate and we're
sort of exploring and then we decrease
and we sort of
explore within a mode and and you know
use those samples
deep ensembles has recently gained a lot
of popularity and has achieved a lot of
success
the idea is to specify neural net
architecture
retrain the neural net a bunch of times
to get a bunch of different sgd
solutions typically starting from
different initializations
as we get a bunch of different weights
uh and then
you know again in regression each model
can be specified to output a mean mu and
a variance sigma squared transfer say
gaussian likelihood
and once we have all these different
architectures corresponding the
different weights that we found through
sgd retraining we take an equally
weighted
ensemble
so you might be wondering why is deep
ensembles
in a section about practical methods for
bayesian neural nets
are deep ensembles bayesian in fact
aren't they often explicitly treated as
a competing approach to bayesian methods
in the next part part 4 will argue that
deep ensembles in fact provide
a better approximation to bayesian model
averaging than many of the methods that
we've described so far that
provide single basin marginalization
we'll also introduce an approach
multi-swag which generalizes deep
ensembles for
even more accurate bayesian model
averaging
we'll note briefly now though that for
example the average here
unlike with mc dropout uh actually does
collapse in the same way as a bayesian
model average
and so uh for example we're not
enriching the hypothesis space we're
just using
uh the same architecture retrained a
bunch of times with different maximum
likelihood or map solutions so as the
likelihood collapses so will the the
bayesian model average so
this ensemble also by
looking at different basis of attraction
we're capturing functional variability
which intuitively is going to be very
important for
estimating this bayesian predictive
distribution
when we're trying to do marginalization
we're trying to estimate that integral
and
given a finite amount of computation
being able to
uh spread where we're querying the lost
surfaces
across different basins of attraction is
going to be
very important for coming up with a good
approximation to that that integral so
in a sense it's
it's doing very well at approximating
the bayesian predictive distribution
given a finite number of resources
next we'll have part 4 on bayesian model
construction
and generalization hi i'm andrew wilson
at new york university
and welcome back to the icml 2020
tutorial on bayesian deep learning
this is part 4 on bayesian model
construction and generalization
in this part we'll consider deep
ensembles and their connection with
bayesian marginalization
we'll use this connection to describe
multi-swag a procedure which
marginalizes within multiple basins of
attraction
we'll also consider tempering prior
specification
rethinking generalization double descent
and with depth trade-offs
here we'll return to a function space
perspective that we considered in depth
in part two
we have a straight line function with a
distribution over its parameters w
naught and w1
we can visualize this induced
distribution over functions by sampling
from this distribution over parameters
and looking at the different straight
lines we get the different slopes and
intercepts
the gray shade here corresponds to a 95
credible set
and the solid blue line corresponds to
the expectation of this distribution
over functions
in this diagram here we have a
conceptualization of all possible data
sets on the horizontal axis
and the marginal likelihood or evidence
on the vertical axis
expressed in equation 74. the marginal
likelihood is the probability
that we would generate a data set if we
were to randomly sample from the
parameters the distribution of over the
parameters of our model
we can see here in equation 74 the
marginal likelihood is what we get when
we integrate away these parameters
in the example from the last slide we're
just generating
straight lines with different slopes and
intercepts so we're not able to generate
many data sets at all
but because the marginal likelihood is a
proper normalizable probability density
it'll have to assign a lot of mass to
those data sets
alternatively we can consider a model
like maybe a huge multi-layer perceptron
with the broad
distribution of its parameters which can
generate lots of different data sets but
each with not very much probability
we could alternatively consider a model
like a convolutional neural net which is
very flexible
so it can generate all sorts of
different data sets but it has very
particular inductive biases like
translation equivalence this idea that
if we translate an image its class label
remains unchanged and so
it will give a lot of mass to structured
image data sets
in order for a model to generalize it
needs to both have large support it
needs to be very flexible
and it should have good inductive biases
a good distribution of that support
so models like convolutional nets can
generalize very effectively on image
recognition problems because it supports
all sorts of different
data sets in this application domain but
it has a good distribution of support
through biases like translation
equivariance
we should be very careful not to
conflate flexibility and complexity as
we are describing in part two of the
talk we can have models like gaussian
processes
which are extraordinarily flexible even
universal approximators but
in a sense have very simple inductive
biases and can generalize very well on
problems with even a small number of
data points
in this figure here in addition to the
panel that we considered on the last
slide
we look at what happens as these three
models are exposed to
a given data set we can see that the
model with large support
with but good inductive biases both
contains the
ground truth description and is able to
contract efficiently around it
the model with truncated support
contracts quickly it's quickly
constrained by the available data but
erroneous
it contracts around an erroneous
solution the model with large support
but poor inductive biases that spreads
its support too thinly across too many
data sets
contains the truth but it doesn't
contract very efficiently
at the end of the last part part three
we were considering deep ensembles
and we started to hint at their
potential connection with bayesian
marginalization
recall that the predictive distribution
that we want to compute
is this integral of our conditional
predictive distribution given parameters
weighted by the posterior probabilities
of parameters given data
and so we want to in words consider
every possible setting of the parameters
and weight them by their posterior
probabilities
now this bayesian approach and the
classical approach will be
similar when the posterior is very
concentrated around a small number of
parameters or
when the conditional predictive
distribution y given w
is not varying a lot where
the posterior distribution w given d has
most of its mass
now in this top panel here we have a
conceptualization of the posterior
distribution
for a neural net multimodal lots of
global optima
and in the second row we have
a conditional distribution now with
with a deep neural net there isn't a lot
of functional variability within a
single basis of attraction
compared to between different bases of
attraction so we can see that
conceptualized in this second row where
we see that p of y given w
doesn't change very much within a basin
but it changes a lot between the basins
in the bottom row here we're thinking
about estimating this integral as an
active learning problem
we've observed a single solution say an
sgd solution here
in one of the basins and we're seeing
where we need to move in weight space
to to decrease the distance between
our approximate predictive distribution
and the exact predictive distribution
the exact answer to this integral
we can see that we would benefit a lot
more by moving to a new basin
then continuing to query points within
the same basin
and so indeed a lot of approaches
to bayesian marginalization focus their
efforts just on a single basis and they
do gaussian approximate posteriors
but in deep learning if we view this
integration problem as really an active
learning problem rather than through the
lens of simple monte carlo
then it's very advantageous given finite
computational resources to do something
like deep ensembles and just select
points from different basins and we'll
get
a better approximation to this integral
and in fact a more bayesian approach
than the bayesian approaches which are
marginalizing within a single basin
so we're in practice in in you know real
world we're always
handed computational constraints and we
try to do our best
given those constraints and deep
ensembles is actually a very good
heuristic for you know achieving a good
approximation to this integral given
those constraints
here we empirically test how close the
dpon solves predictive distribution
is to a near exact predictive
distribution
which we can see in panel a in panel b
we have the deep ensembles we can see
visually it's fairly similar to the
exact distribution
in panel c we have a variational
procedure doing single basin
marginalization with a gaussian
approximate posterior we can see that it
doesn't capture epistemic uncertainty so
well it's
it's doesn't have a lot of uncertainty
between different
uh clusters of data points and certainly
doesn't grow very consistently away from
the data
in the bottom right panel we look at
what happens is we
in terms of the distance between the
approximate predictive distribution
given by both deep ensembles and
stochastic variational inference as we
increase the number of samples we have
access to
we can see that that distance decreases
very quickly for deep ensembles
but hardly at all as we increase the
number of samples that we have at the
variational procedure so
continuing to see samples within a
single basin with this variational
approach
is not really improving our estimate of
the
of the of the bayesian model average
corresponding to this integral that we
want to compute
but training more models in the deep
ensembles is pretty dramatically
decreasing
the distance between our approximation
and the exact answer
so in addition to just selecting
different
points in different basins we can also
try to marginalize within basins of
attraction and we do that in a procedure
called multi-swag where we train
multiple independent swag
methods discussed in in part three uh to
create a mixture of gaussian's
approximation to the posterior
so we're marginalizing within multiple
basins
when we do that and apply multi-swag to
several of the applications in reference
one
evaluating predictive uncertainty under
data set shift
which show that deep ensembles often
outperform single basin marginalization
procedures
we see two key trends
one is that multi-swag tends to
significantly outperform deep ensembles
in cases where we're not training
very many independent models or when
there's a lot of data corruption
in purple here we have an additional
model multi-swa which takes the
means of the different gaussian
components in multi-swag and that
those will be in flatter regions of
those different basins of attraction
than
uh than say sgd solutions and so we see
uh you know a nice compromise here
recently a phenomenon called double
descent
has been of great interest a belkan at
all a couple years ago
showed that we could find these
so-called double descent curves on a
wide range of problems
where uh we have uh our loss on the
vertical axis
and we can see as we increase model
flexibility
initially loss appears to be decreasing
and then it
increases corresponding to overfitting
and then
it turns out it decreases again and so
this first regime is called the
classical regime which is in line with
classical intuitions about
generalization and the next regime is
referred to as the modern interpolating
regime so the training loss here just
you know keeps going down the test loss
has this really non-monotonic
behavior and the question is well why
don't we just keep overfitting why did
it what is suddenly generalization start
getting
better especially once we move past
where the training loss is zero
so i'd like to ask you and feel free to
pause the video for a moment
whether you think a bayesian model
should experience double descent given
what we've discussed so far and
considering this problem
especially think back to that first
example at the beginning of the talk
where we're considering airline
passenger numbers and which model we
would want to fit that data with
using a bayesian approach
so in that airline passenger number
examples i was saying that if we
have you know a reasonable prior and
we're doing exhaustive marginalization
then
we would expect a bayesian method to
improve
monotonically with increases in
flexibility we should embrace
flexibility of large support
and indeed that is actually what we see
if we apply this multi-swag procedure we
see that performance is essentially
monotonic
whereas sgd in this example where we
have c4 100 20
label corruption uh has a prominent
double descent
behavior uh swag where we're doing
single basin marginalization has a less
prominent double descent but you know
it's still
clearly there another really important
feature to note in this plot
is the fact that the multi-swag is
actually just
a lot more accurate than classical
training so
if we have uh you know a resnet uh with
layers of width 20
here we see just below 30
test error on c400 whereas if we're
using sgd we see about 45 percent tester
so it's a really massive discrepancy and
you know an empirical realization
this idea that if we're doing multimodal
marginalization
with a neural net which can capture all
sorts of complementary explanations to
the data
then we're really going to benefit a lot
empirically not just in terms of
uncertainty representation but also in
terms of accuracy
now we'll consider the prior and
function space that's induced by having
a gaussian prior in weight space where
we're just
varying the the the signal variance of
the prior for each of the parameters
alpha squared
and so there are two key results
that suggest that this actually induces
a pretty reasonable prior and function
space
first is the deep image prior which
showed that randomly initialized
confidence without training provide
excellent performance for image
denoising super resolution and in
painting
in other words a sample function from
this induced distribution over functions
captures low level image statistics
before any training another result
from the paper by zhang adol on
rethinking generalization
uh shows that if we pre-process c410
with a randomly initialized untrained
cnn
we get dramatically improved test
performance when we're
using a gaussian kernel directly on
pixels so it goes from 54
accuracy to 71 accuracy and then we just
get another two percent from l2
regularization which would be like
tuning this alpha perimeter
and so this is really saying that these
this induced distribution over functions
is fairly reasonable and
and in a way it's not surprising because
the properties of the induced
distribution over functions comes
largely from the functional form of the
model that's really a big
part of the prior translation
equivariance in sort of a similarity
metric that we get for
for different images etc you know this
is this is going to come from mostly the
functional form of the model
now um in bayesian deep learning it's
typical to consider what's called a
tempered posterior where we raise the
likelihood to a power one over t
where t is a temperature parameter t
less than one corresponds to a cold
posterior where the posterior is
more concentrated around solutions with
high likelihood t equals one corresponds
to the standard
bayesian posterior t is greater than one
corresponds to warm posteriors
where the prior effect is stronger and
posterior collapse is slower
in a paper at this icml by wenzel it all
uh
there's a result that's highlighted that
the the that with a standard
gaussian prior over parameters cold
posteriors often provide
improved performance but if we use just
a temperature equals one we can
we can get even worse performance than
classical training
in this paper they suggest the result is
due to prior misspecification and show
here that
sample functions uh from this induced
distribution or functions with the
standard normal prior seem to assign one
label to most classes on c410 and we
know the classes are balanced
so here we just sample from the standard
normal get
our function uh with our neural nets and
we see that it's giving most of the data
points class six here and another
sample function is giving most of the
data points class 9.
we examine this behavior as we vary the
scale
of the prior variance on the parameters
so for with for alpha equals root 10
we reproduce this results and we see
that one sample function is assigning
most of the data one class
another sample function was the date of
the other class however if we reduce
alpha we see that the samples are very
quickly
assigning about the same amount of data
at each of the classes
and in fact of course in practice we
would specify alpha through cross
validation or through
uh for example just you know what we
normally use for l2 regularization which
is roughly near 0.1 in this case
we also see that even for the
misspecified alpha the unconditional
predictive distribution
is actually quite reasonable it's
basically you know uniform over the
different
classes so it doesn't affect the
predictions
we can also examine the effect of data
on the posterior here so
um here we start with the prior the
misspecified prior that this you know
where sample functions are assigning
most of the data one class
we then observe just ten data points
with this huge resnet and we see that
already the predictive just that the
samples are giving almost uniform
predictions across the
the different different points 100 data
points you know even closer to uniform
and so this is a prior bias that is very
quickly modulated by data we can imagine
with the gaussian process for instance
if we
if we multiplied our kernel by some
factor uh
the the amplitude would be awkward if we
observe um
you know a few just few data points that
the posterior would
quickly collapse in that region so this
is the kind of
prior bias which isn't really going to
affect generalization that much in
practice what's much more important for
example is the induced covariance
function that we get
in this distribution over functions and
we saw this a bit in part two where
things like covariance function really
affected generalization
so here we have the induced correlation
function for
uh mnis digits different classes
um we can see that the correlations
generally decrease as we increase
alpha we also see that um pretty
consistently
uh the same classes the same classes are
most correlated and classes that are
visually similar more correlated than
other classes
this is a good sign this is also
suggestive in addition to the
the the deep image prior and the random
network features that
reducing a prior that is actually pretty
reasonable and you know which covariance
function for example you use
in a gaussian process is going to affect
generalization much much more
than you know whether you have a signal
variance perimeter that's a bit
misspecified
so some thoughts on tempering um i i
think it would be very surprising if t
equals one just happened to be the best
setting of this hyper parameter
um you know i in fact i think you know
we should we should always be doing
tempering it's basically just an
acknowledgement that our model is
mis-specified
that's not to say that we should you
know just say well
who cares about the model specification
will correct it with tempering we should
do our very best
to specify our prior as as honestly as
we can
but it's still going to be misspecified
and we should be honest about that too
and try to correct it by by learning the
temperature
and in fact this isn't really too
different than learning other properties
of
likelihood like noise
now while the the prior p of f x is
certainly
misspecified um in in
you know most cases in bayesian deep
learning the result of assigning one
class to most data is
is really a soft prior bias which one
doesn't hurt predict the predictive
distribution two
is easily corrected by appropriately
setting the prior parameter variance
alpha squared and three is quickly
modulated
by data what's much more important is
this induced covariance function over
images
uh in addition to not tuning alpha
in this whole posteriors paper um the
results
could have been exacerbated due to lack
of multimodal marginalization which
we've shown is extremely important for
generalization
it's also interesting to note that there
are cases when a cold
posterior a t less than one will be
helpful
in coming up with estimates even if we
believe the prior and the likelihood
if we have you know access to a finite
number of samples so
we can imagine for instance estimating
the mean of a standard normal
distribution
in high dimensions in this case the
samples will be very concentrated
around a norm of root d and so uh
if we decrease the temperature we'll
come up with a better estimate for the
mean
and so i encourage you to try this i
just sample from a high t
distribution look at a histogram of the
norms
so there was a paper a few years ago uh
called understanding deep learning
requires rethinking generalization they
showed that
confnets could fit cfar with random
labels and this was presented as if it
were you know in the face of everything
we know about generalization because
um uh uh it showed that the confidence
could
fit in noise they could greatly overfit
yeah they were generalizing in all sorts
of different
problems we can understand this result
from
the probabilistic functions based
perspectives so
in the top row here we have samples from
a gaussian process
in in panel a and panel b we see
some structured data and a gaussian
process predictive distribution which
looks reasonable
in panel c we see a lot of corrupted
data in red and we see an updated gp
predictive distribution which achieves
zero training error
now this red curve unlike the green
curve in panel b
looks nothing like the sample prior
functions however
the gp with the rbf kernel is very
flexible it contains somewhere in its
support
this red curve even though we might have
to sit there sampling for a very very
long time to see anything
like it however if there's a strong
enough likelihood signal we're saying
there's no noise then
it's going to run through that data
perfectly and produce this predictive
distribution
and so it doesn't want to fit that data
but it can
and we can quantify how much a model
wants to fit data with something called
the marginal likelihood
and so we can actually fit a gaussian
process
on a c410 with with altered labels and
show that we get zero percent training
error
so that means that you know this result
in the rethinking generalization paper
wasn't unique to deep neural nets you
can reproduce it with gaussian processes
and we can also compute the approximate
marginal likelihood and show that it
gets
very very low as we increase the number
of altered labels so that noisy c4 set
is somewhere in the tails of that
distribution where we're considering all
possible data sets and the marginal
likelihood of the vertical axis when
we're considering this
gp model with an rbf kernel uh the
uh we can also compute exactly the same
thing using a bayesian neural net
and find the approximate marginal
likelihood using whole class and we see
that it decreases
in in the same way as we increase the
number of altered labels
so this bayesian neural net is able to
fit that kind of data set but it really
doesn't want to
and we can quantify that with the
merchant likelihood and this is sort of
in accordance with
what we would want when we're thinking
about model construction we want
large support but we want to distribute
that support carefully and that means
not giving a lot of mass to things like
noise you see far
but if we see noisy c4 there's a strong
likelihood signal and we can fit it
okay so of course having said all that
um
the the priors in function space aren't
going to be perfect by just having sort
of
generic dowsing distributions or
parameters combine the functional forms
of
of neural nets um and we can certainly
do better and i i certainly embrace the
function space perspective in
constructing
compelling priors for these neural nets
at the same time we should be careful
not to contrive priors over parameters w
to induce distributions over functions
that resemble familiar models such as
gaussian processes with rbf kernels
we could be throwing the baby out with
the bath water and doing that
indeed drill nets are useful as their
own model class precisely because
they have different inductive biases
from other models we already have
gaussian processes with
rbf kernels and we can try to gain
insights in thinking in function space
but this is in a sense what we're
already doing when we're doing
architecture design that thinking and
function space trying to encode
properties such as translation
equivariance rotation equivariance
color and scale invariants and other
interesting properties that would
induce a compelling similarity metric
across our data instances
and so these properties really imbue the
associated distribution of our functions
with desirable properties for
generalization
and really all of the heavy lifting
for the the prior functions that we get
is is is from the the architecture and
um you know even if we're thinking
broadly
uh the functional form of a model is
like a strong prior
um it's a very strong assumption we
can't escape assumptions and we
shouldn't try to we should embrace
assumptions they're
you know we need to make assumptions to
to uh
have good generalization pack bays has
been a very exciting approach for
deriving explicit generalization error
bounds and stochastic networks with
posterior's q prior p
training points in uh uh the the pac
bay's generalization error bounds are
are based on this this term and equation
79
and uh we've uh recently uh uh you know
people have
have derived non-vacuous bounds
exploiting flatness in
queue with at least 80 generalization
accuracy
predicted on binary mnist uh this is you
know very exciting
um it's it's a promising framework uh
but it tends not to be prescriptive
about model construction
or informative for for understanding why
a model generalizes so in a sense it's
very complementary to what we've been
describing so far
in this tutorial and in fact if we were
to try to treat it as a prescription
what we would get is this sort of almost
contrary to a lot of what we've
discussed the bounds are improved by
compact priors for example
um and we've been saying well we want
prioritize with support for all sorts of
different data sets
um and a low dimensional parameter space
in a sense that the kl divergence is
going to be
uh often sort of hard to assign a lot of
overlap between two high dimensional
distributions and so we can
very often achieve better bounds by
doing model compression etc
um but here we've really been trying to
embrace having as many parameters as
possible for good generalization
um also generalization as we've shown
can be significantly improved by
a multi-modal posterior especially in
bayesian deep learning
uh this is you know really uh one of the
key practical takeaways
and in order to realize a good
approximation to the
the unconditional predictive
distribution we really need to do
multimodal marginalization
but the pac-based generalization bounds
aren't typically too influenced by
multimodal posteriors you end up with a
log factor it doesn't really change
things that much so
um that the pack based bounds are
are quite quite complementary and
they're very exciting in the sense that
they
they help us make sort of quantitative
sort of
explicit statements about generalization
error bounds
um uh but they don't tend to be too
prescriptive and this could be partly
also because although we in some cases
get non-vacuous bounds they're also
typically fairly loose
so what improves the bounds won't
necessarily improve generalization
we can relate posterior contraction to a
quantity called
effective dimension and actually gain
insights into
into behavior such as double descents so
uh
effective dimension is of the hessian
here
is the sum of lambda i over lambda i
plus alpha where alpha is a
regularization parameter lambda i are
the eigenvalues of the hessian when we
compute this quantity
of networks resnets of varying width
here we can see that it
tracks a test loss very closely as well
as you know test error
and um in fact the effective dimension
is going down
as we increase the size of the models
and this suggests that actually we're
getting simpler models even though they
have
more parameters we should be very
careful not to treat parameter counting
as a proxy for model
complexity and we can relate effective
dimension to
model compression uh in a sense this
sort of uh
counts the number of kind of sharp
directions uh in this this this
space given by the eigenvectors of the
hessian and um
we can see that that in this regime we
have basically zero training loss all
the different models are providing
lossless compressions of the data
and so the best compression will capture
the most regularity it'll have this
occam's razor property and tend to
provide the best generalization
now the reason sgd finds uh these these
kinds of solutions when we make the
the model size bigger is is largely
because
um you know flat region of the loss will
occupy a much greater volume in high
dimensions and so sgv will
be able to find them more easily we also
look at with depth trade-offs so looking
at
confidence of different widths and
depths and we can see above the green
partition where we have zero training
loss
effective dimension is a very good proxy
for generalization performance
the yellow curves here show level
cursives constant numbers of parameters
um and uh
you know what's interesting here to
especially to consider
different depths uh typically in in
recent years we've been looking at
infinite
widths of neural nets for neural tangent
kernels and things like this
um but in a sense you know depth is what
what gives
deep learning a lot of useful
hierarchical inductive biases that
provides good
generalization and representation
learning and so we can see that
this this quantity associated with
posterior contraction based in deep
learning
uh is is a reasonably good proxy for
generalization
um we can also look at the properties in
function space
as we move in directions given by
eigenvectors of the hessian with the
smallest eigenvalues and show that
the decision boundaries are essentially
unchanged that we get a lot of
functional homogeneity and this provides
a mechanism for why things like subspace
inference discussed in part
three of the talk uh work so well why we
can have a
model with tens of millions of
parameters and then basically just do
marginalization in a five dimensional
subspace and you know
see a big difference for that um and so
uh we can really understand you know
things like model compression by looking
at these quantities
in conclusion what we've been really
reiterating through this tutorial is
that the key defining feature of
bayesian methods is marginalization aka
bayesian model averaging and that's
going to be especially relevant in deep
learning because neural nets are very
underspecified by the data contain
all sorts of complementary and exciting
solutions to a given problem
and it really makes a lot of sense just
to use the sum and product rules of
probability to marginalize those
solutions
and really in trying to do this
marginalization as best as we can
we shouldn't think of the integration
purely through the lens of simple monte
carlo integration
um we should probably think of it more
as an active learning problem and by
doing that we can gain insights into
methods like deep ensembles and also
propose other kind of bayesian methods
that um provide even
better marginalization like multi-swag
um
and you know very excited to say in the
last year or so that a lot of bayesian
methods are now providing
better results than classical training
both in terms of accuracy
and uncertainty representation without a
lot of additional overhead
uh and uh you know really this
emphasizes there's a big difference
between your marginalization just
regularization as we saw in that coin
toss
example uh and you know we should be
careful not to conflate flexibility and
complexity
gaussian processes for instance can be
extremely flexible of an infinite number
of parameters
simple inductive biases good
generalization of a small number of
points
and also careful not to parameter count
to the proxy for complexity sometimes
the models of many more parameters in a
sense are helping us find simpler
solutions and we can really resolve a
lot of mysterious results in deep
learning
by thinking about model construction and
generalization from a probabilistic
perspective
so that's everything and i really like
to thank you for attending this tutorial&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" /><media:content medium="image" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Offline Reinforcement Learning</title><link href="https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/29/offline-rl.html" rel="alternate" type="text/html" title="Offline Reinforcement Learning" /><published>2020-10-29T00:00:00-05:00</published><updated>2020-10-29T00:00:00-05:00</updated><id>https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/29/offline-rl</id><content type="html" xml:base="https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/29/offline-rl.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-10-29-offline-rl.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;what-makes-modern-machine-learning-work?&quot;&gt;what makes modern machine learning work?&lt;a class=&quot;anchor-link&quot; href=&quot;#what-makes-modern-machine-learning-work?&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;so to start off let's start with a big question what is it that makes machine learning work you know i'm going to give a
simple answer to a big question this is
going to be
maybe somewhat controversial but perhaps
many of you will agree with this at
least at a very very high level
i think that what makes machine learning
work today is really
the combination of large and highly
diverse data sets
and large and high capacity models and
what we've seen time and time again
is that across a range of domains from
image recognition
to machine translation to text
recognition and speech recognition
this kind of formula seems to be the
formula that leads to good results
if you collect a large data set like
imagenet or ms coco
and then train a huge model with dozens
or even hundreds of layers
that's going to be the thing that leads
to very good performance and arguably
the widespread enthusiasm about machine
learning in recent years
has really been spurred on by
applications
that follow this basic formula
so what about reinforcement learning
what about uh using
learning to figure out how to make
decisions
well uh reinforcement learning is
fundamentally
at least in the textbook setting an
active learning framework where you have
an agent
that interacts with the world collects
some experience
uses that experience to update its model
policy or value function
and then collects more experience and
this process is repeated many many times
i and we've seen that this basic recipe
does lead to good results across
a range of domains from from playing
video games to basic robotic
locomotion and manipulation skills and
even to play in the game of go
however when you want
learning systems that generalize
effectively to large-scale real-world
settings
like the ones that i showed on the
previous slide you still have to collect
large and diverse data sets and in an
active learning framework
this means that each time the agent
interacts with the world they need to
collect
a breadth of experience that covers the
sort of situations they might see in the
world
for example if you imagine using
reinforcement learning to train an
autonomous driving system
now this system might need to practice
driving in
many many different cities each time it
updates the model
so it goes and drives in san francisco
new york berlin
london updates the model and goes back
to san francisco new york berlin and
london
now this very quickly becomes
impractical and indeed
if we look at the kind of domains in
which modern reinforcement learning
algorithms have been successful
and then contrasted side by side with
the kind of domains where we've seen the
success of supervised learning methods
that can leverage very large and diverse
data sets
we see that there's a really big gulf
it's not that the reinforcing learning
algorithms are not capable they're
learning very sophisticated strategies
very sophisticated behaviors
but they're not exhibiting the same kind
of open world generalization
all of the domains that i'm showing on
the left can all be characterized as
closed world settings
we know exactly what the rules of go are
we know how the emulator for the video
game works even the robotics application
is a laboratory setting
whereas all the applications shown on
the right are open world domains
images mined from flickr from all over
the world
natural text natural speech collected
from real human beings
actually speaking or writing text in the
real world
so if we want to bridge this gulf of
generalization
what can we do how can we bring
reinforcement learning into these open
world settings
to address the kind of applications that
we actually want
well i'm going to posit that in order to
enable this we really need to develop
data-driven reinforcement learning
methods and data driven
means that you need to be able to use
large and diverse previously collected
data sets
in the same way that supervised learning
algorithms can utilize large and diverse
previously collected data sets
the classic textbook version of
reinforcement learning
is really an on policy formulation in an
on policy reinforcement learning
algorithm
you have a setting where each time the
agent updates its policy
it has to go and collect new data
a very common area of study in
reinforcement learning is also to study
off policy reinforcement learning
algorithms and these algorithms can
utilize previous data
but they typically still learn in an
active online fashion
meaning that the agent interacts with
the world collects some data
adds that data to its buffer and then
uses the latest data and all pass data
to improve its policy
and we might think that we could simply
cut that connection we could simply use
prior data
using the same exact algorithms but as
i'll discuss in today's talk
that actually doesn't work very well and
if we really want data driven
rl algorithms then we need to come up
with some new algorithmic techniques to
make this possible
and develop a new class of what i'm
going to call offline reinforcement
learning methods
these have also been called batch
reinforcement methods in the literature
the idea here is that a data set was
collected previously
with some behavior policy that i'm going
to note pi beta in general you might not
even know what pi beta was maybe it was
a person driving a car a person
controlling the robot
maybe it was the robot's own experience
and other tasks whatever it was
it collected a buffer of data a large
and diverse data set
that covers many different open world
situations and now you have to learn the
best policy you can
for your current task using just that
data without the
luxury of interacting with the world
further to try out uh different methods
now of course in reality you might want
something more like a hybrid in reality
maybe
what we have is we have a big data set
from all the past interactions that our
agent has had
maybe you have a robot that has cleaned
the kitchen and mocked the floors and
did many other things and that's sort of
it's it's foundational data it's it's
imagenet
style data set that it's going to use to
get generalizable skills
we're going to use that data set in an
offline rl fashion
and then perhaps we'll actually come
back and interact with the world a
little bit more
just to fine-tune that skill just to
collect task specific
data in small amounts and i'll actually
discuss some methods that can
do this as well but the main component
the main challenge of this recipe is
really the first part is using large and
diverse previous data sets
to get the best policy you can without
having to revisit all those diverse and
varied situations
without having the car go back and drive
in san francisco and new york and berlin
and london each time its policy changes
and if we can accomplish this if we can
build algorithms that have this
capability
then we will not only be able to train
better policies for
robots and things like that but we'll
also be able to apply reinforcement
learning
to domains where conventionally it has
been very difficult to use
for instance we could imagine using past
data to use reinforcement learning
to train a policy for advising doctors
on treatment plans and diagnoses
it would be pretty crazy to imagine an
active exploration procedure for doing
this because that would result in
enormous costs and liability but if you
can use previously expected offline data
to get the best decision making system
you can to support doctors in a medical
setting
well that seems like a pretty good
recipe you can imagine the same
procedure being applied to scientific
discovery problems
controlling power plants chemical plants
operations research
logistics and inventory management
finance in all of these domains
there's ample previously collected
historical data
but it's extremely difficult and costly
to run active online data collection so
we would expect that these would be the
domains that could be revolutionized
with effective offline reinforcement
learning algorithms
so in today's talk i'm going to cover
the following topics
first i'll discuss why offline
reinforcement learning is difficult
then i'll talk about some basic recipes
for designing offline rl algorithms and
a little bit about recent progress in
this area i'll talk about
some of our recent work on model based
offline rl
and then i'll discuss a new algorithm
that we're pretty excited about called
conservative q learning
which seems to be a way to do offline
url that works quite a bit better
than many of the previous methods and
then i'll conclude with a discussion
of how we should be evaluating our
offline reinforcement learning
algorithms
and discuss a benchmark that we've
developed recently called d4rl
all right but let's start with the first
topic let's talk about what makes
offline reinforcement learning hard
and why we can't use our standard off
policy rl algorithms
to solve it so first
let's start with a quick primer on off
policy reinforcement learning
in a standard reinforced learning
problem you have an agent that interacts
with the world by selecting actions
and the world responds with states and
the rewards the consequences of those
actions
what the agent needs to do is select a
policy which i'm going to denote as pi
of a given s
and that policy needs to be selected so
as to maximize
the reinforced learning objective which
is the sum of expected rewards over all
time
so the agent doesn't just take the
action that yields the highest reward
right now
it's supposed to take the action that
yields the highest rewards
in the long run now a very useful object
for maximizing this reinforcement
learning objective
is the q function the q function tells
you
if you start a particular state take a
particular action
and then follow your current policy pi
what is the total reward that you will
accumulate
and if you can somehow determine the q
function for a given policy
then you can always improve that policy
by taking an action with probability one
if it is the r max of the q function and
zero otherwise
so this greedy policy with respect to q
pi will always be at least as good or
better than the previous pi
and you can also skip the middleman and
directly enforce
the bellman optimality equation for all
state action tuples
so if you may if you can make qsa equal
to rsa
plus the max of the q value at the next
time step then you can also show
that recovering the greedy policy from
this will get you the optimal policy
and one way you could do this is you
could enforce this equation at all
states
by minimizing the difference between its
left-hand side and right-hand side
now this is all fairly basic stuff this
is kind of what we learned
uh you know when we first learned about
reinforcement learning it's textbook
rl but you know as many of you might
already know
the nice thing about this equation is
that you don't necessarily need
on policy data to enforce it now the
stock is going to focus pretty much
entirely on approximate dynamic
programming methods basically
q learning style methods or after critic
methods there are many other methods for
offline rl
based more around policy gradient style
estimators and i'm not going to talk
about this so we'll basically start with
q learning and actor critic and sort of
stay there just as a as a warning
okay so this procedure that i've laid
out here this classic q learning
approach
is an off policy algorithm which means
that you don't need on policy data
to make the left-hand side and
right-hand side of the bellman
optimality equation
equal to each other so you could imagine
an off policy
include q learning method or a theta q
iteration procedure that has the
following basic recipe this is sort of
classic stuff from literature collect
the data set of transitions
using some policy add it to a buffer
sample a batch from that buffer minimize
the difference between the left hand
side and right hand side
of the bellman optimality equation and
then repeat this process some number of
times
and then periodically you could go out
and collect more data
you could also think of this graphically
you interacted with the world
collected a data set of transitions
you're going to run your q-learning
procedure on that data set
and then periodically go back and
explore and of course if we're doing
offline rl then this is the part that we
would omit we would just
take our buffer and just keep crunching
away with q learning on that buffer
so this is an off policy algorithm and
it can accommodate previously collected
data in principle
so does it solve the awful nrl problem
and if not what goes wrong
well we studied this question a little
bit so what i'm going to show next is
some results from a large-scale
reinforced learning project that we
actually did at google a couple of years
back called qt opt
our goal in this project is to develop a
highly generalizable and powerful
robotic grasping system that could learn
to grasp any object
directly from raw images but in the
process of doing this research
we actually evaluated a little bit how
these q learning style methods compare
in the fully offline regime
versus the regime where they're allowed
to collect additional online data
now here there's you know everything is
kind of scaled up so there are seven
different robots that are all collecting
lots of data in parallel they're pushing
it to a
decentralized uh distributed buffer
storing all the data from past
experiments and there are multiple
workers that are actually updating q
values
on everything in the buffer
and we looked at how the system worked
it actually worked really well
it could pick up novel objects heavy
objects awkward objects i could respond
to perturbations that i'd never seen
before so basically
when we scale up reinforcement learning
it does actually generalize something
is really working and we're seeing
generalization that kind of resembles
the kind of
uh open world generalization that we saw
in the supervised system
of course here one might argue it's not
entirely open world it's still in the
laboratory
but it is generalizing to never before
seen objects
so we evaluated this method on a test
set of objects of the policy never saw
during training we actually bought
entirely new objects just to ensure that
they were not in the lab
during the training process uh and
and then we compared the algorithm train
in fully offline mode
meaning using just the data collected
before trainable policy and see how it
does and this was fairly good offline
data it came from past rl experiments
and we also evaluated the algorithm in
fine-tuned mode where
the policy was allowed to collect a
little bit of additional data and
fine-tune online the offline data set
consisted of
580 000 uh episodes
so this is a very large grasping episode
the online fine tuning added another 28
000.
so the amount of additional online data
was pretty negligible
so it was clearly not the increase in
the data set size it was really that was
online
and we saw the following numbers the
success rate was 87
for the offline method and 96 with the
additional online fine-tuning
and while this might seem like a small
difference if we rewrite this as error
rates we have an error rate of 13
for the offline method and four percent
uh for the fine-tuned method so
less than a third of the number of
mistakes that's a pretty big difference
actually
so the system clearly works very well
but
you're getting three times fewer
mistakes
if you fine-tune you believe in a small
amount of data so something about the
fully offline setting
seems to be pretty hard now
more recently uh we actually studied
this question
in a paper called stabilizing off policy
queue learning via bootstrapping error
reduction
by overall kumar justin food george
tucker and myself
and we want to understand what what are
the reasons why this might be so
difficult
we had a few hypotheses that we wanted
to investigate one hypothesis we had was
maybe there's some kind of or fitting
effect maybe when you train on offline
data
if you train too long you sort of
overfit to that data and you start
seeing bad performance
but if it was an overfitting effect then
what we would expect
is that increasing the data set size
should decrease the problem
so what this plot shows is offline rl
performance
on the half cheetah benchmark and
different colors denote different data
sets so
blue is 1000 uh transitions and
red is 1 million transitions
and you can see that there is virtually
no difference between 1 000 and 1
million so
if there is an overfitting effect it is
not the conventional kind of overfitting
it doesn't seem to go away as you add
more data
now we also looked at how well the q
function thought it was doing
and meaning if you actually look at the
q values
for the current policy that tells you
how well the q function
thinks it's going to do when it's
executed in the world
and this is a plot showing that now
something to note here is that the y
axis here is actually a log scale so
what this is showing is
enormous overestimation actual
performance is below zero
estimated performance is between ten to
the seventh and ten to the twentieth
power
so the q function thinks it'll do
extremely well and it's doing extremely
poorly
well that's kind of weird um another
hypothesis we had as well
maybe the trouble is just the training
date is just not good like you know
maybe the best you can do with the state
is -250
now one way that you can evaluate that
hypothesis you guys should look at the
best
transitions in the data set if you just
copy the best transitions
how will we do we'll actually do
actually pretty well so
this is usually not the case in these
offline rl problems it's not that the
training data doesn't have good behavior
it's that somehow the q function becomes
excessively confident and optimistic
about actions that are not actually very
good
to understand what's happening here we
first have to understand distributional
shift
so here's distribution shift in a
nutshell
when we run supervised learning we're
typically solving what's called an
empirical risk minimization problem
so we have some data distribution p of x
we have some label distribution p of y
given x
our training set consists of samples
from p of x and p of y given x
and then we're going to minimize some
loss on those samples
and if we're doing vellum bellman error
minimization then we're minimizing the
squared error
between the values predicted by our
function f theta
and the target values y now when we run
empirical risk minimization
we could ask the question given some new
data point x star
is f theta of x star going to be correct
well one thing that we do know is if we
had enough samples
then the expected value of the error
under our training distribution should
be low that's what generalization means
generalization means the training error
which is the empirical risk is
representative of
generalization if you have a large
training set minimizing training error
is going to minimize generalization
error unless you overfit
but that doesn't necessarily mean that
we're going to make an accurate
prediction
on some other point x star
for example the expected value of our
error under some other distribution over
x
p bar of x is not in general going to be
low
unless p bar of x is equal to p of x so
our error under a different
input distribution might be in fact very
high
in fact even if x star was sampled from
p of x we're not guaranteed that error
on x star is low because we're
minimizing expected error
so we might still have some points with
higher we're not minimizing point wise
there or just expected error
now you might say that usually when we
train deep neural networks
we're not too worried about this kind of
distributional shift because deep
networks are really powerful they
generalize really really well
so maybe it's okay but what if we select
x star so as to maximize f theta of x
see we might have a function that fits
the true function really well
let's say that the green curve
represents the true function and the
blue curve represents our fit
mostly our fit is extremely good however
if we pick the largest value of f theta
of x
um which point are we going to land in
we're going to land exactly on that peak
we're going to find the point that has
the largest error in the positive
direction
so even if we're generalizing really
really well even if we're doing well for
x star points even ones outside of the
distribution
if we maximize the x we're going to get
big errors
so what does this have to do with q
learning well
let's look at our target values in q
learning it's r of s
a plus the max over a prime of q of s
prime comma a prime
and i'm going to rewrite this in kind of
a funny way i'm going to write this
as r plus the expected value of a prime
under the policy pi nu where pi nu is
this r max policy so pi nu
assigns the probability of 1 to the r
max now this is kind of just a really
weird way of writing the max
but i think it makes the distribution
shift problem much more apparent
so let's say that our target values are
called y
what is the objective for the q function
well it's to minimize
the empirical risk the empirical error
against y
under the distribution over states and
actions induced by our behavior policy
pi beta
which means that we would expect q to
give us accurate estimates of q values
under pi beta
right pi beta is the behavior policy
so we expect good accuracy when pi beta
is equal to pi nu so if pi nu is pi beta
then we're fine we're going we're going
to have lower estimates
but how often is that true pi news
chosen to maximize the q value so unless
pi beta
was actually also maximizing those same
q values before it even knew what they
were
these things are not going to match and
it's even worse
pioneer is selected to be the r max
and if you think back to the previous
slide when you select your point with a
max
you get some bad news and that's why we
see
on the safchita experiments that the
policy does poorly
whereas the q function thinks it's doing
really well because it's finding the
actions
with the largest error in the positive
direction and that's why just naively
applying off-pulse crl methods to the
offline setting
does not in general yield great results
so we have to somehow combat this
overestimation
issue we have to combat the
distributional shift
in the action space so let's talk about
how we can do this
how do we design offline rl algorithms
well one very large class of methods in
the literature and this is summarized in
a tutorial
that we assembled recently called
offline reinforcement tutorial review
and perspectives on open problems
one very large class of methods is what
we're going to call policy constraint
methods
so it's easiest to describe policy
constraint methods in the actor critics
setting but they can be applied in the q
learning setting too
so here we're going to update our q
function just like before using the
expected value under pi new
and then we'll update our policy not
just by taking the r
max but by taking the rmax subject to a
constraint
that some measure of divergence between
pi nu and pi beta
is bounded by epsilon so the idea is
that if
pi nu stays close to pi beta then the
distributional shift
will be bounded and therefore the
effective auto distribution actions will
be bounded
and then you repeat this process so it's
just like an actor critic algorithm
only with this additional constraint
against the behavior policy
so does a solid distributional shift
does it mean that we have no more
erroneous values
well to a degree it actually does so
there's a
pretty large class of methods that have
explored various kinds of policy
constraints
uh it's it's a very old idea we coined
the term policy constraint i think in
our tutorial
but it's this these kinds of approaches
have been called many things in
literature
including kl divergence control uh
maximum prl linearly cell blm dps all
sorts of things
and there are many uh researchers that
have studied this for decades
not just in the offline rl literature
but in the url literature more broadly
more recently this has been used
extensively for various off policy and
offline rl methods
here is just a collection of recent
citations kind of from the last five
years
that have done some variant of this
now this does have a number of problems
one problem is that
this kind of approach might be way too
conservative
imagine the following scenario let's say
that you have
a behavior policy that is
highly random in some places and almost
deterministic in other places
now when the behavior policy is highly
random you actually get pretty good
coverage of actions
and the effect of out of distribution is
actually minimized in fact if the
behavior policy was
uniformly random nothing would be out of
distribution because it has full support
but if you have a policy that is highly
random in some places and highly
deterministic in others
it's actually very difficult to pick a
value of epsilon that works
because if you if you pick a value of
epsilon that is too low
then in the places where the behavioral
policy is highly random you'll be able
to do all sorts of interesting things in
its support
but in the places where it's not highly
random you might take a really bad
action
right so imagine it's kind of close to
deterministic when it's
crossing a narrow bridge where with some
small probability it falls off
if you admit uh that amount of error you
might fall off the bridge
if you use a tighter constraint if you
limit the amount of deviation from the
behavior policy very strictly
then you won't fall off the bridge
you'll match it in that highly
deterministic region will you also be
forced
to match the highly random distribution
in the region where it's random and
that's just useless right
if the policy is being very random being
just as random
doesn't seem to give you anything like
if it's random you can just choose
within that support whatever you want
so it's hard to pick a single value of
epsilon that works better
now one thing that can mitigate this is
to use a support constraint
basically say it's not that you have to
match the distribution you have to match
the support of that distribution
so if the distribution is highly random
you can do sort of anything you want
within that support
but if it's highly deterministic you
really need to do whatever it did
because you don't have any wiggle room
to go out of support and we explored
this a little bit
uh in a in a paper from 2019 that
introduced an algorithm called bear
which uses support constraint the second
issue
which is a bit tougher to deal with and
actually pretty problematic in practice
is that estimating the behavior of the
the behavior policy itself
in order to enforce this constraint can
be very difficult
if you don't know what the behavior
policy was maybe the data was collected
by a human
you have to somehow figure out what
policy that human was following
and if you make a mistake in figuring it
out then your constraint might be
incorrect
so for example if you fit the behavior
policy with supervised learning
we know that supervised learning will
make certain mistakes it has kind of a
moment matching effect which means that
it will average across modes
which means you might have high
probability actions under your behavior
policy
that at a very low probability under the
true behavior policy
and when you enforce a constraint
against that the constraint will be
highly imperfect
and this can lead to very poor
performance in practice
now when is estimating the behavior
policy especially hard
well it's especially hard when the
behavior policy actually consists of a
mixture of multiple different behaviors
and this is actually exactly the setting
that we want because remember i
motivated all of this
by talking about the setting where you
have a large and highly diverse data set
and that's exactly what you would expect
that your
behavior policy would actually be a
mixture of many different policies
and at that point estimating it with a
single neural network is actually very
hard
so the easy case is when all the data
comes from the same markovian policy
but this is not very common or very
realistic uh you know if your data came
from humans they're certainly not going
to be markovian if it came from many
past tasks you've done
while each individual test might be
markovian the mixture might not be
so the hard case is where the data comes
from many different policies
and this is very common in reality and
it's also very common when you're doing
online fine-tuning so if you remember
when i motivated all this i also said
that we want to collect lots of data
from past behaviors train a policy with
offline rl and then maybe fine tune it
further with a little bit of online data
so let's use this online fine-tuning as
a kind of test case
in reality what we care about is the
setting where data comes from many
different policies but the online
fine-tuning situation
is a nice sort of petri dish in which to
explore this problem
so i'm going to talk about a method that
we've developed that specifically
targets that setting
where first you have offline training on
a large previously collected data
set and then you have some online
fine-tuning and during online
fine-tuning you're adding more data
from many different policies
so this is work by uh two of my students
astronautier and amazon gupta together
with an undergraduate student
named mortis and so the experiment is
that we do online fine-tuning from
offline initialization
and what i'm showing in this plot is the
log likelihood
of the behavior policy fit so this is
for one of these standard
policy constraint methods in this case
bayer and the left side shows the
offline training so you can see during
offline training we have pretty good log
likelihood for fitting our behavior
policy
but during online training when we have
these additional samples from
many different policies being pushed to
the buffer the likelihood
of the policy estimate drops
precipitously it drops very sharply for
the online data and even drops for the
old offline data because
you have to also model the new offline
data so you do worse on the offline
offline data so so our fit
gets bad and in fact we can see that uh
this method this is bare but this would
be true i think for for most policy
constraint methods
uh it doesn't do so well so this is just
showing the online fine tuning
and you're gonna have two choices you
can use a strict constraint or a loose
constraint
if you use a strict constraint then you
do a little bit better at the beginning
that's the yellow line
but you fail to improve because as the
behavior policy deteriorates
that strict constraint just basically
causes everything to get stuck
if you use a loose constraint then you
improve over the course of online
training but you start off in a really
bad place
and in this case it's no better than
just initializing from scratch
and in fact this general principle is
borne out in a few other papers for
example
this paper i have cited here called maq
shows that using a much more powerful
class of behavior policies
leads to substantial improvement
implying behavior policy modeling
is really a major bottleneck for these
policy constraint methods
so one of the things we could do is we
could actually enforce a constraint
without explicit modeling of the
behavior policy so this is a kind of
implicit constraint method
so here's the idea the problem we want
to solve
is this constrained argmax
it turns out that you can prove that the
solution to this constrained
optimization problem
can be expressed in closed form and the
way that you do this is you write down
the lagrangian
solve for the dual and then you can
actually write down the optimum for the
dual
and the optimal solution is given by one
over z times the behavior policy
times the exponential of the advantage
function
and the advantage function is multiplied
by one over lambda where lambda
is a lagrange multiplier and this is
straightforward to show using a duality
argument
uh we didn't actually come up with this
this is this is actually been brought up
in a number of previous papers including
the
reps papers by peters at all uh the
rolex at all sci learning paper and many
others this is kind of actually a very
well known identity
in kl regularized control
but the interesting thing about this
equation is that it shows
that we can approximate the solution to
the constrained optimization
using a weighted maximum likelihood
objective
and the way to do this is to note that
matching this pi
star is the same as
maximizing the likelihood of actions
taken from
pi star and you can do that by taking
action instead from pi
beta and weighting them by the
exponentiated advantage
so the if you can find the r
max of the expression i have written
here then you will get pi star
provided you can match pi star exactly
and the the cool thing about this
expression now is that
pi beta only shows up as the
distribution the expectation so even
though you need pi beta for this
in reality all you actually need is
samples from pi beta which means that
you do not need to actually know the
distribution pi beta
and samples from pi beta that's exactly
uh what our data set is composed of
so we no longer need to estimate the
behavioral policy we just need samples
from it and our data set already
contains those
and of course the advantage we get from
our critic so you could imagine an actor
critic algorithm
which updates the critic to get the
advantages and then updates the policy
using this weighted maximum likelihood
expression
all right so does this procedure work
well if we look at some results from the
paper we evaluated on these kind of
dexterous manipulation tasks
where we had a data set from human
demonstrations
uh with a with a data glove and then if
you look at what those human
demonstrations do they get a success
rate of about 24
on this door task with some online fine
tuning and get 88
success rate so this is doing much
better than if you just copy the
demonstrations
and you get meaningful fine tuning now
of course this is an offline rl method
so it doesn't actually need
demonstrations per se
that's just what we had in these
experiments we had other experiments
that also used random data
and quantitatively the method actually
does really well it's shown by this red
line here
the pen task is the easiest the door is
kind of medium and the relocate task is
the hardest and you can see that as the
tasks get harder
this method denoted as awac ends up
doing the best
so it kind of matches previous work on
the easy task and then greatly exceeds
it on the harder tasks
so this shows that something you know is
really working with these implicit
constraints
all right now let me take a little
detour to
delve a little bit into the world of
model based reinforcement learning
because
we can also develop effective offline rl
algorithms in the model based world too
so uh just a brief overview of
model-based rl
for those of you that aren't familiar
with it in model based rl
what we're going to do using data that
we collect from the environment is we're
going to fit a model we're going to fit
an
estimate to the transition probabilities
p hat of st plus 1
given st comma 80. and then we'll
somehow use that model
to train a policy pi of a t given st and
then typically repeat the process
so one way that you could imagine such a
model based rl algorithm working and
this is sort of a dyna style recipe
is that you collect real data denoted by
these block trajectories
you use that real data to fit your model
and then you're going to use that model
to make
kind of little short roll outs from
different states in your data set
and these little short rollouts will be
on policy using your latest policy
so the model kind of answers these
what-if questions if i were in this
state that i had seen before
what would have happened if i took a
different action and this of course lets
you train your policy much more
efficiently
and of course in offline rl we're going
to omit this error we're not going to
allow the algorithm to collect more data
it has to contend itself with the data
that it already has so what could go
wrong
well as you might have already guessed
the problem again is going to be
distributional shift
when we make these rollouts under the
model
if the policy that we're now evaluating
is very different from the policy they
originally collected the data
rolling out the model under that policy
will result in state visitations
that are very different from the states
under which the model was trained
and the model will make very large
mistakes and of course since the policy
is again being trained to maximize the
return
it'll learn to exploit the model to
essentially cheat so the model
erroneously produces good states
now in the in the literature one of the
ways that people have mitigated this
effect
is by just shortening these rollouts so
if you don't allow the policy to raw for
very many steps under the model there's
only so much exploiting that it can do
but in the fully offline setting of
course you don't want to do this because
your policy might deviate drastically
from the behavior policy so you want to
give it longer roll outs
so you can actually tell how well it's
doing
so the model's predictions will be
invalid if you get these out of
distribution states
very much like the problem we saw before
so
one solution we've developed and this is
uh joint work with
kenya you uh and garrett thomas who are
the co-first authors as well as a number
of collaborators
most of the team here is from stanford
university led by professors
chelsea finn and thank you mom and
there's also some concurrent work
by kadambi at all that also proposes a
very closely related algorithm called
moral
but i'm going to talk about our paper
called mopo
so the solution is to basically punish
the policy for exploiting these other
distribution states
so what we're going to do is we're going
to take our original reward function
and we'll subtract a penalty
mult with a coefficient lambda and this
penalty u
is essentially going to be some quantity
that is larger for states that are more
out of distribution or state action
tuples that are more out of distribution
so it's a kind of uncertainty penalty
and that's the only change we're going
to make we're just going to change our
reward like this
and then use some existing model based
rl algorithm the one that we used in
mopo is based on an algorithm called
mbpo model based policy optimization
so the idea now is when you visit one of
these outer distribution states
you get a really big penalty and that
results in the policy not wanting to
visit those other distribution states
instead of curving back
to the support of the data
okay so there's some pretty interesting
theoretical analysis that we can do for
this algorithm
and the theory this is kind of the
statement from the main theorem in the
paper
uh which states that under two technical
assumptions
the learned policy pi hat in the moba
algorithm satisfies the property that
the return of pi hat
is greater than or equal to the supremum
over pi
of uh the return of pi minus two times
lambda lambda
times epsilon u okay so what is what are
all these things
there are two technical assumptions one
is that we can basically represent
the value function so things will break
down badly if you have really bad
function approximation error and the
second one
is an assumption on you which says that
the true model error
is bounded above by you so this is this
is essentially saying that u is a
consistent estimator
for the error in the model and you need
this property
for basically because you need you to
mean something in practice the way that
we estimate mu
is by training an ensemble of models and
measuring their disagreement but any
estimator that upper bounds
the error in the model will do the job
so this is the true return of the policy
train under the model so not the return
the model thinks it has but the
return it actually has and
epsilon here is just the expected value
of u
so what this is saying is that the true
return of a policy we optimize under our
model
will actually have high will actually be
at least as high
as the best policy that still has to pay
this
error price another way of saying that
it'll be at least as good
as the best in distribution policy of
course you can get the optimal policy
because you don't know what happens out
of distribution we can at least have to
do at least as well
as the best in distribution policy
another way of saying this is that um
you can construct this policy pi delta
which is
the best policy whose error is bound
basically the best policy that
doesn't visit states with error larger
than delta
so this is under the true mdp but it's
saying you're going to
find the best policy under the true mdp
that doesn't visit states where the
model would have made mistakes
and this result says that the policy we
learn with model-based rl will be at
least as good as that policy
minus an error term that scales us two
times lambda times delta
so this basically shows that we'll get a
good policy within the support of our
data
so some implications of this this means
that we always improve over the behavior
policy
and we can actually quantify the
optimality gap against the optimal
policy in terms of model error so
basically
if our error on the states that the
optimal policy would visit is small
then we'll get close to the optimal
policy
empirically this method does very well
it outperforms regular mbpo it also
outperforms
quite a few of the previously proposed
policy constraint methods
all right so now let me discuss the last
algorithm i'm going to cover
which is called conservative q learning
conservative q learning takes a slightly
different approach
to offline rl from policy constraint
methods so
just to remind you the problem we saw
before is that the q function kept
thinking that it's going to do much
better than it's actually going to do
so it's going to overestimate like this
because when we take the r
max we get the point with the largest
positive error
so instead of trying to constrain the
policy what if we try to directly fix
the problem what if we directly push
down on erroneously high q values
one of the ways we could do this is we
can formulate a q
learning objective with a penalty so we
have the usual term which says
minimize development error and then we
have a penalty term which says
pick the actions with the high q values
and push down on those q values so
minimize the q values under this
distribution mu
which is chosen so that the q values
under mu are high
so this will basically find these
erroneous positive points and push them
down
it turns out that with this very simple
regularizer we can actually prove
that the q function we learn is a lower
bound on the true q function for the
policy pi
if you choose alpha the weight on the
regularizer appropriately
that's pretty cool we're actually
guaranteed not to overestimate if we do
this
um so this is work that was primarily
led by my student averal kumar
and the particular algorithm that
overall proposed is a kind of actor
critic style algorithm
where you learn the slower bound q
function for the current policy
uh so that q hat is less than or equal
to q
and then you update your policy now you
don't actually need to represent the
policy exactly it's just a little
explicitly you can still have a q
learning algorithm where it's implicit
where it's the r max policy
but it's a little easier to explain in
active critics setting we call this
conservative q learning though because
it's also very simple to instantiate as
a q learning method
now you can also derive a much better
bound for conservative q learning the
bound i had on the previous page
was actually too conservative it
actually pushed down the q values too
much
what you can do is you can push down on
the high q values
but you can compensate by also pushing
up on the q values in the data set
so you push down on the q values of the
q function thinks are high
and then you push up on the q values uh
for the actions in the data set
intuitively what this means that is that
if the high q values are all for actions
that are in the data set
then these right two terms will cancel
out and the regularizer goes away so
when you're in distribution
there is no regularization if you go
more out of distribution you get more
regularization
so these are the two error terms push
down on actions from you
push up on actions from d from the
dataset
now you're no longer guaranteed to have
a bound for all state action tuples if
you do this
but you are turns out still guaranteed
to have a bound in expectation under
your policy
so the expected q value under pi for q
hat will still be less than or equal to
the expected value
under the uh the true q function for pi
for all the states provided that alpha
is chosen appropriately of course
so um the full bound uh
since it's a the full conservative q
learning algorithm is shown here you
minimize the big q values and you
maximize
uh the q values under the data the full
bound is written out here
so the left side is the estimated value
function it's less than or equal to the
true value function
minus a positive pessimism term due to
the regularizer and then there's this
error term that accounts for sampling
error
so you just have to choose alpha so that
this positive pessimism term
is larger than the positive sampling
error term obviously if you have more
data then your sampling error is lower
and you don't have to worry as much if
you have high sampling error then you
need a higher alpha
to be conservative
okay so does this bound hold in practice
one of the things we did is we actually
empirically measured underestimation
versus overestimation on a simple
benchmark task
so what i'm going to be showing is
basically the expected value of the
learned q function
q hat minus the expected value of the
true q function
so if these values are negative that
means that q hat is less than
q and expectation if they're positive
that means we're overestimating
and we get the true q function by
actually rolling out the policy many
times and using monte carlo
so here are the results so the first
column shows this
the full cql method with both the
minimization and maximization term
the second column shows just the basic
method that has just the minimization
but no maximization
then we have four different ensembles so
an ensemble of two networks four
networks 10 networks and 28 networks
and then we have bayer which is a
representative policy constraint on it
now the first thing that you might note
is that all of the ensembles and the
policy constraint method
are overestimating massively despite the
policy constraint
the simple cq element that just has the
minimization
is underestimating but by quite a lot so
rewards here are on the order of a few
hundred
so getting minus 150 means that you're
very heavily underestimating
whereas the full cql method
underestimates but only by a little bit
so we are having we do have a lower
bound and the lower bound is pretty good
so the cqr always has negative errors
which means that it's pessimistic
all right now before i tell you how well
cql actually does empirically
i want to talk a little bit about how we
should evaluate offline rl methods
so how should we evaluate them well uh
ultimately what we're going to want to
do is train our offline rl methods using
large and highly diverse data sets
but in the meantime when we're just
prototyping algorithmic ideas
we need to somehow collect uh you know
some data to set up a benchmark task so
maybe one thing we could do is we could
collect some data using some online rl
algorithm and then use it to evaluate
offline rl
this is actually pretty typical in prior
work you train pi beta with online rl
and then you either collect data
throughout training or you take data
from the final policy
i'm going to claim that this is a really
bad idea this is a really terrible way
to evaluate offline reinforcement
learning methods and if you're doing
research on offline rl
you should not use data sets that have
just this kind of data
because if you already have a good
policy why bother with offline rl
but perhaps more importantly in the real
world that's not what your data is going
to look like
your data is not going to come from a
markovian policy it's going to come from
humans from many different sources from
your past
behavior it's going to be multitask it's
going to be diverse it's not going to
look like the replay buffer
for an online url run so human users and
engineer policies etc
so if you really want to value your
offline rl method you really have to use
data that is representative
of real-world settings and leaves plenty
of room for improvement
and then offline allows to learn
policies that are much better than the
behavior policy that are better than the
best thing in your data set
without testing for these properties you
can't really trust
that our offline rl algorithms are good
and
of course in past work from my group
we've also been guilty of doing this but
we're we're mending our ways we're not
going to do this anymore
so we developed a benchmark suite called
d4rl it stands for data sets for data
driven dprl
this was led by my student justin food
together with avril kumar
uh of your not true truman george tucker
and d4rl is actually rapidly uh you know
picking up it's rapidly becoming the
most popular benchmark for offline rl
because it really exercises the kinds of
properties you want in offline rl
algorithms
so what are some important principles to
keep in mind well you want data from
non-rl policies
including data from humans so we
included things like the dexterous
manipulation data that i showed before
this was based on data collected by
argentoswar and colleagues
you want to evaluate whether your
algorithm can put together different
parts of different trajectories we'll
call this stitching
so if you've seen for example you can go
from a to b and you've seen that you can
go from b to c
but it's more optimal to go from a to c
the data actually tells you everything
you need to know to figure that out
so you should be able to do this and do
better than the best trajectory in the
data set
uh so we evaluate this using some maze
navigation tasks both in a simple low
dimensional 2d setting
and a complex setting where you've got a
simulated four-legged robot actually
walk through the maze
so you never see full paths from the
start to the goal but you see paths
between different places in the maze in
your data center
and you also have to have realistic
tasks we included first person driving
data from images using the carlos
simulator
data manipulating objects in a kitchen
from paper by abhishek gupta called
relay policy learning
and traffic control data from professor
alex bines lab
that simulates the effect of autonomous
cars on traffic
so the set of uh d4l tasks includes the
standard
mujoco gym tasks
with some difficult data sets the
stitching tasks and the mazes
dexterous manipulation tasks with data
from real humans
robot manipulation tests in this kitchen
environment again using human data
traffic control data from a flow
simulator and
image-based driving in karla
now if we look at how cql compares on
this benchmark first let me show you the
prior methods
one of the things to note is on the
harder benchmark tasks
we actually see that first of all
nothing works on the harder stitching
task so
on the larger mazes previous methods
basically don't learn anything these
scores are all normalized between 0 and
100.
the most competitive baseline across all
of these harder tasks
is just simple behavior cloning which
suggests that previously proposed
offline reinforcement learning methods
which have primarily been tested
on data from other rl policies are not
actually doing very well
so nothing beats behavior cloning on
these harder tasks
if we look at the performance of cql it
achieves state-of-the-art results on
nearly all of these tasks
so i'm showing two variants of cql and
one of these two variants
is the best on all but one of the tasks
or tied for the best so it's up to five
times better on the harder dexterous
manipulation tasks
fifty uh to three hundred percent better
on on the
adjust the human data 10 to 30 better on
the kitchen tasks
and essentially infinitely better on the
larger mazes where it's the only
algorithm that's able to exhibit the
stitching behavior
we also evaluated the method on atari
data from paper by agrowall at all and
we saw there also that
cql is 50 to 600 percent better uh than
previously proposed algorithms
so this method is doing really well it
seems to work quite well across many
tasks
and we seem to know why it works because
of this lower bound property
of course there's still plenty of room
for improvement so if you want to
develop better offline rl methods
there there's plenty of work to do and
plenty of ways
in which you can improve the results all
right
so just to wrap up and conclude i talked
about how offline rl
is quite difficult but has enormous
promise and initial results suggest that
it can be extremely powerful
i talked about how effective dynamic
programming offline rl methods can be
implemented by imposing constraints on
the policy and perhaps implicit
constraints
can get around the need to model the
behavior policy and i talked about how
this
learning a lower bound on the q function
using conservative q learning
can substantially improve offline rl
performance thank you very much&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" /><media:content medium="image" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Model Based Reinforcement Learning (MBRL)</title><link href="https://kargarisaac.github.io/blog/reinforcement%20learning/mbrl/jupyter/2020/10/26/mbrl.html" rel="alternate" type="text/html" title="Model Based Reinforcement Learning (MBRL)" /><published>2020-10-26T00:00:00-05:00</published><updated>2020-10-26T00:00:00-05:00</updated><id>https://kargarisaac.github.io/blog/reinforcement%20learning/mbrl/jupyter/2020/10/26/mbrl</id><content type="html" xml:base="https://kargarisaac.github.io/blog/reinforcement%20learning/mbrl/jupyter/2020/10/26/mbrl.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-10-26-mbrl.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This post is a summary (almost!) of the model-based RL tutorial at ICML-2020 by &lt;a href=&quot;https://twitter.com/IMordatch&quot;&gt;Igor Mordatch&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/jhamrick&quot;&gt;Jess Hamrick&lt;/a&gt;. You can find the videos &lt;a href=&quot;https://sites.google.com/view/mbrl-tutorial&quot;&gt;here&lt;/a&gt;. The pictures are from the slides in the talk.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Introduction-and-Motivation&quot;&gt;Introduction and Motivation&lt;a class=&quot;anchor-link&quot; href=&quot;#Introduction-and-Motivation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Having access to a world model, and using it for decision-making is a powerful idea. 
There are a lot of applications of MBRL in different areas like robotics (manipulation- what will happen by doing an action), 
self-driving cars (having a model of other agents decisions and future motions and act accordingly),
games (AlphaGo- search over different possibilities), Science ( chemical use-cases),
and operation research and energy applications (allocate renewable energy at different points in time to meet the demand).&lt;/p&gt;
&lt;h2 id=&quot;Problem-Statement&quot;&gt;Problem Statement&lt;a class=&quot;anchor-link&quot; href=&quot;#Problem-Statement&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In sequential decision making, the agent will interact with the world by doing action $a$ and getting the next state $s$ and reward $r$.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/rl.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We can write this problem as a Markov Decision Process (MDP) as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;States $S \epsilon R^{d_S}$&lt;/li&gt;
&lt;li&gt;Actions $A \epsilon R^{d_A}$&lt;/li&gt;
&lt;li&gt;Reward function $R: S \times A \rightarrow R$&lt;/li&gt;
&lt;li&gt;Transition function $T: S \times A \rightarrow S$&lt;/li&gt;
&lt;li&gt;Discount $\gamma \epsilon (0,1)$&lt;/li&gt;
&lt;li&gt;Policy $\pi: S \rightarrow A$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The goal is to find a policy which maximizes the sum of discounted future rewards:
$$
\text{argmax}_{\pi} \sum_{t=0}^\infty \gamma^t R(s_t, a_t)
$$
subject to
$$
a_t = \pi(s_t) , s_{t+1}=T(s_t, a_t)
$$&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;How to solve this optimization problem?!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Collect data $D= \{ s_t, a_t, r_{t+1}, s_{t+1} \}_{t=0}^T$.&lt;/li&gt;
&lt;li&gt;Model-free: learn policy directly from data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
$$ D \rightarrow \pi \quad \text{e.g. Q-learning, policy gradient}$$
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model-based: learn model, then use it to &lt;strong&gt;learn&lt;/strong&gt; or &lt;strong&gt;improve&lt;/strong&gt; a policy &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
$$ D \rightarrow f \rightarrow \pi$$
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-is-a-model?&quot;&gt;What is a model?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-a-model?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;a model is a representation that explicitly encodes knowledge about the structure of the environment and task.&lt;/p&gt;
&lt;p&gt;This model can take a lot of different forms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A transition/dynamic model: $s_{t+1} = f_s(s_t, a_t)$&lt;/li&gt;
&lt;li&gt;A model of rewards: $r_{t+1} = f_r(s_t, a_t)$&lt;/li&gt;
&lt;li&gt;An inverse transition/dynamics model (which tells you what is the action to take and go from one state to the next state): $a_t = f_s^{-1}(s_t, s_{t+1})$&lt;/li&gt;
&lt;li&gt;A model of distance of two states: $d_{ij} = f_d(s_i, s_j)$&lt;/li&gt;
&lt;li&gt;A model of future returns: $G_t = Q(s_t, a_t)$ or $G_t = V(s_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Typically when someone says MBRL, he/she means the firs two items.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/model.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Sometimes we know the ground truth dynamics and rewards. Might as well use them! Like game environments or simulators like Mujoco, Carla, and so on.&lt;/p&gt;
&lt;p&gt;But we don't have access to the model in all cases, so we need to learn the model. In cases like in robots, complex physical dynamics, and interaction with humans.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;How-to-use-a-model?&quot;&gt;How to use a model?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-use-a-model?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In model-free RL agent, we have a policy and learning algorithm like the figure below:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/rl2.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In model-based RL we can use the model in three different ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;simulating the environment: replacing the environment with a model and use it to generate data and use it to update the policy.&lt;/li&gt;
&lt;li&gt;Assisting the learning algorithm: modify the learning algorithm to use the model to interpret the data it is getting differently. &lt;/li&gt;
&lt;li&gt;Strengthening the policy: allow the agent at test time to use the model to try out different actions before it commits to one of them (taking action in the real world).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbrl.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In general, to compare model-free and model-based:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbrl_vs_mfrl.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;How-to-learn-a-model?&quot;&gt;How to learn a model?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-learn-a-model?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Two different dimensions are useful to pay attention to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;representation of the features for the states that the model is being learned over them&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;representation of the transition between states&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To continue, we take a look at different transition models.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;state-transition-models&quot;&gt;state-transition models&lt;a class=&quot;anchor-link&quot; href=&quot;#state-transition-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We know equations of motion and dynamics in some cases, but we don't know the exact parameters like mass. We can use system identification to estimate unknown parameters like mass. But these sorts of cases require having a lot of domain knowledge about how exactly the system works.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model2.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model3.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In some cases that we don't know the dynamics of motion, we can use an MLP to get a concatenation of $s_t, a_t$, and output the next state $s_{t+1}$.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model4.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In cases that we have some, not perfect, domain knowledge about the environment, we can use graph neural networks (GNNs) to model the agent (robot). For example, in Mujoco, we can model a robot (agent) with nodes as its body parts and edges as joint and learn the physics engine.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model5.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;observation-transition-models&quot;&gt;observation-transition models&lt;a class=&quot;anchor-link&quot; href=&quot;#observation-transition-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In these cases, we don't have access to states (low-level states like joint angles), but we have access to images. The MDP for these cases would be like this:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model6.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;So what can we do with this?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Directly predict transitions between observations (observation-transition models)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model7.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reconstruct observation at every timestep: Using sth like LSTMs. Here we need to reconstruct the whole observation in each timestep. The images can be blurry in these cases.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model8.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model88.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;latent-state-transition-models&quot;&gt;latent state-transition models&lt;a class=&quot;anchor-link&quot; href=&quot;#latent-state-transition-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Another option when we have just access to observation is to instead of making transition between observations we can infere a latent state and then make transitions in that latent space (latent state-transition models) not in the observation space. It would be much faster than reconstructing the observation on every timestep. We take our initial observation or perhaps the last couple of observations and embed them into the latent state and then unroll it in time and do predictions in $z$ instead of $o$.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model9.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Usually we use the observation and reconstruct it during training but at test time we can unroll it very quickly. we can also reconstruct observation at each timestep we want (not necessarily in all timesteps).&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model10.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Structured-latent-state-transition-models&quot;&gt;Structured latent state-transition models&lt;a class=&quot;anchor-link&quot; href=&quot;#Structured-latent-state-transition-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Another thing that you can do if you have a little bit more domain knowledge is to add a little bit of structure into your latent state. For example, if you know that the scene that you are trying to model consists of objects, you can try to actually explicitly detect those objects, segment them out and then learn those transitions between objects.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model11.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Recurrent-value-models&quot;&gt;Recurrent value models&lt;a class=&quot;anchor-link&quot; href=&quot;#Recurrent-value-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The idea is that when you unroll your latent-state, you additionally predict the value of the state at each point of the future, in addition to reward. We can train the model without necessarily needing to train using observations, but just training it by predicting the value progressing toward actual observed values when you roll it out in the real environment.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model12.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Why is this useful? Because some types of planners only need you to predict values rather than predicting states like MCTS (Monte Carlo tree search).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Non-Parametric-models&quot;&gt;Non-Parametric models&lt;a class=&quot;anchor-link&quot; href=&quot;#Non-Parametric-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;So far, we talked about parametric ways of learning the model. We can also use non-parametric methods like graphs.&lt;/p&gt;
&lt;p&gt;For example, the replay buffer that we use in off-policy methods can be seen as an approximation to a type of model, where if you have enough data in your replay buffer, you can sample from the buffer and basically access the density model over your transitions. You can use extra replay to get the same level performances you would get using a model-based method that learns a parametric model.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model13.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We can also use data in the buffer to use data points and learn the transition between them and interpolate to find states between those states in the buffer. Somehow learning distribution and use it to generate new data points.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model14.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Another form of non-parametric transition is a symbolic description popular in the planning community, not in the deep learning community.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model15.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The other form of non-parametric models is gaussian processes, which give us strong predictions using a very small amount of data. PILCO is one example of these algorithms.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model16.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Model-based-control-and-how-to-use-a-model?&quot;&gt;Model-based control and how to use a model?&lt;a class=&quot;anchor-link&quot; href=&quot;#Model-based-control-and-how-to-use-a-model?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We will be using this landscape of various methods and categories that exist, including some representative algorithms:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc1.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;As we saw earlier, we can use the model in three different ways. In continue, we will see some examples of each case.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Simulating-the-environment&quot;&gt;Simulating the environment&lt;a class=&quot;anchor-link&quot; href=&quot;#Simulating-the-environment&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc2.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;One way is to mix the real data with model-generated experience and then apply traditional model-free algorithms like Q-learning, policy gradient, etc. In these cases, the model offers a larger and augmented training dataset.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dyna-Q&lt;/strong&gt; is an example that uses Q-learning with a learned model. Dyna does the traditional Q-learning updates on real transitions and uses a model to create fictitious imaginary transitions from the real states and perform exactly the same Q-learning updates on those. So it's basically just a way to augment the experience.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc3.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This can also be applied to policy learning. We don't need to perform just a single step but multiple steps according to the &lt;strong&gt;model&lt;/strong&gt; to generate experience even further away from the real data and do policy parameter updates entirely on these fictitious experiences.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc4.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Assisting-the-learning-algorithm&quot;&gt;Assisting the learning algorithm&lt;a class=&quot;anchor-link&quot; href=&quot;#Assisting-the-learning-algorithm&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc5.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;One important way that this can be done is to allow end-to-end training through our models. End-to-end training has recently been very successful in improving and simplifying supervised learning methods in computer vision, NLP, etc.&lt;/p&gt;
&lt;p&gt;The question is, &quot;can we apply the same type of end-to-end approaches to RL?&quot;&lt;/p&gt;
&lt;p&gt;One example is just the policy gradient algorithm. Let's say we want to maximize the sum of the discounted future reward of some parametric policy. We can write the objective function with respect to the policy parameters $\theta$&lt;/p&gt;
$$
 J(\theta) = \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t)  , \quad a_t = \pi_{\theta}(s_t) , \quad s_{t+1} = T(s_t, a_t)
$$&lt;p&gt;Now we need to apply gradient ascent (for maximization) on policy gradient with respect to policy parameters $\theta  \rightarrow  \nabla_{\theta}J$.&lt;/p&gt;
&lt;p&gt;So how can we calculate this $\nabla_{\theta}J$ ?&lt;/p&gt;
&lt;p&gt;Sampling-based methods have been proposed, like REINFORCE, to estimate this gradient. But the problem with them is that they can have very high variance and often require the policy to have some randomness to make decisions. This can be unfavorable.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc6.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The next solution is when we have accurate and smooth models. Accurate and smooth models, aside from imaginary experiences, offer derivatives:&lt;/p&gt;
$$
s_{t+1} = f_s(s_t, a_t) \quad  r_t = f_r(s_t, a_t)
$$$$
\nabla_{s_t}(s_{t+1}), \quad \nabla_{a_t}(s_{t+1}), \quad \nabla_{s_t}(r_t), \quad \nabla_{a_t}(r_t), \quad ...
$$&lt;p&gt;And they are able to answer questions such as: &lt;em&gt;how do small changes in action change next state or reward any of other quantities?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc7.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Why is this useful? This is useful because it will allow us to do this type of end-to-end differentiation algorithms like &lt;strong&gt;back-propagation&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let's rewrite our objective function using models:&lt;/p&gt;
$$
 J(\theta) \approx \sum_{t=0}^{H} \gamma^t r_t  , \quad a_t = \pi_{\theta}(s_t) , \quad s_{t+1} = f_s(s_t, a_t), \quad r_t=f_r(s_t,a_t)
$$&lt;p&gt;So how can we use these derivatives to calculate $\nabla_{\theta}J$ ?&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc8.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The highlighted derivatives are easy to calculate using some libraries like PyTorch or TensorFlow.&lt;/p&gt;
&lt;p&gt;By calculating $\nabla_{\theta}J$ in this way:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;pros&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The policy gradient that we get is actually a deterministic quantity, and there is no variance to it. &lt;/li&gt;
&lt;li&gt;It can support potentially much longer-term credit assignment&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;cons&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is prone to local minima&lt;/li&gt;
&lt;li&gt;Poor conditioning (vanishing/exploding gradients)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are two examples to use model-based back-propagation (derivatives) either along real or model-generated trajectories to do end to end training:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc9.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;real trajectories are safer but need to be from the current policy parameters (so it’s less sample-efficient)&lt;/li&gt;
&lt;li&gt;model-generated trajectories allow larger policy changes without interacting with the real world but might suffer more from model inaccuracies&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Strengthening-the-policy&quot;&gt;Strengthening the policy&lt;a class=&quot;anchor-link&quot; href=&quot;#Strengthening-the-policy&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;So far, we talked about the first two ways of using a model in RL. These two ways are in the category of &lt;strong&gt;Background Planning&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;There is another category based on the &lt;em&gt;Sutton and Barto (2018)- Reinforcement Learning: An Introduction&lt;/em&gt; categorization, called &lt;strong&gt;Decision-Time Planning&lt;/strong&gt;, which is a unique option we have available in model-based settings.&lt;/p&gt;
&lt;h4 id=&quot;What-is-the-difference-between-background-and-decision-time-planning?&quot;&gt;What is the difference between background and decision-time planning?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-the-difference-between-background-and-decision-time-planning?&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In background planning, we can think of it as answering the question, &quot;how do I learn how to act in any possible situation to succeed and reach the goal?&quot;&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc10.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The optimization variables are parameters of a policy or value function or ..., and are trained using expectation over all possible situations.&lt;/li&gt;
&lt;li&gt;Conceptually, we can think of background planning as learning a set of habits that we could reuse.&lt;/li&gt;
&lt;li&gt;We can think of background planning as learning a fast type of thinking.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In decision-time planning, we want to answer the question, &quot;what is the best sequence of actions just for my current situation to succeed or reach the goal?&quot;&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc11.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The optimization parameters are just a sequence of actions or states.&lt;/li&gt;
&lt;li&gt;Conceptually, we can think of decision-time planning as finding our consciously improvising just for the particular situation that we find ourselves in.&lt;/li&gt;
&lt;li&gt;We can think of decision-time planning as learning a slow type of thinking.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Why use one over the other?&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc12.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Act on the most recent state of the world&lt;/em&gt;: decision-time planning is just concerned about the current state in finding the sequence of actions. You can act based on the most recent state of the world. By contrast, in background planning, the habits may be stale and might take a while to get updated as the world's changes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Act without any learning&lt;/em&gt;: decision-time planning allows us to act without any learning at all. There is no need for policy or value networks before we can start making decisions. It is just an optimization problem as long as you have the model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Competent in unfamiliar situations&lt;/em&gt;: if you find yourself in situations that are far away from where you were training, your set of habits or policy network might not have the competence (the ability to do something successfully or efficiently) there. So you don't have any information to act or are very uncertain, or even in the worst case, it will with confidence make decisions that just potentially make no sense. This is out of distribution and generalization problem. In these cases, decision-time planning would be more beneficial.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Independent of observation space&lt;/em&gt;: another advantage of decision-time planning is that it is also independent of the observation space that you decide on. In background methods, we need to consider some encoding or description of the state, joint angles, or pixels or graphs into our policy function. These decisions may play a large role in the total learning performance. When something is not working, you will not really know that is it because of the algorithm or state-space, which doesn't contain enough information. In contrast, decision-time planning avoids this confounded, which in practice can actually be quite useful when you're prototyping new methods.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc13.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Partial observability&lt;/em&gt;: decision-time plannings have some issues with it. They assume that you know the full state of the world when you're making the plan. So it's hard to hide information from decision-time planners. It is possible, but it is more costly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Fast computation at deployment&lt;/em&gt;: decision-time planners require more computation. It is not just evaluating a habit, but it needs more thinking.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Predictability and coherence&lt;/em&gt;: decision-time planners do some actions which are not necessarily predictable or coherent. Because you are consciously thinking about each footstep, you might not have exactly the same plan. So you may have a very chaotic behavior that still succeeds. In contrast, background planning, because it learns a set of habits, it can perform a very regular behavior.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Same for discrete and continuous actions&lt;/em&gt;: background planning has a very unified treatment of discrete and continuous actions, which is conceptually simpler. In decision-time planning, there are different algorithms for discrete and continuous actions. We will see in the following sections more about them.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also mix and match the background and decision-time plannings.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;What-is-the-difference-between-discrete-and-continuous-planning?&quot;&gt;What is the difference between discrete and continuous planning?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-the-difference-between-discrete-and-continuous-planning?&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;It depends on the problem which you want to solve. So it is not a choice that you can make. For example, in controlling a robot, the actions might be the torques for the motors (continuous), or in biomechanical settings, it might be muscle excitations (continuous), or in medical problems, the treatment that should be applied (discrete).&lt;/p&gt;
&lt;p&gt;The distinction between discrete and continuous actions is not significant for background planning methods.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You just learn stochastic policies that sample either from discrete or continuous distributions.&lt;/li&gt;
&lt;/ul&gt;
$$
a \sim \pi(.|s) \quad \leftarrow Gaussian, categorical, ...
$$&lt;ul&gt;
&lt;li&gt;Backpropagation is still possible via some reparametrization techniques. See &lt;em&gt;Jang et al (2016). Categorical reparametrization with Gumbel-Softmax&lt;/em&gt; for an example.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In either of these cases (continuous and discrete in background planning methods), your final objective and optimization problem is still smooth wrt the policy parameters because you are optimizing over expectations.&lt;/p&gt;
$$
J(\theta) = E_{\pi}[\sum_t r_t], \quad a_t \sim \pi(.|s_t, \theta)
$$&lt;p&gt;But for decision-time planning, this distinction leads to specialized methods for discrete and continuous actions: discrete search or continuous trajectory optimization.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Let's see some examples to be able to compare them.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc14.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;MCTS-(monte-carlo-tree-search)&quot;&gt;MCTS (monte carlo tree search)&lt;a class=&quot;anchor-link&quot; href=&quot;#MCTS-(monte-carlo-tree-search)&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;This algorithm is in a discrete action group and is used in alpha-go and alpha-zero. You keep track of Q-value, which is long term reward, for all states and actions that you want to consider. And also the number of times that the state and action have been previously visited.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize $Q_0(s, a) = 0, N_0(s, a)=0, k=0$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc15.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Expansion: Starting from the current situation and expand nodes and selecting actions according to a search policy: &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;
$$\pi_k(s) = Q_k(s,a)$$
&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc16.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Evaluation: When a new node is reached, estimate its long-term value using Monte-Carlo rollouts&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc17.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Backup: Propagate the Q-values to parent nodes:&lt;/li&gt;
&lt;/ol&gt;
$$
Q_{k+1}(s, a) = \frac{Q_k(s,a) N_k(s,a) + R}{N_k(s,a)+1}
$$$$
N_{k+1}(s,a) = N_k(s,a)+1
$$&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc18.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Repeat Steps 2-4 until the search budget is exhausted.
$$
k = k + 1
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc19.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Trajectory-Optimization&quot;&gt;Trajectory Optimization&lt;a class=&quot;anchor-link&quot; href=&quot;#Trajectory-Optimization&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;Instead of keeping track of a tree of many possibilities, you keep track of one possible action sequence.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize $a_0, ..., a_H$ from guess&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc20.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Expansion&lt;/strong&gt;: execute sequence of actions $a = a_0, ..., a_H$ to get a sequence of states $s_1, ..., s_H$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc21.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: get trajectory reward $J(a) = \sum_{t=0}^H r_t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Back-propagation&lt;/strong&gt;: because everything is differentiable, you can just calculate the gradient of the reward via back-propagation using reward model derivatives and transition model derivatives.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
$$
\nabla_a J = \sum_{t=0}^H \nabla_a r_t
$$$$
\nabla_a r_t = \nabla_s f_r(s_t, a_t) \nabla_a s_t + \nabla_a f_r (s_t, a_t)
$$$$
\nabla_a s_t = \nabla_a f_s(s_{t-1}, a_{t-1}) + \nabla_s f_s(s_{t-1}, a_{t-1})\nabla_a s_{t-1}
$$$$
\nabla_a s_{t-1} = ...
$$&lt;ol&gt;
&lt;li&gt;Update all actions via gradient ascent $ a \leftarrow a + \nabla_a J$ and repeat steps 2-5.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc22.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The differences between discrete and continuous actions can be summarized as follows:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc23.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The continuous example we saw above can be categorized in &lt;strong&gt;shooting methods&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&quot;Variety-and-motivations-of-continuous-planning-methods&quot;&gt;Variety and motivations of continuous planning methods&lt;a class=&quot;anchor-link&quot; href=&quot;#Variety-and-motivations-of-continuous-planning-methods&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Why so many variations? They all try to mitigate the issues we looked at like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sensitivity and poor conditioning&lt;/li&gt;
&lt;li&gt;Only reaches local optimum&lt;/li&gt;
&lt;li&gt;Slow convergence&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Addressing each leads to a different class of methods.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc24.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Sensitivity-and-poor-conditioning&quot;&gt;Sensitivity and poor conditioning&lt;a class=&quot;anchor-link&quot; href=&quot;#Sensitivity-and-poor-conditioning&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc24-2.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shooting methods&lt;/strong&gt; that we have seen have this particular issue that small changes in early actions lead to very large changes downstream.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc25.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;By expanding the objective function, this can be understood more clearly.&lt;/p&gt;
$$
\max_{a_0,...,a_H} \sum_{t=0}^H r(s_t, a_t), \quad s_{t+1} = f(s_t, a_t)
$$$$
\sum_{t=0}^H r(s_t, a_t) = r(s_0, a_0) + r(f(s_0, a_0), a_1)+...+r(f(f(...),...), a_H)
$$&lt;p&gt;It means that each state implicitly is dependent on all actions that came before it. This is similar to the exploding/vanishing gradient problem in RNNs that hurts long-term credit assignment. But unlike the RNN training, we cannot change the transition function because it is dictated to us by the environment.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc26.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To address this problem, &lt;strong&gt;Collocation&lt;/strong&gt; is introduced, which is optimizing for states and/or actions &lt;em&gt;directly&lt;/em&gt;, instead of actions only. So we have a different set of parameters that we are optimizing over.&lt;/p&gt;
$$
\max_{s_0,a_0,...,s_H,a_H} \sum_{t=0}^H r(s_t, a_t), \quad ||s_{t+1} - f(s_t, a_t) || = 0 \leftarrow \text{explicit optimization constraint}
$$&lt;p&gt;It is an explicit constrained optimization problem, rather than just beeng satisfied by construction as in shooting methods.&lt;/p&gt;
&lt;p&gt;As a result, you only have pairwise dependencies between variables, unlike the dense activity graph in the previous figure for shooting methods.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc27.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;These methods have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Good conditioning: changing $s_0, a_0$ has a similar effect as changing $s_H, a_H$.&lt;/li&gt;
&lt;li&gt;Larger but easier to optimize search space. It is useful for contact-rich problems such as some robotics applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Only-reaches-local-optimum&quot;&gt;Only reaches local optimum&lt;a class=&quot;anchor-link&quot; href=&quot;#Only-reaches-local-optimum&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc28.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Some approaches try to avoid local optima like sampling-based methods: Cross-Entropy Methods (CEM) and $\text{PI}^2$.&lt;/p&gt;
&lt;p&gt;For example, in CEMs, instead of just maintaining the optimal trajectory, it maintains the optimal trajectory's mean and covariance.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc29.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc30.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc31.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Despite being very simple, this works surprisingly well and has very nice guarantees on performance.&lt;/p&gt;
&lt;p&gt;Why does this work?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Search space of decision-time plans much smaller than space of policy parameters: ex. 30x32 vs 32x644x32&lt;/li&gt;
&lt;li&gt;More feasible plans than policy parameters&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Slow-convergence&quot;&gt;Slow convergence&lt;a class=&quot;anchor-link&quot; href=&quot;#Slow-convergence&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc32.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Gradient descent is too slow to converge, and we need to wait for thousands-millions of iterations to train a policy. But this is too long for a one-time plan that we want to through it away after.&lt;/p&gt;
&lt;p&gt;Can we do something like Newton’s method for trajectory optimization, like non-linear optimization? YES!&lt;/p&gt;
&lt;p&gt;We can approximate transitions with linear functions and rewards with quadratics:&lt;/p&gt;
$$
\max_{a_0,...,a_H} \sum_{t=0}^H r_t, \quad s_{t+1} = f_s(s_t, a_t), \quad r_t=f_r(s_t, a_t)
$$$$
f_s(s_t, a_t) \approx As_t + Ba_t, \quad f_r(s_t, a_t) \approx s_t^TQs_t + a_t^TRa_t
$$&lt;p&gt;Then it becomes the Linear-Quadratic Regulator (LQR) problem and can be solved exactly.&lt;/p&gt;
&lt;p&gt;For iLQR, locally approximate the model around the current solution, solve the LQR problem to update the solution, and repeat.&lt;/p&gt;
&lt;p&gt;For Differential dynamic programming (DDP), it is similar, but with a higher-order expansion of $f_s$.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Model-based-control-in-the-loop&quot;&gt;Model-based control in the loop&lt;a class=&quot;anchor-link&quot; href=&quot;#Model-based-control-in-the-loop&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We want to answer this question of how to both learn the model and act based on that simultaneously?&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc33.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&quot;Gathering-data-to-train-models&quot;&gt;Gathering data to train models&lt;a class=&quot;anchor-link&quot; href=&quot;#Gathering-data-to-train-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;How can we gather data to train the model? this is a chicken or the egg problem. Bad policy leads to a bad experience, leads to a bad model, leads to bad policy ...&lt;/p&gt;
&lt;p&gt;This leads to some training stability issues in practice. There are some recent works in game theory to provide criteria for stability. See &lt;em&gt;Rajeswaran et al (2020). A Game Theoretic Framework for Model Based Reinforcement Learning.&lt;/em&gt; for example.&lt;/p&gt;
&lt;h4 id=&quot;Fixed-off-line-datasets&quot;&gt;Fixed off-line datasets&lt;a class=&quot;anchor-link&quot; href=&quot;#Fixed-off-line-datasets&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Another way to address this in the loop issues is to see if we can actually train from a fixed experience that is not related to the policy. Some options that we have are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Human demonstration&lt;/li&gt;
&lt;li&gt;Manually-engineered policy rollouts&lt;/li&gt;
&lt;li&gt;Another (sub-optimal) policy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc34.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This leads to a recent popular topic &lt;em&gt;model-based offline reinforcement learning&lt;/em&gt;. You can see some recent works like &lt;em&gt;Kidambi et al (2020). MOReL: Model-Based Offline Reinforcement Learning.&lt;/em&gt;, 
&lt;em&gt;Yu et al (2020). MOPO: Model-based Offline Policy Optimization.
See also: Levine et al (2020).&lt;/em&gt;, and &lt;em&gt;Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems.&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Data-augmentation&quot;&gt;Data augmentation&lt;a class=&quot;anchor-link&quot; href=&quot;#Data-augmentation&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Another way to generate data is to use the model to generate data to train itself. For example, in &lt;em&gt;Venkatraman et al (2014). Data as Demonstrator&lt;/em&gt;. You might have some trajectory of a real experiment that you got by taking certain actions; then you roll out the model and train to pull its predicted next states to true next states.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc35.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc36.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;There are also some adversarial approaches to generate data to self-audit the model like &lt;em&gt;Lin et al (2020). Model-based Adversarial Meta-Reinforcement Learning.&lt;/em&gt; and &lt;em&gt;Du et al (2019). Model-Based Planning with Energy Models.&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;But even if we do all of these works, models are not going to be perfect. We cannot have experience everywhere, and there will be some approximation errors always. These small errors propagate and compound. We may end up in some states that are a little bit further away from true data, which might be an unfamiliar situation. So it might end up making even bigger errors next time around and so on and so forth that the model rollouts might actually land very far away over time from where you would expect them to be.&lt;/p&gt;
&lt;p&gt;What's worse is that the planner might actually intentionally &lt;em&gt;exploit&lt;/em&gt; these model errors to achieve the goal.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc37.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This leads to a longer model rollouts to be less reliable.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc38.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;You can check &lt;em&gt;Janner et al (2019). When to Trust Your Model:
Model-Based Policy Optimization&lt;/em&gt; for more details.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Acting-under-imperfect-models&quot;&gt;Acting under imperfect models&lt;a class=&quot;anchor-link&quot; href=&quot;#Acting-under-imperfect-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The question is that &quot;Can we still act with imperfect models?&quot; the answer is yes!&lt;/p&gt;
&lt;h4 id=&quot;Replan-via-model-predictive-control&quot;&gt;Replan via model-predictive control&lt;a class=&quot;anchor-link&quot; href=&quot;#Replan-via-model-predictive-control&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The first approach is to not commit to just one single plan (open-loop control) but continually re-plan as you go along (closed-loop control).&lt;/p&gt;
&lt;p&gt;Let's see one example.&lt;/p&gt;
&lt;p&gt;You might start at some initial state and create an imaginary plan using the trajectory optimization methods like CEM or other methods. Then apply just the first action of this plan. That might take you to some state that might not in the practice match with your model imagined you would end up with. But it's ok! You can just re-plan from this new state, again and again, take the first action and ... and by doing this, there is a good chance to end up near the goal.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc39.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc40.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc41.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc42.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc43.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;By doing this, the errors don't accumulate. So you don't need a perfect model; just one pointing in the right direction is enough. This re-planning might be expensive, but one solution is to reuse solutions from previous steps as initial guesses for the next plan.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc44.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Plan-conservatively&quot;&gt;Plan conservatively&lt;a class=&quot;anchor-link&quot; href=&quot;#Plan-conservatively&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;We have'seen that longer rollouts become more unreliable. One solution would be just to keep the rollouts short. So we don't deviate too far from where we have real data. And as we saw in Dyna, just one single rollout can be also very helpful to improve learning.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc45.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The other option to plan conservatively is to consider a distribution over your models and plan for either the average or worst case wrt distribution over your model or model uncertainty.&lt;/p&gt;
$$
\max_{\theta} E_{f \sim F} [\sum_t \gamma^t r_t], \quad a_t=\pi_{\theta}(s_t), \quad s_{t+1}=f_s(s_t, a_t), \quad r_t=f_r(s_t, a_t)
$$&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc46.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc47.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Another option for conservative planning is to try to stay close to states where the model is certain. There are a couple of ways to do this:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc48.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Implicitly: stay close to past policy that generated the real data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Peters et al (2012). Relative Entropy Policy Search&lt;/li&gt;
&lt;li&gt;Levine et al (2014). Guided Policy Search under Unknown Dynamics.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Explicitly: add penalty to reward or cost function for going into unknown region&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kidambi et al (2020). MOReL: Model-Based Offline Reinforcement Learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In the last two options for conservative planning, we need uncertainty. So how do we get this model uncertainty?&lt;/p&gt;
&lt;h3 id=&quot;Estimating-model-uncertainty&quot;&gt;Estimating model uncertainty&lt;a class=&quot;anchor-link&quot; href=&quot;#Estimating-model-uncertainty&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Model uncertainty, if necessary for conservative planning, but it has other applications too that we will see later.&lt;/p&gt;
&lt;p&gt;We consider two sources of uncertainty:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Epistemic uncertainty&lt;ul&gt;
&lt;li&gt;Model's lack of knowledge about the world&lt;/li&gt;
&lt;li&gt;Distribution over beliefs&lt;/li&gt;
&lt;li&gt;Reducible by gathering more experience about the world&lt;/li&gt;
&lt;li&gt;Changes with learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Aleatoric uncertainty/Risk&lt;ul&gt;
&lt;li&gt;World's inherent stochasticity&lt;/li&gt;
&lt;li&gt;Distribution over outcomes&lt;/li&gt;
&lt;li&gt;Irreducible&lt;/li&gt;
&lt;li&gt;Static as we keep learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are multiple approaches to estimate these uncertainties, which are listed as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Probabilistic neural networks that try to model distributions over the outputs of your model.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Model explicitly outputs means and variances (typically Gaussian)&lt;/p&gt;
&lt;p&gt;$$ p(s_{t+1}|s_t, a_t) = N(\mu_{\theta}(s_t, a_t), \sigma_{\theta}(s_t, a_t))$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Simple and reliable (supervised learning)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Only captures aleatoric uncertainty/risk&lt;/li&gt;
&lt;li&gt;No guarantees for reasonable outputs outside of training data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bayesian neural network&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Model has a distribution over neural network weights&lt;/p&gt;
&lt;p&gt;$$ p(s_{t+1}|s_t, a_t) = E_{\theta}[p(s_{t+1}|s_t, a_t, \theta)]$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Captures epistemic and aleatoric uncertainty&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Factorized approximations can underestimate uncertainty&lt;/li&gt;
&lt;li&gt;Can be hard to train (but an active research area)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc49.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gaussian processes &lt;ul&gt;
&lt;li&gt;Captures epistemic uncertainty&lt;/li&gt;
&lt;li&gt;Explicitly control state distance metric&lt;/li&gt;
&lt;li&gt;Can be hard to scale (but an active research area)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pseudo-counts&lt;ul&gt;
&lt;li&gt;Count or hash states you already visited&lt;/li&gt;
&lt;li&gt;Captures epistemic uncertainty&lt;/li&gt;
&lt;li&gt;Can be sensitive to state space in which you count&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc50.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ensembles&lt;ul&gt;
&lt;li&gt;Train multiple models independently and combine predictions across models&lt;/li&gt;
&lt;li&gt;Captures epistemic uncertainty&lt;/li&gt;
&lt;li&gt;Simple to implement and applicable in many contexts&lt;/li&gt;
&lt;li&gt;Can be sensitive to state space and network architecture&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For discussion in the context of reinforcement learning, see &lt;em&gt;Osband et al (2018). Randomized Prior Functions for Deep Reinforcement Learning.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Between the above options, Ensembles are currently popular due to simplicity and flexibility.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Combining-planning-and-learning&quot;&gt;Combining planning and learning&lt;a class=&quot;anchor-link&quot; href=&quot;#Combining-planning-and-learning&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We compared these two methods in previous sections and saw that background and decision-time planning have complementary strengths and weaknesses.&lt;/p&gt;
&lt;p&gt;How to combine decision-time planning and background planning methods and get the benefits of both?&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Distillation&quot;&gt;Distillation&lt;a class=&quot;anchor-link&quot; href=&quot;#Distillation&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In this approach, we gather a collection of initial states and run our decision-time planner for each initial state and get a collection of trajectories that succeed at reaching the goal. Once we collected this collection of optimal trajectories, we can use a supervised learning algorithm to train either policy function or any other function to map states to actions. This is similar to Behavioral Cloning (BC).&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb1.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb2.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Some issues that can arise:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What is the learned policies that have compounding errors?&lt;/strong&gt; If we rollout the policy from one of the states, it does something different than what we intended to do.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Create new decision-time plans from these states that have been visited by the policy.&lt;/li&gt;
&lt;li&gt;Add these trajectories (new decision-time plans) to the distillation dataset (expand dataset where policy makes errors)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb3.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This is the idea of Dagger algorithm:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb4.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What if the plans are not consistent?&lt;/strong&gt; There are several ways to achieving a goal, and we've seen that by changing the initial condition only a little bit, the decision-time planner can give us pretty different solutions to reach a single goal. This chaotic behavior might be hard to distill into the policy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb5.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;we can make it so that the policy function that we are learning actually feeds back and influences our planner. &lt;/li&gt;
&lt;li&gt;To do this, we can add an additional term in our cost that says stay close to the policy. $D$ in the below cost function is the distance between actions of the planner, $a_t$, and the policy outputs, $\pi(s_t)$. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb6.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb7.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb8.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Terminal-value-functions-(value-of-the-terminal-state)&quot;&gt;Terminal value functions (value of the terminal state)&lt;a class=&quot;anchor-link&quot; href=&quot;#Terminal-value-functions-(value-of-the-terminal-state)&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;One of the issues with many trajectory optimizations or discrete search approaches is that the planning horizon is typically finite. This may lead to myopic or greedy behavior.&lt;/p&gt;
$$
J^H = \sum_{t=0}^H \gamma^t r_t
$$&lt;p&gt;To solve this problem, we can use the value function at the terminal state and add it to the objective function. This learned value function guides plans to good long-term states. So the objective function would be infinite horizon:&lt;/p&gt;
$$
J^{\infty} = \sum_{t=0}^{\infty} \gamma^t r_t = \sum_{t=0}^H \gamma^t r_t + \gamma^H V(s_H)
$$&lt;p&gt;This is another kind of combining decision-time planning (optimization problem) with background planning (learned value function).&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb9.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This can be used in both discrete and continuous action spaces:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb10.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Planning-as-policy-improvement&quot;&gt;Planning as policy improvement&lt;a class=&quot;anchor-link&quot; href=&quot;#Planning-as-policy-improvement&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;So far, we used policy (background) or decision-time planner to make a decision and generate trajectory and actions.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb11.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;But we can combine them and use the planner as policy improvement. We can use the policy to provide some information for the planner. For example, the policy can output its set of trajectories, and the planner can use it as a warm start or initialization to improve upon. We would like to train the policy such that the improvement proposed by the planner has no effect. So the policy trajectory is the best that we can do. I think we can see the planner as a teacher for the policy.&lt;/p&gt;
&lt;p&gt;Some related papers are listed here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Silver et al (2017). Mastering the game of Go without human knowledge.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Levine et al (2014). Guided Policy Search under Unknown Dynamics.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Anthony et al (2017). Thinking Fast and Slow with Deep Learning and Tree Search.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb12.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Implicit-planning&quot;&gt;Implicit planning&lt;a class=&quot;anchor-link&quot; href=&quot;#Implicit-planning&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In addition, to use a planner to improve policy trajectory, we can put the planner as a component &lt;em&gt;inside&lt;/em&gt; the policy network and train end-to-end.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb13.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The advantage of doing this is that the policy network dictates abstract state/action spaces to plan in. But the downside of this is that it requires differentiating through the planning algorithm. But the good news is that multiple algorithms we've seen have been made differentiable and amenable to integrating into such a planner.&lt;/p&gt;
&lt;p&gt;some examples are as follows:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb14.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb15.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb16.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;There are also some works that show the planning could &lt;em&gt;emerge&lt;/em&gt; in generic black-box policy network and model-free RL training.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb17.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-else-can-models-be-used-for?&quot;&gt;What else can models be used for?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-else-can-models-be-used-for?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Consider we have a model of the world. We can use the model in a lot of different ways like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exploration&lt;/li&gt;
&lt;li&gt;Hierarchical Reasoning&lt;/li&gt;
&lt;li&gt;Adaptivity &amp;amp; Generalization&lt;/li&gt;
&lt;li&gt;Representation Learning&lt;/li&gt;
&lt;li&gt;Reasoning about other agents&lt;/li&gt;
&lt;li&gt;Dealing with partial observability&lt;/li&gt;
&lt;li&gt;Language understanding&lt;/li&gt;
&lt;li&gt;Commonsense reasoning&lt;/li&gt;
&lt;li&gt;and more!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here we're gonna just focus on the first four ways that we can use the model to encourage better behavior.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu1.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Exploration&quot;&gt;Exploration&lt;a class=&quot;anchor-link&quot; href=&quot;#Exploration&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;One of the good things about having a model of the world is that you can reset to any state in the world that you might care about. It's not possible in all environments to reset like a continual learning problem. But if you have the model of the world, you can reset to any state you want.&lt;/p&gt;
&lt;p&gt;We can also consider resetting to intermediate states in the middle of the episode as a starting point. The idea is to keep track of one of the interesting states and does exploration from there. So if you have the world's model, you can again reset to that state and efficiently perform additional explorations.&lt;/p&gt;
&lt;p&gt;You can also reset from the final state rather than the initial state. This can be useful in situations where there is only a single goal state like Rubik's Cube. In this case, there is only one goal but maybe several possible starting states. So it would be useful to reset to the final state and explore backward from there rather than starting from the initial state.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu2.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Another way that models can be used to facilitate exploration is by using &lt;em&gt;intrinsic reward&lt;/em&gt;. In these cases, we want to explore places that we haven't been much so that we can gather data in those locations and learn more about them. One way to identify where we haven't been is to use model prediction error as a proxy. Basically, we learn a world model, then we predict what the next state is going to be and then take action and observe the next state and compare it with the predicted state and calculate the model error. We can then use this prediction error as a signal in the intrinsic reward to encourage the agent to explore the locations we haven't visited often to learn more about them.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu3.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In addition to the above approach, we can also &lt;em&gt;plan to explore&lt;/em&gt;. In &lt;em&gt;POLO&lt;/em&gt; paper, rather than using the error from your prediction model, they use the error across an ensemble of value functions and use it as an intrinsic reward. Actually, at each state, we compute a bunch of different values from our ensemble of value functions, then take softmax over them to give us an optimistic estimate of what the value is going to be. We can use this optimistic value estimate as an intrinsic reward. We plan to maximize this optimistic value estimate, and then this allows us to basically, during planning, identify places that we should direct our behavior towards are more surprising or more interesting.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute intrinsic reward during (decision-time) planning to direct the agent into new regions of state-space&lt;/li&gt;
&lt;li&gt;Intrinsic reward = softmax across an ensemble of value functions&lt;/li&gt;
&lt;/ul&gt;
$$
\hat{V}(s) = log(\sum_{k=1}^K exp(k\hat{V}_{\theta_k}(s)))
$$&lt;p&gt;
&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/003rQ5vUcek?t=3&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Lowrey et al. (2019). Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control. ICLR 2019.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also use the same idea, but instead of using a set of disagreement across on ensemble of value functions, we can compute disagreement across transition functions. Now because we are just using state transitions, this turns into a task agnostic exploration problem. We can then plan where there is a disagreement between our transition functions and direct behavior towards those regions of space to learn a really robust world model. And then use this model of the world to learn new tasks either using zero-shot or few-shot (examples of experience).&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu5.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Finally, another form of exploration is that if we have a model of possible states that we might find ourselves in, not necessarily a transition model but a density model over goals, we can sample possible goals from this density model and then train our agent achieve the goals.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu6.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Hierarchical-reasoning&quot;&gt;Hierarchical reasoning&lt;a class=&quot;anchor-link&quot; href=&quot;#Hierarchical-reasoning&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;A very classic way of doing hierarchical reasoning is what's called &lt;em&gt;task and motion planning (TAMP)&lt;/em&gt; in robotics. You jointly plan symbolically at the task level, and then you also plan in the continuous space and do motion planning at the low-level—you sort of doing these things jointly in order to solve relatively long-horizon and multi-step tasks. For example, in the following figure, to control a robot arm and to get block $A$ and put it in the washer, wash it, and then put it in storage. In order to do this, you first have to move $C$ and $B$ out of the way and put $A$ into the washer, then move $D$ out of the way and then put $A$ into the storage. By leveraging symbolic representation, like PDDL from the beginning of the post, allows you to be able to jointly solve these hierarchical tasks.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu7.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The other example of this is the OpenAI Rubik's Cube solver. The idea is that you use a high-level symbolic algorithm, Kociemba's algorithm, to generate the solution (plan) of high-level actions, for example, which faces should be rotated, and then you have a low-level neural network policy that generates the controls needed to achieve these high-level actions. This low-level control policy is quite challenging to learn.

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/kVmp0uGtShk&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;OpenAI et al. (2019). Solving Rubik's Cube with a Robot Hand. arXiv.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The question that might arise is that where does this high-level state-space come from?&lt;/p&gt;
&lt;p&gt;We don't want to hand-code symbolically on these high-level roles that we want to achieve. Some model-free works try to answer this, but we focus on some MBRL approaches here for this problem.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu8.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&quot;Subgoal-based-approaches&quot;&gt;Subgoal-based approaches&lt;a class=&quot;anchor-link&quot; href=&quot;#Subgoal-based-approaches&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;We can consider that any state you might find yourself in in the world as a subgoal. We don't want to construct a super long sequence of states to go through, but a small sequence. So the idea would be which states do we pick as a subgoal. Rather than learning a forward state transition model, we can learn a universal value function approximator, $V(s, g)$, that tells us the value of going from state $s$ to goal state $g$. We can train these value functions between our subgoals to estimate how good a particular plan of length $k$ is. A plan of length $k$ is then given by maximizing:&lt;/p&gt;
$$
\text{arg}\max_{\{s_i\}_{i=1}^k} (V(s_0, s_1) + V(s_k, s_g) + \sum_{i=1}^{k-1} V(s_i, s_{i+1}))
$$&lt;p&gt;The figure below shows the idea. If you start from state $s_0$ and you want to go to $s_{\infty}$, you can break up this long plan of length one into a plan of length two by inserting a subgoal. You can do this recursively multiple times to end up with a plan of length $k$ or, in this case, a plan of length three.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu9.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;When we use a planner to identify which of these subgoals we should choose in order to maximize the above equation, in the figure below, you see which white subgoal it is considering as subgoal in order to find a path between the green and the blue points.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/maze.gif&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Nasiriany et al. (2019). Planning with Goal-Conditioned Policies. NeurIPS.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Jurgenson et al. (2019). Sub-Goal Trees -- A Framework for Goal-Directed Trajectory Prediction and Optimization. arXiv.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Parascandolo, Buesing, et al. (2020). Divide-and-Conquer Monte Carlo Tree Search For Goal-Directed Planning. arXiv.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Skill-based-approaches&quot;&gt;Skill-based approaches&lt;a class=&quot;anchor-link&quot; href=&quot;#Skill-based-approaches&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Here, rather than identifying discrete states as subgoals that we want to try to achieve, what we want to do is to learn a set of skills that sort of fully parametrize the space of possible trajectories that we might want to execute. So, for example, in the Ant environment, a nice parametrization of skills would be to say a particular direction that you want to get to move in. So the approach taken by this paper is to learn a set of skills those outcomes are both (1) easy to predict, so if you train a dynamics model to predict the outcome of executing the skill, and (2) the skills are diverse from one another. That's why you get this nice diversity of the ant moving in different directions. This works very well for zero-shot adaptation to new sequences of goals. As you can see on the bottom, this is an ant going to a few different locations in space, and it is doing this by just pure planning using this set of skills that it is learned during the unsupervised training phase.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn a set of skills whose outcomes are (1) easy to predict and (2) diverse&lt;/li&gt;
&lt;li&gt;Learn dynamics model over skills, and plan with MPC&lt;/li&gt;
&lt;li&gt;Can solve long-horizon sequences of high-level goals with no additional learning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/ant.gif&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Sharma et al. (2020). Dynamics-Aware Unsupervised Discovery of Skills. ICLR.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;representation-Learning&quot;&gt;representation Learning&lt;a class=&quot;anchor-link&quot; href=&quot;#representation-Learning&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Beyond just using models for prediction, they can be used as regularizers for training other types of representations that then you can train a policy on.&lt;/p&gt;
&lt;p&gt;One way is to learn a model as an &lt;em&gt;auxiliary loss&lt;/em&gt;. For example, if you have an A2C algorithm and add an auxiliary loss to predict the reward it's gonna achieve, in some cases, you can get a large improvement in performance by just adding this auxiliary loss. By considering this loss during training, we are actually forcing it to learn the future and capture the structure of the world, which is useful. We also don't use this learned model in planning and just for representation learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Jaderberg et al. (2017). Reinforcement learning with unsupervised auxiliary tasks. ICLR 2017.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The other same idea is to use a &lt;em&gt;contrastive loss&lt;/em&gt;, like CPC paper (below), that tries to predict what observations it might encounter in the future, and by adding this additional loss during training, we see improvement in performance.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;van den Oord, Li, &amp;amp; Vinyals (2019). Representation Learning with Contrastive Predictive Coding. arXiv.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another idea is &lt;em&gt;plannable representations&lt;/em&gt; that make it much easier to plan in. For example, if we are in a continuous space, we can discretize it in an intelligent way that might make it easy to use some of these discrete search methods, like MCTS, to rapidly come up with a good plan of actions. Or maybe we can come up with a representation for our state space such that moving along a direction in the latent state space corresponds to planning. So you can basically just interpolate between states in order to come up with a plan.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn an embedding of states that is easier to plan in, e.g.&lt;ul&gt;
&lt;li&gt;Discretized&lt;/li&gt;
&lt;li&gt;States that can be transitioned between should be near to each other in latent space!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Related to notions in hierarchical RL (state abstraction)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu10.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Corneil et al. (2018). Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation. ICML.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Kurutach et al. (2018). Learning Plannable Representations with Causal InfoGAN. NeurIPS.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Adaptivity-&amp;amp;-generalization&quot;&gt;Adaptivity &amp;amp; generalization&lt;a class=&quot;anchor-link&quot; href=&quot;#Adaptivity-&amp;amp;-generalization&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Models of the world can also be used for fast adaptation and generalization.&lt;/p&gt;
&lt;p&gt;The world can be changed in two different ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Change in rewards&lt;/strong&gt;. So we're being asked to do a new task, but the dynamics are the same.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Change in dynamics&lt;/strong&gt;. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Based on the above changes, we can do different things in response to them.&lt;/p&gt;
&lt;p&gt;In a model-free approach, we just adapt to the policy. But this tends to be relatively slow because it's hard to quickly adapt changes in rewards to the same dynamics and vice versa because they are sort of entangled with each other.&lt;/p&gt;
&lt;p&gt;If we have an explicit model of the world, we can update our behavior differently. One option would be that we can adapt the planner, but we can also adapt the model itself, or we can do both.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu11.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&quot;Adapting-the-planner-in-new-states&quot;&gt;Adapting the planner in new states&lt;a class=&quot;anchor-link&quot; href=&quot;#Adapting-the-planner-in-new-states&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;A pre-trained policy may not generalize to all states (especially in combinatorial spaces). So some states that we might find ourselves in might be required harder or more reasoning, and others may require less.
We have to try to detect when planning is required, and they adapt the amount of planning depending on the difficulty of the task. For example, in the following gifs, in the upper case, the n-body agent can easily solve the task and reach the center's goal using just a couple of simulations. But in the bottom case, it is much harder to reason about because it starts on one of the planets, which requires many more simulations. We can adaptively change this amount of computation as needed. Save the computation on easy scenes and then spend it more on the hard ones.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/easy.gif&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/hard.gif&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Hamrick et al. (2017). Metacontrol for adaptive imagination-based optimization. ICLR 2017.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Pascanu, Li, et al. (2017). Learning model-based planning from scratch. arXiv.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Adapting-the-planner-to-new-rewards&quot;&gt;Adapting the planner to new rewards&lt;a class=&quot;anchor-link&quot; href=&quot;#Adapting-the-planner-to-new-rewards&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Here is another same idea in a life-long learning setup where the reward can suddenly change, and either the agents can observe the change in the reward, or they just have to infer the reward has changed. Because of changes in reward, it needs more planning because the prior policy is less reliable, and more planning allows you to better explore these different options for the reward function. In the video below, as you can see in the bottom agent after the reward is changed, the agent needs to do more planning to have a nice movement compared to the other two agents. 

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/3T3QuKregt0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Lu, Mordatch, &amp;amp; Abbeel (2019). Adaptive Online Planning for Continual Lifelong Learning. NeurIPS Deep RL Workshop.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Adapting-the-model-to-new-dynamics&quot;&gt;Adapting the model to new dynamics&lt;a class=&quot;anchor-link&quot; href=&quot;#Adapting-the-model-to-new-dynamics&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;For the times that the dynamics change, it could be very useful to adapt the model. One way to approach this is to train the model using the meta-learning objective so that during training, you're always training it to adapt  to a slightly different environment around you, and at the test time, you actually see a new unobserved environment that you never saw before, you can take a few gradient steps to adapt the model to deal with these new situations. Here is an example where the agent, half cheetah, has been trained to walk along some terrain, but it's never seen as a little hill before. Therefore, the baseline methods that cannot adapt their model cannot get the agent to go up the hill, where this meta-learning version can get the cheetah to go up the hill. 

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/ejG2nzCNdZ8?t=144&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Nagabandi et al. (2019). Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning. ICLR.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What's-missing-from-model-based-methods?&quot;&gt;What's missing from model-based methods?&lt;a class=&quot;anchor-link&quot; href=&quot;#What's-missing-from-model-based-methods?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Humans are ultimate model-based reasoners and we can learn a lot from how we build and deploy models of the world. - &lt;strong&gt;Motor control&lt;/strong&gt;: forward kinematics models in the cerebellum. We have a lot of motor systems that are making predictions about how our muscles are going to affect the kinematics of our bodies.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Language comprehension&lt;/strong&gt;: we build models of what is being communicated in order to understand.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pragmatics&lt;/strong&gt;: we construct models of listener &amp;amp; speaker beliefs in order to try to understand what is tryingto be communicated.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Theory of mind&lt;/strong&gt;: we construct models of other agents’ beliefs and behavior in order to predict what they are going to do.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decision making&lt;/strong&gt;: model-based reinforcement learning&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Intuitive physics&lt;/strong&gt;: forward models of physical dynamics&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scientific reasoning&lt;/strong&gt;: mental models of scientific phenomena&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Creativity&lt;/strong&gt;: being able to imagine novel combinations of things&lt;/li&gt;
&lt;li&gt;… and much more!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more you can see the following reference:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Markman, Klein, &amp;amp; Suhr (2008). Handbook of Imagination and Mental Simulation.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Abraham (2020). The Cambridge Handbook of the Imagination.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;If you look at the mentioned different domains, where people are engaging a model based reasoning, a few themes emerge that could be really useful in thinking about how to continue to develop our models in MBRL.&lt;/p&gt;
&lt;p&gt;Humans use their models of the world in ways that are compositional, causal, incomplete, adaptive, efficient, and abstract. Taking these ideas and trying to distill them into MBRL enables us to do&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;faster planning&lt;/li&gt;
&lt;li&gt;have systems with higher tolerance to model error&lt;/li&gt;
&lt;li&gt;can be scaled to much much harder problems.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This will lead us to more robust real-world applications and better common sense reasoning.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis1.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Compositionality&quot;&gt;Compositionality&lt;a class=&quot;anchor-link&quot; href=&quot;#Compositionality&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Humans are much much stronger than MBRL algorithms that we have in compositionality.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis2.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&quot;Causality&quot;&gt;Causality&lt;a class=&quot;anchor-link&quot; href=&quot;#Causality&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis3.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&quot;Incompleteness&quot;&gt;Incompleteness&lt;a class=&quot;anchor-link&quot; href=&quot;#Incompleteness&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Another facet of human model based reasoning in the fact that we can reason about incomplete models, but reason about them in very tich ways. This is in contrast to model-based RL which if we have model error, it would be a huge deal and are very far from human capabilities.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis4.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&quot;Adaptivity&quot;&gt;Adaptivity&lt;a class=&quot;anchor-link&quot; href=&quot;#Adaptivity&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The way that we (humans) use our models is also incredibly adaptive. We can rapidly assemble our compositional knowledge into on-the-fly models that are adapted to the current task. Then we quickly solve these models, leveraging both mental simulation &amp;amp; (carefully chosen) real experience&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/adapt.gif&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Allen, Smith, &amp;amp; Tenenbaum (2019). The tools challenge: Rapid trial-and-error learning in physical problem solving. CogSci 2019.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Dasgupta, Smith, Schulz, Tenenbaum, &amp;amp; Gershman (2018). Learning to act by integrating mental simulations and physical experiments. CogSci 2018.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Efficiency&quot;&gt;Efficiency&lt;a class=&quot;anchor-link&quot; href=&quot;#Efficiency&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Humans' model-based reasoning is also very efficient. Figure below illustrates how much of an improvement Alpha-zero was over former state of the art chess engine which requires a tens of millions of moves during simulation. Whereas Alpha-zero only needs tens of thousands. But again it is not comparable to human grandmaster, which only requires hundreds of moves. So we need to continue to develop planners that are able to sort of leverage our models as quickly and as efficiently as possible towards this type of efficiency.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis5.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&quot;Abstraction&quot;&gt;Abstraction&lt;a class=&quot;anchor-link&quot; href=&quot;#Abstraction&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The final feature of humans' ability to use models of the world is abstraction. We go through all of different levels of abstraction as we're planning over multiple timescales, over multiple forms of state abstraction, and we move up and down different forms of abstraction as needed and so we ideally want integrated agents that could do the same.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis6.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Conclusion&quot;&gt;Conclusion&lt;a class=&quot;anchor-link&quot; href=&quot;#Conclusion&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In this tutorial, we discussed what it means to have a model of the world and different types of models that you can learn. We also talked about where the model fits into the RL loop. We talked about landscape of model-based methods and some practical considerations that we care about when integrating models into the loop. We also saw how we can try to improve models by looking towards human cognition.&lt;/p&gt;
&lt;h3 id=&quot;Ethical-and-broader-impacts&quot;&gt;Ethical and broader impacts&lt;a class=&quot;anchor-link&quot; href=&quot;#Ethical-and-broader-impacts&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Because MBRL inherits methods both from model-free RL and model learning in general, it inherits the problems from both of them too.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis7.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" /><media:content medium="image" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Online Reinforcement Learning</title><link href="https://kargarisaac.github.io/blog/rl/2020/06/15/online-rl.html" rel="alternate" type="text/html" title="Online Reinforcement Learning" /><published>2020-06-15T00:00:00-05:00</published><updated>2020-06-15T00:00:00-05:00</updated><id>https://kargarisaac.github.io/blog/rl/2020/06/15/online-rl</id><content type="html" xml:base="https://kargarisaac.github.io/blog/rl/2020/06/15/online-rl.html">&lt;h1 id=&quot;online-reinforcement-learning&quot;&gt;Online Reinforcement Learning&lt;/h1&gt;

&lt;p&gt;In this post I will overview different single and multi-agent online Reinforcement Learning (RL) algorithms. By &lt;strong&gt;online&lt;/strong&gt; I mean the algorithms that can interact with an environment and collect data, in contrast to offline RL. I will update this post and add algorithms periodically.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/rl-diagram.png&quot; alt=&quot;RL diagram&quot; /&gt; &lt;em&gt;RL diagram&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here are some resources to learn more about RL!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;David Silver’s &lt;a href=&quot;https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&quot;&gt;course&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CS287 at UC Berkeley - Advanced Robotics &lt;a href=&quot;https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNBPJdt8WamRAt4XKc639wF&quot;&gt;course&lt;/a&gt; - Instructor: Pieter Abbeel&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CS 285 at UC Berkeley - Deep Reinforcement Learning &lt;a href=&quot;http://rail.eecs.berkeley.edu/deeprlcourse/&quot;&gt;course&lt;/a&gt; - Instructor: Sergey Levine&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CS234 at Stanford - Reinforcement Learning &lt;a href=&quot;http://web.stanford.edu/class/cs234/index.html&quot;&gt;course&lt;/a&gt; - Instructor: Emma Brunskill&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CS885 at University of Waterloo - Reinforcement Learning &lt;a href=&quot;https://www.youtube.com/playlist?list=PLdAoL1zKcqTXFJniO3Tqqn6xMBBL07EDc&quot;&gt;course&lt;/a&gt; - Instructor: Pascal Poupart&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Arthur Juliani’s &lt;a href=&quot;https://medium.com/@awjuliani&quot;&gt;posts&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Jonathan Hui’s &lt;a href=&quot;https://medium.com/@jonathan_hui/rl-deep-reinforcement-learning-series-833319a95530&quot;&gt;posts&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A Free &lt;a href=&quot;https://simoninithomas.github.io/deep-rl-course/&quot;&gt;course&lt;/a&gt; in Deep Reinforcement Learning from beginner to expert by Thomas Simonni&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;single-agent&quot;&gt;Single agent&lt;/h1&gt;

&lt;h3 id=&quot;dqn&quot;&gt;DQN&lt;/h3&gt;

&lt;p&gt;We will take a look at DQN with experience replay buffer and the target network.&lt;/p&gt;

&lt;p&gt;DQN is a value-based method. It means that we try to learn a value function and then use it to achieve the policy. In DQN we use a neural network as a function approximator for our value function. It gets the state as input and outputs the value for different actions in that state. These values are not limited to be between zero and one, like probabilities, and can have other values based on the environment and the reward function we define.&lt;/p&gt;

&lt;p&gt;DQN is an off-policy method which means that we are using data from old policies, the data that we gather in every interaction with the environment and save it in the experience replay buffer, to sample from it later and train the network. The size of the replay buffer should be large enough to reduce the $i.i.d$ property between data that we sample from it.&lt;/p&gt;

&lt;p&gt;To use DQN, the action should be discrete. We can use it for continuous action spaces by discretizing the action space, but it’s better to use other techniques that can handle continuous action spaces such as Policy Gradients.
First, let’s see the algorithm’s sudo code:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/dqn.png&quot; alt=&quot;DQN algorithm&quot; title=&quot;DQN algorithm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this algorithm, we have experience replay buffer and a target network with a different set of parameters that will be updated every $C$ steps. These tricks help to get a better and more stable method rather than pure DQN. There are a lot of improvements for DQN and we will see some of them in the next posts too.&lt;/p&gt;

&lt;p&gt;First, we initialize the weights of both networks and then start from the initial state s and take action a with epsilon-greedy policy. In the epsilon-greedy policy, we select an action a randomly or using the Q-network. Then we execute the selected action and get the next state, reward, and the done values from the environment and save them in our replay buffer. Then we sample a random batch from the replay buffer and calculate target based on the Bellman equation in the above picture and use MSE loss and gradient descent to update the network weights. We will update the weights of our target network every $C$ steps.&lt;/p&gt;

&lt;p&gt;In the training procedure, we use epsilon decay. It means that we consider a big value for epsilon, such as $1$. Then during the training procedure, as we go forward, we reduce its value to something like $0.02$ or $0.05$, based on the environment. It will help the agent to do more exploration in the first steps and learn more about the environment. It’s better to have some exploration always. That’s a trade-off between exploration-exploitation.
In test time, we have to use a greedy policy. It means we have to select the action with the highest value, not randomly anymore (set epsilon to zero actually).&lt;/p&gt;

&lt;h3 id=&quot;reinforce&quot;&gt;REINFORCE&lt;/h3&gt;

&lt;p&gt;REINFORCE is a Monte-Carlo Policy Gradient (PG) method. In PGs, we try to find a policy to map the state into action directly.&lt;/p&gt;

&lt;p&gt;In value-based methods, we find a value function and use it to find the optimal policy. Policy gradient methods can be used for stochastic policies and continuous action spaces. If you want to use DQN for continuous action spaces, you have to discretize your action space. This will reduce the performance and if the number of actions is high, it will be difficult and impossible. But REINFORCE algorithms can be used for discrete or continuous action spaces. They are on-policy because they use the samples gathered from the current policy.&lt;/p&gt;

&lt;p&gt;There are different versions of REINFORCE. The first one is without a baseline. It is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/reinforce.png&quot; alt=&quot;reinforce algorithm&quot; title=&quot;from Sutton Barto book: Introduction to Reinforcement Learning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this version, we consider a policy (here a neural network) and initialize it with some random weights. Then we play for one episode and after that, we calculate discounted reward from each time step towards the end of the episode. This discounted reward (G in the above sudo code) will be multiplied by the gradient. This G is different based on the environment and the reward function we define. For example, consider that we have three actions. The first action is a bad action and the other two actions are some good actions that will cause more future discounted rewards. If we have three positive G values for three different actions, we are pushing the network towards all of them. Actually, we push the network towards action number one slightly and towards others more. Now consider we have one negative G value for the first action and two G values for the other two actions. Here we are pushing the network far from the first action and towards the other two actions. You see?! the value of G and its sign is important. It guides our gradient direction and its step size. To solve such problems, one way is to use baseline. This will reduce the variance and accelerate the learning procedure. For example, subtract the value of the state from it, or normalize it with the mean and variance of the discounted reward of the current episode. You can see the sudo code for REINFORCE with baseline in the following picture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/reinforce2.png&quot; alt=&quot;reinforce algorithm&quot; title=&quot;from Sutton Barto book: Introduction to Reinforcement Learning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this version, first, we initialize the policy and value networks. It is possible to use two separate networks or a multi-head network with a shared part. Then we play an episode and calculate the discounted reward from every step until the end of the episode (reward to go). Then subtract the value (from the learned neural net) for that state from the discounted reward (REINFORCE with baseline) and use it to update the weights of value and policy networks. Then generate another episode and repeat the loop.&lt;/p&gt;

&lt;p&gt;In the Sutton&amp;amp;Barto book, they do not consider the above algorithm as actor-critic (another RL algorithm that we will see in the next posts). It learns the value function but it is not used as a critic! I think it is because we do not use the learned value function (critic) in the first term of the policy gradient rescaler (for bootstrapping) to tell us how good is our policy or action in every step or in a batch of actions (in A2C and A3C we do the update every t_max step). In REINFORCE we update the network at the end of each episode.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“The REINFORCE method follows directly from the policy gradient theorem. Adding a state-value function as a baseline reduces REINFORCE’s variance without introducing bias. Using the state-value function for bootstrapping introduces bias but is often desirable for the same reason that bootstrapping TD methods are often superior to Monte Carlo methods (substantially reduced variance). The state-value function assigns credit to — critizes — the policy’s action selections, and accordingly the former is termed the critic and the latter the actor, and these overall methods are termed actor–critic methods.
Actor–critic methods are sometimes referred to as advantage actor–critic (“A2C”) methods in the literature.”&lt;/em&gt;
[Sutton&amp;amp;Barto — second edition]&lt;/p&gt;

&lt;p&gt;I think Monte-Carlo policy gradient and Actor-Critic policy gradient are good names as I saw in the slides of David Silver course.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/reinforce3.png&quot; alt=&quot;reinforce algorithm&quot; title=&quot;source: https://www.youtube.com/watch?v=KHZVXao4qXs&amp;amp;list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&amp;amp;index=7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I also saw the following slide from the Deep Reinforcement Learning and Control course (CMU 10703) at Carnegie Mellon University:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/reinforce4.png&quot; alt=&quot;reinforce algorithm&quot; title=&quot;source: https://www.andrew.cmu.edu/course//10-703/slides/Lecture_PG-NatGrad-10-8-2018.pdf&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here they consider every method that uses value function (V or Q) as actor-critic and if you just consider reward to go in the policy gradient rescaler, it is REINFORCE. The policy evaluation by the value function can be TD or MC.&lt;/p&gt;

&lt;p&gt;Summary of the categorization:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Vanilla REINFORCE or Policy gradient → we use G as gradient rescaler.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;REINFORCE with baseline → we use $\frac{G-mean(G)}{std(G)}$ or $(G-V)$ as gradient rescaler. We do not use $V$ in $G$. $G$ is only the reward to go for every step in the episode → $G_t = r_t + \gamma r_{t+1} + … $&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Actor-Critic → we use $V$ in the first term of gradient rescaler and call it Advantage ($A$):&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$A_t = Q(s_t, a_t) - V(s_t)$&lt;/p&gt;

&lt;p&gt;$A_t = r_t + \gamma V_{s_{t+1}} - V_{s_t}$ → for one-step&lt;/p&gt;

&lt;p&gt;$A_t = r_t + \gamma r_{t+1} + \gamma^2 V_{s_{t+2}} - V_{s_t}$ → for 2-step&lt;/p&gt;

&lt;p&gt;and so on.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In Actor-Critics you can do the update each $N$ step based on your task. This $N$ can be less than an episode.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anyway, let’s continue.&lt;/p&gt;

&lt;p&gt;This algorithm can be used for either discrete or continuous action spaces. In discrete action spaces, it will output a probability distribution over action, which means that the activation function of the output layer is a softmax. For exploration-exploitation, it samples from the actions based on their probabilities. Actions with higher probabilities have more chances to be selected.&lt;/p&gt;

&lt;p&gt;In continuous action spaces, the output will not have any softmax. Because the output is a mean for a normal distribution. We consider one neuron for each action and it can have any value. In fact, the policy is a normal distribution and we calculate its mean by a neural network. The variance can be fixed or decrease over time or can be learned. You can consider it as a function of the input state, or define it as a parameter that can be learned by gradient descent. If you want to learn the sigma too, you have to consider the number of actions. For example, if we want to map the front view image of a self-driving car into steering and throttle-brake, we have two continuous actions. So we have to have two mean and two variance for these two actions. During training, we sample from this normal distribution for exploration of the environment, but in the test, we only use the mean as action.&lt;/p&gt;

&lt;h3 id=&quot;a2c&quot;&gt;A2C&lt;/h3&gt;

&lt;h3 id=&quot;a3c&quot;&gt;A3C&lt;/h3&gt;

&lt;h3 id=&quot;ppo&quot;&gt;PPO&lt;/h3&gt;

&lt;h3 id=&quot;ddpg&quot;&gt;DDPG&lt;/h3&gt;

&lt;p&gt;This algorithm is from the &lt;em&gt;“Continuous Control with Deep Reinforcement Learning”&lt;/em&gt; &lt;a href=&quot;https://arxiv.org/pdf/1509.02971.pdf&quot;&gt;paper&lt;/a&gt; and uses the ideas from deep q-learning in the continuous action domain and is a model-free method based on the deterministic policy gradient.&lt;/p&gt;

&lt;p&gt;In Deterministic Policy Gradient (DPG), for each state, we have one clearly defined action to take (the output of policy is one value for action and for exploration we add a noise, normal noise for example, to the action). But in Stochastic Gradient Descent, we have a distribution over actions (the output of policy is mean and variance of a normal distribution) and sample from that distribution to get the action, for exploration. In another term, in stochastic policy gradient, we have a distribution with mean and variance and we draw a sample from that as an action. When we reduce the variance to zero, the policy will be deterministic.&lt;/p&gt;

&lt;p&gt;When the action space is discrete, such as q-learning, we get the max over q-values of all actions and select the best action. But in continuous action spaces, you cannot apply q-learning directly, because in continuous spaces finding the greedy policy requires optimization of $a_t$ at every time-step and would be too slow for large networks and continuous action spaces. Based on the proposed equation in the reference paper, here we approximate &lt;em&gt;max Q(s, a)&lt;/em&gt; over actions with &lt;em&gt;Q(a, µ(s))&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In DDPG, they used function approximators, neural nets, for both action-value function $Q$ and deterministic policy function $\mu$. In addition, DDPG uses some techniques for stabilizing training, such as updating the target networks using soft updating for both $\mu$ and $Q$. It also uses batch normalization layers, noise for exploration, and a replay buffer to break temporal correlations.&lt;/p&gt;

&lt;p&gt;This algorithm is an actor-critic method and the network structure is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/ddpg_post/ddpg_diagram.jpg&quot; alt=&quot;DDPG diagram&quot; title=&quot;DDPG diagram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;First, the policy network gets the state and outputs the action mean vector. This will be a vector of mean values for different actions. For example, in a self-driving car, there are two continuous actions: steering and acceleration&amp;amp;braking (one continuous value between $-x$ to $x$, the negative values are for braking and positive values are for acceleration). So we will have two mean for these two actions. To consider exploration, we can use Ornstein-Uhlenbeck or normal noise and add it to the action mean vector in the training phase. In the test phase, we can use the mean vector directly without any added noise. Then this action vector will be concatenated with observation and fed into the $Q$ network. The output of the $Q$ network will be one single value as a state-action value. In DQN, because it had discrete action space, we had multiple state-action values for each action, but here because the action space is continuous, we feed the actions into the $Q$ network and get one single value as the state-action value.&lt;/p&gt;

&lt;p&gt;Finally, the sudo code for DDPG is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/ddpg_post/ddpg_algorithm.jpg&quot; alt=&quot;DDPG algorithm&quot; title=&quot;DDPG algorithm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To understand the algorithm better, it’s good to try to implement it and play with its parameters and test it in different environments. Here is a good implementation in PyTorch that you can start with &lt;a href=&quot;https://github.com/higgsfield/RL-Adventure-2/blob/master/5.ddpg.ipynb&quot;&gt;this&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also found the Spinningup implementation of DDPG very clear and understandable too. You can find it &lt;a href=&quot;https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ddpg/ddpg.py&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For POMDP problems, it is possible to use LSTMs or any other RNN layers to get a sequence of observations. It needs a different type of replay buffer for sequential data.&lt;/p&gt;

&lt;h3 id=&quot;sac&quot;&gt;SAC&lt;/h3&gt;

&lt;h3 id=&quot;ape-x&quot;&gt;Ape-X&lt;/h3&gt;

&lt;h3 id=&quot;r2d2&quot;&gt;R2D2&lt;/h3&gt;

&lt;h3 id=&quot;impala&quot;&gt;IMPALA&lt;/h3&gt;

&lt;h3 id=&quot;never-give-up&quot;&gt;Never Give-Up&lt;/h3&gt;

&lt;h3 id=&quot;agent57&quot;&gt;Agent57&lt;/h3&gt;

&lt;h1 id=&quot;multi-agent&quot;&gt;Multi-Agent&lt;/h1&gt;

&lt;p&gt;In Multi-Agent Reinforcement Learning (MARL)problems, there are several agents who usually have their own private observation and want to take an action based on that observation. This observation is local and different from the full state of the environment in that time-step. The other problem that we face in such environments is the non-stationary problem because all agents are learning and their behavior would be different during training as they learn to act differently.&lt;/p&gt;

&lt;p&gt;To solve this problem, the most naive approach is to use single-agent RL algorithms for each agent and treat other agents as part of the environment. Some methods like Independent Q-Learning (IQL) work fine in some multi-agent RL problems in practice but there is no guarantee for them to converge. In IQL, each agent has one separate action-value function that gets the agent’s local observation to select its action based on that. It is also possible to use additional inputs like previous actions as input. Usually, in partially observable environments, we use RNNs to consider a history of several sequential observation-actions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/marl1.png&quot; alt=&quot;MARL&quot; title=&quot; source: https://arxiv.org/pdf/1706.05296.pdf &quot; /&gt;&lt;/p&gt;

&lt;p&gt;The other approach is to have a fully centralised method to learn and act in a centralised fashion. We can consider this type as a big single-agent problem. This approach is also valid in some problems that you don’t need decentralised execution. For example for traffic management or traffic light management, it is possible to use such approaches.&lt;/p&gt;

&lt;p&gt;There is one more case that is somewhere between the previous two ones: centralised training and decentralised execution. Usually in the training procedure, because we train agents in a simulation environment or in a lab, we have access to the full state and information in the training phase. So it is better to use this knowledge. On the other hand, the learned policy should be decentralised in some environments and agents cannot have access to the full state during the execution phase. So having algorithms to use the available knowledge in the training phase and learn a policy that is not dependent on the full state in the execution time is necessary. Here we focus on the last case.&lt;/p&gt;

&lt;p&gt;There are several works that try to propose such an algorithm and can be divided into two groups:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;value-based methods like Value Decomposition Networks (VDN) and QMIX&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;actor-critic methods like MADDPG and COMA&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;value-based-methods&quot;&gt;VALUE-BASED METHODS&lt;/h2&gt;

&lt;p&gt;These approaches try to propose a way to be able to use value-based methods like Q-learning and train them in a centralised way and use them for decentralised execution.&lt;/p&gt;

&lt;h3 id=&quot;vdn&quot;&gt;VDN&lt;/h3&gt;

&lt;p&gt;This work proposes a way to have separate action-value functions for multiple agents and learn them by just one shared team reward signal. The joint action-value function is a linear summation of all action-value functions of all agents. Actually, by using a single shared reward signal, it tries to learn decomposed value functions for each agent and use it for decentralised execution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/vdn1.png&quot; alt=&quot;VDN&quot; title=&quot; source: https://arxiv.org/pdf/1706.05296.pdf &quot; /&gt;&lt;/p&gt;

&lt;p&gt;Consider a case with 2 agents, the reward would be:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/vdn2.png&quot; alt=&quot;VDN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then the total $Q$ function is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/vdn3.png&quot; alt=&quot;VDN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is using the same Bellman equation to standard q-learning approach and just replaces $Q$ in that equation with the new $Q$ value.&lt;/p&gt;

&lt;h3 id=&quot;qmix&quot;&gt;QMIX&lt;/h3&gt;

&lt;p&gt;QMIX is somehow an extension to value decomposition networks (VDN) but tries to mix the Q-value of different agents in a nonlinear way. They use global state $s_t$ as input to hypernetworks to generate weights and biases of the mixing network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/qmix1.png&quot; alt=&quot;QMIX&quot; title=&quot; source: https://arxiv.org/pdf/1803.11485.pdf&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here again, the equation to update the weights is the standard Bellman equation in which the $Q$ is replaced with $Q_tot$ in the above figure.&lt;/p&gt;

&lt;h2 id=&quot;actor-critic-based-methods&quot;&gt;ACTOR-CRITIC BASED METHODS&lt;/h2&gt;

&lt;p&gt;This group of methods tries to use actor-critic architecture to do centralised training and decentralised execution. Usually, they use the full state and additional information which are available in the training phase in the critic network to generate a richer signal for the actor.&lt;/p&gt;

&lt;h3 id=&quot;maddpg&quot;&gt;MADDPG&lt;/h3&gt;

&lt;p&gt;Multi-Agent DDPG (MADDPG) is a method to use separate actors and critics for each agent and train the critic in a centralised way and use the actor in execution. So each agent has one actor and one critic. The actor has access to its own action-observation data and is trained by them and the critic has access to observation and action of all agents and is trained by all of them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/maddpg.png&quot; alt=&quot;MADDPG&quot; title=&quot; source: https://arxiv.org/pdf/1706.02275.pdf &quot; /&gt;&lt;/p&gt;

&lt;p&gt;The centralised action-value function for each agent can be written as:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/maddpg2.png&quot; alt=&quot;MADDPG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And the gradient can be written as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/maddpg3.png&quot; alt=&quot;MADDPG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you see, the policy is conditioned on the observation of the agent itself, o_i, and the critic is conditioned on the full state and actions of all agents.
This separate critic for each agent allows us to have agents with different rewards, cooperative or competitive behaviors.&lt;/p&gt;

&lt;h3 id=&quot;coma&quot;&gt;COMA&lt;/h3&gt;

&lt;p&gt;The talk can be found &lt;a href=&quot;https://www.youtube.com/watch?v=3OVvjE5B9LU&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Counterfactual Multi-Agent (COMA) policy gradient is a method for cooperative multi-agent systems and uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents’ policies. In addition, to address the problem of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent’s action, while keeping the other agents’ actions fixed. The idea comes from difference rewards, in which each agent learns from a shaped reward $D_a = r(s, u) − r(s,(u^{-a}, c_a))$ that compares the global reward to the reward received when the action of agent $a$ is replaced with a default action $c_a$.&lt;/p&gt;

&lt;p&gt;COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/coma.png&quot; alt=&quot;COMA&quot; title=&quot; source: https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/17193 &quot; /&gt;&lt;/p&gt;

&lt;p&gt;For each agent $a$, we can then compute an advantage function that compares the Q-value for the current action $u^a$ to a counterfactual baseline that marginalizes out $u^a$, while keeping the other agents’ actions $u^{-a}$ fixed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/coma2.png&quot; alt=&quot;COMA&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In contrast to MADDPG, COMA is an on-policy approach and has only one critic network.&lt;/p&gt;</content><author><name></name></author><summary type="html">Online Reinforcement Learning</summary></entry><entry><title type="html">AlphaGo - Mastering the game of Go with deep neural networks and tree search</title><link href="https://kargarisaac.github.io/blog/jupyter/2020/04/12/AlphaGo.html" rel="alternate" type="text/html" title="AlphaGo - Mastering the game of Go with deep neural networks and tree search" /><published>2020-04-12T00:00:00-05:00</published><updated>2020-04-12T00:00:00-05:00</updated><id>https://kargarisaac.github.io/blog/jupyter/2020/04/12/AlphaGo</id><content type="html" xml:base="https://kargarisaac.github.io/blog/jupyter/2020/04/12/AlphaGo.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-12-AlphaGo.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_0.png?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_1.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_2.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_3.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_4.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_5.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_6.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_7.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_8.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;That’s it for the first one. In the next post, I will review the AlphaGo Zero paper.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Isaac Kargar</name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kargarisaac.github.io/blog/_notebooks/my_icons/alphago/alphago_0.png" /><media:content medium="image" url="https://kargarisaac.github.io/blog/_notebooks/my_icons/alphago/alphago_0.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>