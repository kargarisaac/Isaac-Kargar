<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://kargarisaac.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kargarisaac.github.io/blog/" rel="alternate" type="text/html" /><updated>2022-02-04T10:33:21-06:00</updated><id>https://kargarisaac.github.io/blog/feed.xml</id><title type="html">Isaac Kargar</title><subtitle>My posts about Machine Learning</subtitle><entry><title type="html">Data Engineering - Week 3</title><link href="https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/30/data-engineering-w3.html" rel="alternate" type="text/html" title="Data Engineering - Week 3" /><published>2022-01-30T00:00:00-06:00</published><updated>2022-01-30T00:00:00-06:00</updated><id>https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/30/data-engineering-w3</id><content type="html" xml:base="https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/30/data-engineering-w3.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-01-30-data-engineering-w3.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Data-Warehouse&quot;&gt;Data Warehouse&lt;a class=&quot;anchor-link&quot; href=&quot;#Data-Warehouse&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;OLTP-vs-OLAP&quot;&gt;OLTP vs OLAP&lt;a class=&quot;anchor-link&quot; href=&quot;#OLTP-vs-OLAP&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The two terms look similar but refer to different kinds of systems. Online transaction processing (OLTP) captures, stores, and processes data from transactions in real time. Online analytical processing (OLAP) uses complex queries to analyze aggregated historical data from OLTP systems.&lt;/p&gt;
&lt;p&gt;An OLTP system is a database that captures and retains transaction data. Individual database entries made up of numerous fields or columns are involved in each transaction. Banking and credit card transactions, as well as retail checkout scanning, are examples.
Because OLTP databases are read, written, and updated frequently, the emphasis in OLTP is on fast processing. Built-in system logic protects data integrity if a transaction fails.&lt;/p&gt;
&lt;p&gt;For data mining, analytics, and business intelligence initiatives, OLAP applies complicated queries to massive amounts of historical data aggregated from OLTP databases and other sources. The emphasis in OLAP is on query response speed for these complicated queries. Each query has one or more columns of data derived from a large number of rows. Financial performance year over year or marketing lead generation trends are two examples. Analysts and decision-makers can utilize custom reporting tools to turn data into information using OLAP databases and data warehouses. OLAP query failure does not affect or delay client transaction processing, but it can affect or delay the accuracy of business intelligence insights. [&lt;a href=&quot;https://www.stitchdata.com/resources/oltp-vs-olap/#:~:text=OLTP%20and%20OLAP%3A%20The%20two,historical%20data%20from%20OLTP%20systems.&quot;&gt;ref&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w3/1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w3/2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Data-Warehouse&quot;&gt;Data Warehouse&lt;a class=&quot;anchor-link&quot; href=&quot;#Data-Warehouse&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;
&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/jrHljAoD6nM&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;A data warehouse (DW or DWH), often known as an enterprise data warehouse (EDW), is a reporting and data analysis system that is considered a key component of business intelligence. DWs are central data repositories that combine data from a variety of sources. They keep current and historical data in one place and utilize it to generate analytical reports for employees across the company.&lt;/p&gt;
&lt;p&gt;The data in the warehouse comes from the operating systems and is uploaded there (such as marketing or sales). Before being used in the DW for reporting, the data may transit via an operational data store and require data cleansing for extra procedures to ensure data quality.&lt;/p&gt;
&lt;p&gt;The two major methodologies used to design a data warehouse system are extract, transform, load (ETL) and extract, load, transform (ELT). [&lt;a href=&quot;https://en.wikipedia.org/wiki/Data_warehouse&quot;&gt;wikipedia&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w3/3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Google BigQuery and Amazon Redshift are two data warehouse services proposed by google and amazon.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;BigQuery&quot;&gt;BigQuery&lt;a class=&quot;anchor-link&quot; href=&quot;#BigQuery&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;BigQuery is a fully managed enterprise data warehouse that helps you manage and analyze your data with built-in features like machine learning, geospatial analysis, and business intelligence. BigQuery's serverless architecture lets you use SQL queries to answer your organization's biggest questions with zero infrastructure management. BigQuery's scalable, distributed analysis engine lets you query terabytes in seconds and petabytes in minutes.&lt;/p&gt;
&lt;p&gt;BigQuery maximizes flexibility by separating the compute engine that analyzes your data from your storage choices. You can store and analyze your data within BigQuery or use BigQuery to assess your data where it lives. Federated queries let you read data from external sources while streaming supports continuous data updates. Powerful tools like BigQuery ML and BI Engine let you analyze and understand that data.&lt;/p&gt;
&lt;p&gt;BigQuery interfaces include Google Cloud Console interface and the BigQuery command-line tool. Developers and data scientists can use client libraries with familiar programming including Python, Java, JavaScript, and Go, as well as BigQuery's REST API and RPC API to transform and manage data. ODBC and JDBC drivers provide interaction with existing applications including third-party tools and utilities.&lt;/p&gt;
&lt;p&gt;As a data analyst, data engineer, data warehouse administrator, or data scientist, the BigQuery ML documentation helps you discover, implement, and manage data tools to inform critical business decisions. [&lt;a href=&quot;https://cloud.google.com/bigquery/docs/introduction&quot;&gt;BigQuery docs&lt;/a&gt;]

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/CFw4peH2UwU&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Partitioning&quot;&gt;Partitioning&lt;a class=&quot;anchor-link&quot; href=&quot;#Partitioning&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;A partitioned table is a special table that is divided into segments, called partitions, that make it easier to manage and query your data. You can typically split large tables into many smaller partitions using data ingestion time or &lt;code&gt;TIMESTAMP/DATE&lt;/code&gt; column or an &lt;code&gt;INTEGER&lt;/code&gt; column. BigQuery’s decoupled storage and compute architecture leverages column-based partitioning simply to minimize the amount of data that slot workers read from disk. Once slot workers read their data from disk, BigQuery can automatically determine more optimal data sharding and quickly repartition data using BigQuery’s in-memory shuffle service. [&lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explained-storage-overview&quot;&gt;source&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w3/4.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explained-storage-overview&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Check the previous video and also &lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explained-storage-overview&quot;&gt;here&lt;/a&gt; to see an example of the performance gain.&lt;/p&gt;
&lt;p&gt;The example SQL query can be like this:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OR&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;REPLACE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stackoverflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;questions_2018_partitioned&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;PARTITION&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BY&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;creation_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AS&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SELECT&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;FROM&lt;/span&gt;
 &lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigquery&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;public&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stackoverflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;posts_questions&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;WHERE&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;creation_date&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BETWEEN&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;2018-01-01&amp;#39;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;2018-07-01&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Clustering&quot;&gt;Clustering&lt;a class=&quot;anchor-link&quot; href=&quot;#Clustering&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;When a table is clustered in BigQuery, the table data is automatically organized based on the contents of one or more columns in the table’s schema. The columns you specify are used to collocate related data. Usually high cardinality and non-temporal columns are preferred for clustering.&lt;/p&gt;
&lt;p&gt;When data is written to a clustered table, BigQuery sorts the data using the values in the clustering columns. These values are used to organize the data into multiple blocks in BigQuery storage. The order of clustered columns determines the sort order of the data. When new data is added to a table or a specific partition, BigQuery performs automatic re-clustering in the background to restore the sort property of the table or partition. Auto re-clustering is completely free and autonomous for the users. [&lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explained-storage-overview&quot;&gt;source&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w3/5.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explained-storage-overview&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Clustering can improve the performance of certain types of queries, such as those using filter clauses and queries aggregating data.
When a query containing a filter clause filters data based on the clustering columns, BigQuery uses the sorted blocks to eliminate scans of unnecessary data.
When a query aggregates data based on the values in the clustering columns, performance is improved because the sorted blocks collocate rows with similar values.
BigQuery supports clustering over both partitioned and non-partitioned tables. When you use clustering and partitioning together, your data can be partitioned by a DATE or TIMESTAMP column and then clustered on a different set of columns (up to four columns). [&lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explained-storage-overview&quot;&gt;source&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;Check the previous video and also &lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explained-storage-overview&quot;&gt;here&lt;/a&gt; to see an example of the performance gain.&lt;/p&gt;
&lt;p&gt;The example SQL query can be like this:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;CREATE OR REPLACE TABLE &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;stackoverflow.questions_2018_clustered&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
PARTITION BY
 DATE&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;creation_date&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
CLUSTER BY
 tags AS
SELECT
 *
FROM
 &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;bigquery-public-data.stackoverflow.posts_questions&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
WHERE
 creation_date BETWEEN &lt;span class=&quot;s1&quot;&gt;&amp;#39;2018-01-01&amp;#39;&lt;/span&gt; AND &lt;span class=&quot;s1&quot;&gt;&amp;#39;2018-07-01&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Check the following video to learn more:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/-CqXf7vhhDs&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;You can also chech this &lt;a href=&quot;https://codelabs.developers.google.com/codelabs/gcp-bq-partitioning-and-clustering#0&quot;&gt;codelab&lt;/a&gt; to learn more about partitioning and clustering.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Now let's see how to load or ingest data into BigQuery and analyze them.&lt;/p&gt;
&lt;h4 id=&quot;Direct-Import-(Managed-Tables):&quot;&gt;Direct Import (Managed Tables):&lt;a class=&quot;anchor-link&quot; href=&quot;#Direct-Import-(Managed-Tables):&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;BigQuery can ingest datasets from a variety of different formats directly into its native storage. BigQuery native storage is fully managed by Google—this includes replication, backups, scaling out size, and much more.&lt;/p&gt;
&lt;p&gt;There are multiple ways to load data into BigQuery depending on data sources, data formats, load methods and use cases such as batch, streaming or data transfer. At a high level following are the ways you can ingest data into BigQuery:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Batch Ingestion: Batch ingestion involves loading large, bounded, data sets that don’t have to be processed in real-time&lt;/li&gt;
&lt;li&gt;Streaming Ingestion: Streaming ingestion supports use cases that require analyzing high volumes of continuously arriving data with near-real-time dashboards and queries.&lt;/li&gt;
&lt;li&gt;Data Transfer Service (DTS): The BigQuery Data Transfer Service (DTS) is a fully managed service to ingest data from Google SaaS apps such as Google Ads, external cloud storage providers such as Amazon S3 and transferring data from data warehouse technologies such as Teradata and Amazon Redshift .&lt;/li&gt;
&lt;li&gt;Query Materialization: When you run queries in BigQuery their result sets can be materialized to create new tables. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w3/6.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explained-data-ingestion&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&quot;Query-without-Loading-(External-Tables)&quot;&gt;Query without Loading (External Tables)&lt;a class=&quot;anchor-link&quot; href=&quot;#Query-without-Loading-(External-Tables)&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Using a federated query is one of the options to query external data sources directly without loading into BigQuery storage. You can query across Google services such as Google Sheets, Google Drive, Google Cloud Storage, Cloud SQL or Cloud BigTable without having to import the data into BigQuery.&lt;/p&gt;
&lt;p&gt;You don’t need to load data into BigQuery before running queries in the following situations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Public Datasets: Public datasets are datasets stored in BigQuery and shared with the public. &lt;/li&gt;
&lt;li&gt;Shared Datasets: You can share datasets stored in BigQuery. If someone has shared a dataset with you, you can run queries on that dataset without loading the data.&lt;/li&gt;
&lt;li&gt;External data sources (Federated): You can skip the data loading process by creating a table based on an external data source.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Apart from the solutions available natively in BigQuery, you can also check data integration options from Google Cloud partners who have integrated their industry-leading tools with BigQuery.&lt;/p&gt;
&lt;p&gt;To read more, please check &lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explained-data-ingestion&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Check the following video for BQ best practices:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/k81mLJVX08w&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here are some example queries in BQ which shows how to do partitioning and clustering on a public available dataset in BQ [&lt;a href=&quot;https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_3_data_warehouse/big_query.sql&quot;&gt;ref&lt;/a&gt;]:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;-- Query public available table&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;station_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bigquery&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;public&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_york_citibike&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;citibike_stations&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;LIMIT&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;-- Creating external table referring to gcs path&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;OR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;REPLACE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;EXTERNAL&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;external_yellow_tripdata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;OPTIONS&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;format&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;CSV&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;uris&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;gs://nyc-tl-data/trip data/yellow_tripdata_2019-*.csv&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;gs://nyc-tl-data/trip data/yellow_tripdata_2020-*.csv&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;-- Check yello trip data&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;external_yellow_tripdata&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;limit&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;-- Create a non partitioned table from external table&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;OR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;REPLACE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow_tripdata_non_partitoned&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;external_yellow_tripdata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;-- Create a partitioned table from external table&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;OR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;REPLACE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow_tripdata_partitoned&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;PARTITION&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;external_yellow_tripdata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;-- Impact of partition&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;-- Scanning 1.6GB of data&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DISTINCT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VendorID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow_tripdata_non_partitoned&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BETWEEN&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;2019-06-01&amp;#39;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;2019-06-30&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;-- Scanning ~106 MB of DATA&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DISTINCT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VendorID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow_tripdata_partitoned&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BETWEEN&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;2019-06-01&amp;#39;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;2019-06-30&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;-- Let&amp;#39;s look into the partitons&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;table_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partition_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_rows&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INFORMATION_SCHEMA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PARTITIONS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;table_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;yellow_tripdata_partitoned&amp;#39;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_rows&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DESC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;-- Creating a partition and cluster table&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;OR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;REPLACE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow_tripdata_partitoned_clustered&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;PARTITION&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CLUSTER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VendorID&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;external_yellow_tripdata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;-- Query scans 1.1 GB&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trips&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow_tripdata_partitoned&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BETWEEN&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;2019-06-01&amp;#39;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;2020-12-31&amp;#39;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VendorID&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;-- Query scans 864.5 MB&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trips&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow_tripdata_partitoned_clustered&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BETWEEN&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;2019-06-01&amp;#39;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;2020-12-31&amp;#39;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VendorID&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Furthermore, if you are curious to know about internals of BQ, you can check &lt;a href=&quot;https://cloud.google.com/blog/products/data-analytics/new-blog-series-bigquery-explained-overview&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explained-storage-overview&quot;&gt;here&lt;/a&gt; and also the following video.

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/k81mLJVX08w&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;It is also possible to do machine learning in BigQuery instead of doing it outside of it. BigQuery ML increases development speed by eliminating the need to move data.&lt;/p&gt;
&lt;p&gt;BigQuery ML supports the following types of models: [&lt;a href=&quot;https://cloud.google.com/bigquery-ml/docs/introduction&quot;&gt;ref&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear regression for forecasting; for example, the sales of an item on a given day. Labels are real-valued (they cannot be +/- infinity or NaN).&lt;/li&gt;
&lt;li&gt;Binary logistic regression for classification; for example, determining whether a customer will make a purchase. Labels must only have two possible values.&lt;/li&gt;
&lt;li&gt;Multiclass logistic regression for classification. These models can be used to predict multiple possible values such as whether an input is &quot;low-value,&quot; &quot;medium-value,&quot; or &quot;high-value.&quot; Labels can have up to 50 unique values. In BigQuery ML, multiclass logistic regression training uses a multinomial classifier with a cross-entropy loss function.&lt;/li&gt;
&lt;li&gt;K-means clustering for data segmentation; for example, identifying customer segments. K-means is an unsupervised learning technique, so model training does not require labels nor split data for training or evaluation.&lt;/li&gt;
&lt;li&gt;Matrix Factorization for creating product recommendation systems. You can create product recommendations using historical customer behavior, transactions, and product ratings and then use those recommendations for personalized customer experiences.&lt;/li&gt;
&lt;li&gt;Time series for performing time-series forecasts. You can use this feature to create millions of time series models and use them for forecasting. The model automatically handles anomalies, seasonality, and holidays.&lt;/li&gt;
&lt;li&gt;Boosted Tree for creating XGBoost based classification and regression models.&lt;/li&gt;
&lt;li&gt;Deep Neural Network (DNN) for creating TensorFlow-based Deep Neural Networks for classification and regression models.&lt;/li&gt;
&lt;li&gt;AutoML Tables to create best-in-class models without feature engineering or model selection. AutoML Tables searches through a variety of model architectures to decide the best model.&lt;/li&gt;
&lt;li&gt;TensorFlow model importing. This feature lets you create BigQuery ML models from previously trained TensorFlow models, then perform prediction in BigQuery ML.&lt;/li&gt;
&lt;li&gt;Autoencoder for creating Tensorflow-based BigQuery ML models with the support of sparse data representations. The models can be used in BigQuery ML for tasks such as unsupervised anomaly detection and non-linear dimensionality reduction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w3/8.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://cloud.google.com/blog/products/data-analytics/automl-tables-now-generally-available-bigquery-ml&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Check &lt;a href=&quot;https://cloud.google.com/bigquery-ml/docs/introduction&quot;&gt;here&lt;/a&gt; and the following video to learn more about how to train a linear regression model in BQ:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/B-WtpB0PuG4&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here is an example of training a liner regression model in BQ on the dataset we uploaded to GCS and BQ in the previous week using Airflow, and also how to do hyperparameter tuning: [&lt;a href=&quot;https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_3_data_warehouse/extract_model.md&quot;&gt;ref1&lt;/a&gt;]&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;-- SELECT THE COLUMNS INTERESTED FOR YOU&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;passenger_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trip_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PULocationID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DOLocationID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;payment_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fare_amount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tolls_amount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tip_amount&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow_tripdata_partitoned&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fare_amount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;-- CREATE A ML TABLE WITH APPROPRIATE TYPE&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;OR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;REPLACE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow_tripdata_ml&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;passenger_count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;INTEGER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trip_distance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FLOAT64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PULocationID&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;STRING&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DOLocationID&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;STRING&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;payment_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;STRING&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fare_amount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FLOAT64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolls_amount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FLOAT64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tip_amount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FLOAT64&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;passenger_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trip_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;cast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PULocationID&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;STRING&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;CAST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DOLocationID&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;STRING&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CAST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;payment_type&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;STRING&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fare_amount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tolls_amount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tip_amount&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow_tripdata_partitoned&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fare_amount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;-- CREATE MODEL WITH DEFAULT SETTING&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;OR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;REPLACE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MODEL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tip_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;OPTIONS&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;linear_reg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_label_cols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;tip_amount&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;DATA_SPLIT_METHOD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;AUTO_SPLIT&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow_tripdata_ml&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tip_amount&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IS&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NOT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;-- CHECK FEATURES&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ML&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FEATURE_INFO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MODEL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tip_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;-- EVALUATE THE MODEL&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ML&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EVALUATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MODEL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tip_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow_tripdata_ml&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tip_amount&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IS&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NOT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;-- PREDICT THE MODEL&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ML&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PREDICT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MODEL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tip_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow_tripdata_ml&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tip_amount&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IS&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NOT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;-- PREDICT AND EXPLAIN&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ML&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EXPLAIN_PREDICT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MODEL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tip_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow_tripdata_ml&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tip_amount&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IS&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NOT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;STRUCT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_k_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;-- HYPER PARAM TUNNING&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;OR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;REPLACE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MODEL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tip_hyperparam_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;OPTIONS&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;linear_reg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_label_cols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;tip_amount&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;DATA_SPLIT_METHOD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;AUTO_SPLIT&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_trials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;max_parallel_trials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;l1_reg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hparam_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;l2_reg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hparam_candidates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nytaxi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow_tripdata_ml&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tip_amount&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IS&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NOT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;After training the model, we need to deploy it. The following video explains how to that:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/BjARzEWaznU&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;And here is the deployment steps: [&lt;a href=&quot;https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_3_data_warehouse/extract_model.md&quot;&gt;ref1&lt;/a&gt;, &lt;a href=&quot;https://cloud.google.com/bigquery-ml/docs/export-model-tutorial&quot;&gt;ref2&lt;/a&gt;]&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;- gcloud auth login
- bq --project_id taxi-rides-ny extract -m nytaxi.tip_model gs://taxi_ml_model/tip_model
- mkdir /tmp/model
- gsutil cp -r gs://taxi_ml_model/tip_model /tmp/model
- mkdir -p serving_dir/tip_model/1
- cp -r /tmp/model/tip_model/* serving_dir/tip_model/1
- docker pull tensorflow/serving
- docker run -p &lt;span class=&quot;m&quot;&gt;8501&lt;/span&gt;:8501 --mount &lt;span class=&quot;nv&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;bind,source&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;/serving_dir/tip_model,target&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  /models/tip_model -e &lt;span class=&quot;nv&quot;&gt;MODEL_NAME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;tip_model -t tensorflow/serving &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt;
- curl -d &lt;span class=&quot;s1&quot;&gt;&amp;#39;{&amp;quot;instances&amp;quot;: [{&amp;quot;passenger_count&amp;quot;:1, &amp;quot;trip_distance&amp;quot;:12.2, &amp;quot;PULocationID&amp;quot;:&amp;quot;193&amp;quot;, &amp;quot;DOLocationID&amp;quot;:&amp;quot;264&amp;quot;, &amp;quot;payment_type&amp;quot;:&amp;quot;2&amp;quot;,&amp;quot;fare_amount&amp;quot;:20.4,&amp;quot;tolls_amount&amp;quot;:0.0}]}&amp;#39;&lt;/span&gt; -X POST http://localhost:8501/v1/models/tip_model:predict
- http://localhost:8501/v1/models/tip_model
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://www.visual-design.net/post/how-to-build-ml-model-using-bigquery&quot;&gt;Here&lt;/a&gt; is another nice blog post on using ML in BQ.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Other useful resources:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/ZVgt1-LfWW4&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Amazon-Redshift&quot;&gt;Amazon Redshift&lt;a class=&quot;anchor-link&quot; href=&quot;#Amazon-Redshift&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Amazon Redshift is another data warehouse service by AWS which is not covered in the course, but very similar to BigQuery. It is a fully managed, petabyte-scale data warehouse service in the AWS Cloud. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Amazon Redshift ML makes it easy for data analysts and database developers to create, train, and apply machine learning models using familiar SQL commands in Amazon Redshift data warehouses. With Redshift ML, you can take advantage of Amazon SageMaker, a fully managed machine learning service, without learning new tools or languages. Simply use SQL statements to create and train Amazon SageMaker machine learning models using your Redshift data and then use these models to make predictions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w3/7.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://aws.amazon.com/redshift/features/redshift-ml/&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" /><media:content medium="image" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Data Engineering - Week 2</title><link href="https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/25/data-engineering-w2.html" rel="alternate" type="text/html" title="Data Engineering - Week 2" /><published>2022-01-25T00:00:00-06:00</published><updated>2022-01-25T00:00:00-06:00</updated><id>https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/25/data-engineering-w2</id><content type="html" xml:base="https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/25/data-engineering-w2.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-01-25-data-engineering-w2.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The content of this post is from the course videos, my understandings and searches, and reference documentations.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;
&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/W3Zm6rjOq70&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Data-Lake&quot;&gt;Data Lake&lt;a class=&quot;anchor-link&quot; href=&quot;#Data-Lake&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w2/1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;A data lake is a collection of technologies that enables querying of data contained in files or blob objects. When used effectively, they enable massive scale and cost-effective analysis of structured and unstructured data assets [&lt;a href=&quot;https://lakefs.io/data-lakes/&quot;&gt;source&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Data lakes are comprised of four primary components: storage, format, compute, and metadata layers [&lt;a href=&quot;https://lakefs.io/data-lakes/&quot;&gt;source&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w2/2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;A data lake is a centralized repository for large amounts of data from a variety of sources. Data can be structured, semi-structured, or unstructured in general.
The goal is to rapidly ingest data and make it available to or accessible to other team members such as data scientists, analysts, and engineers.
The data lake is widely used for machine learning and analytical solutions.
Generally, when you store data in a data lake, you associate it with some form of metadata to facilitate access. Generally, a data lake solution must be secure and scalable.
Additionally, the hardware should be affordable. The reason for this is that you want to store as much data as possible quickly.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Data-Lake-vs-Data-Warehouse&quot;&gt;Data Lake vs Data Warehouse&lt;a class=&quot;anchor-link&quot; href=&quot;#Data-Lake-vs-Data-Warehouse&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w2/3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Generally a data lake is an unstructured data and the target users are data scientists or data analysts. It stores huge amount of data, sometimes in the size of petabytes and terabytes. The use cases which are covered by data lake are basically stream processing, machine learning, and real-time analytics.
On the data warehouse side, the data is generally structured. The users are business analysts, the data size is generally small, and the use case consists of batch processing or BI reporting.&lt;/p&gt;
&lt;p&gt;To read more, please check &lt;a href=&quot;https://lakefs.io/data-lakes/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://luminousmen.com/post/data-lake-vs-data-warehouse&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;ETL-vs-ELT&quot;&gt;ETL vs ELT&lt;a class=&quot;anchor-link&quot; href=&quot;#ETL-vs-ELT&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Extract Transform and Load vs Extract Load and Transform&lt;/li&gt;
&lt;li&gt;ETL is mainly used for a small amount of data whereas ELT is used for large amounts of data&lt;/li&gt;
&lt;li&gt;ELT provides data lake support (Schema on read)&lt;/li&gt;
&lt;li&gt;ETL provides data warehouse solutions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w2/4.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.guru99.com/etl-vs-elt.html#:~:text=ETL%20stands%20for%20Extract%2C%20Transform,directly%20into%20the%20target%20system.&amp;amp;text=ETL%2C%20ETL%20is%20mainly%20used,for%20large%20amounts%20of%20data.&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w2/5.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.guru99.com/etl-vs-elt.html#:~:text=ETL%20stands%20for%20Extract%2C%20Transform,directly%20into%20the%20target%20system.&amp;amp;text=ETL%2C%20ETL%20is%20mainly%20used,for%20large%20amounts%20of%20data.&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Data lake solutions provided by main cloud providers are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GCP - cloud storage&lt;/li&gt;
&lt;li&gt;AWS - S3&lt;/li&gt;
&lt;li&gt;AZURE - AZURE BLOB&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Workflow-Orchestration&quot;&gt;Workflow Orchestration&lt;a class=&quot;anchor-link&quot; href=&quot;#Workflow-Orchestration&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;
&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/0yK7LXwYeD0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We saw a simple data pipeline in week 1. One of the problems in that data pipeline was that we did several important jobs in the same place: downloading data and doing small processing and putting it into postgres. What if after downloading data, some error happens in the code or with the internet? We will lose the downloaded data and should do everything from scratch. That's why we need to do those steps separately.&lt;/p&gt;
&lt;p&gt;A data pipeline is a series of steps for data processing. If the data has not yet been loaded into the data platform, it is ingested at the pipeline's start. Then there is a series of steps, each of which produces an output that serves as the input for the subsequent step. This procedure is repeated until the pipeline is completed. In some instances, independent steps may be performed concurrently. [&lt;a href=&quot;https://hazelcast.com/glossary/data-pipeline/&quot;&gt;source&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;A data pipeline is composed of three critical components: a source, a processing step or series of processing steps, and a destination. The destination may be referred to as a sink in some data pipelines. Data pipelines, for example, enable the flow of data from an application to a data warehouse, from a data lake to an analytics database, or to a payment processing system. Additionally, data pipelines can share the same source and sink, allowing the pipeline to focus entirely on data modification. When data is processed between points A and B (or B, C, and D), there is a data pipeline between those points [&lt;a href=&quot;https://hazelcast.com/glossary/data-pipeline/&quot;&gt;source&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w2/6.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://hazelcast.com/glossary/data-pipeline/&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In our example, the data pipeline we had in the previous week can be as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w2/7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;We separated downloading dataset using &lt;code&gt;wget&lt;/code&gt; and then ingesting it into postgres. I think we can have even another more step for processing (changing the string to datetime in the downloaded dataset).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;But this week, we will do something more complex. Let's have a look at the data workflow.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w2/8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The above figure is called a DAG (Directed Acyclic Graph). We need to be sure that all steps are done sequentially and we can retry some of the steps if some thing happens and then go to the next step. There are some tools called workflow engines tat allow us to define these DAGs and do the data workflow orchestration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LUIGI&lt;/li&gt;
&lt;li&gt;APACHE AIRFLOW (we will go for this)&lt;/li&gt;
&lt;li&gt;PREFECT&lt;/li&gt;
&lt;li&gt;Google Cloud Dataflow&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let's get more familiar with the last two ones:&lt;/p&gt;
&lt;h3 id=&quot;Airflow&quot;&gt;Airflow&lt;a class=&quot;anchor-link&quot; href=&quot;#Airflow&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Airflow is a platform to programmatically author, schedule and monitor workflows.
Use Airflow to author workflows as Directed Acyclic Graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.
When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative [&lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/index.html&quot;&gt;Airflow docs&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w2/airflow.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/index.html&quot;&gt;Airflow docs&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Google-Cloud-Dataflow&quot;&gt;Google Cloud Dataflow&lt;a class=&quot;anchor-link&quot; href=&quot;#Google-Cloud-Dataflow&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Real-time data is generated by websites, mobile applications, IoT devices, and other workloads. All businesses make data collection, processing, and analysis a priority. However, data from these systems is frequently not in a format suitable for analysis or effective use by downstream systems. That is where Dataflow enters the picture! Dataflow is used to process and enrich batch or stream data for analysis, machine learning, and data warehousing applications.&lt;/p&gt;
&lt;p&gt;Dataflow is a serverless, high-performance, and cost-effective service for stream and batch processing. It enables portability for processing jobs written in the open source Apache Beam libraries and alleviates operational burden on your data engineering teams by automating infrastructure provisioning and cluster management [&lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/dataflow-backbone-data-analytics&quot;&gt;Google cloud docs&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w2/9.jpeg&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/dataflow-backbone-data-analytics&quot;&gt;Google cloud docs&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://stackshare.io/stackups/airflow-vs-google-cloud-dataflow&quot;&gt;Here&lt;/a&gt; is a comparison between Airflow and Google cloud dataflow.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Airflow&quot;&gt;Airflow&lt;a class=&quot;anchor-link&quot; href=&quot;#Airflow&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;
&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/lqDMzReAtrw&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w2/10.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html&quot;&gt;Airflow architecture&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let's review the Airflow architecture. An Airflow installation generally consists of the following components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Web server&lt;/strong&gt;: GUI to inspect, trigger and debug the behaviour of DAGs and tasks. Available at &lt;a href=&quot;http://localhost:8080&quot;&gt;http://localhost:8080&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Scheduler&lt;/strong&gt;: Responsible for scheduling jobs. Handles both triggering &amp;amp; scheduled workflows, submits Tasks to the executor to run, monitors all tasks and DAGs, and then triggers the task instances once their dependencies are complete.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Worker&lt;/strong&gt;: This component executes the tasks given by the scheduler.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Metadata database (postgres)&lt;/strong&gt;: Backend to the Airflow environment. Used by the scheduler, executor and webserver to store state.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other components (seen in docker-compose services):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;redis&lt;/em&gt;: Message broker that forwards messages from scheduler to worker.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;flower&lt;/em&gt;: The flower app for monitoring the environment. It is available at &lt;a href=&quot;http://localhost:5555&quot;&gt;http://localhost:5555&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;airflow-init&lt;/em&gt;: initialization service (customized as per this design)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please read more about Airflow architecture &lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html#architecture-overview&quot;&gt;here&lt;/a&gt; before continuing the blog post.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Now let's install airflow environment using docker.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;You may need Python version 3.7+.&lt;/p&gt;
&lt;p&gt;You may also need to upgrade your docker-compose version to v2.x+ (as suggested in the course - however airflow documentation suggests v1.29.1 or newer).&lt;/p&gt;
&lt;p&gt;Default amount of memory available for Docker on MacOS is often not enough to get Airflow up and running. If enough memory is not allocated, it might lead to airflow webserver continuously restarting. You should at least allocate 4GB memory for the Docker Engine (ideally 8GB). You can check and change the amount of memory in Resources&lt;/p&gt;
&lt;p&gt;You can also check if you have enough memory by running this command [&lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html&quot;&gt;Airflow docs&lt;/a&gt;]:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker run --rm &lt;span class=&quot;s2&quot;&gt;&amp;quot;debian:buster-slim&amp;quot;&lt;/span&gt; bash -c &lt;span class=&quot;s1&quot;&gt;&amp;#39;numfmt --to iec $(echo $(($(getconf _PHYS_PAGES) * $(getconf PAGE_SIZE))))&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;For me, this is 16 GB:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Unable&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;find&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;debian:buster-slim&amp;#39;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;locally&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;buster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pulling&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;debian&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;6552179&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c3509&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pull&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;complete&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;Digest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sha256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f6e5cbc7eaaa232ae1db675d83eabfffdabeb9054515c15c2fb510da6bc618a7&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Status&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Downloaded&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;newer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;debian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;buster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slim&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;If enough memory is not allocated, it might lead to airflow-webserver continuously restarting. I used &lt;a href=&quot;https://stackoverflow.com/questions/49839028/how-to-upgrade-docker-compose-to-latest-version#:~:text=First%2C%20remove%20the%20old%20version%3A&quot;&gt;this&lt;/a&gt; answer to update mine. For limiting memory, it is easy to do it in mac and windows like &lt;a href=&quot;https://stackoverflow.com/questions/44533319/how-to-assign-more-memory-to-docker-container&quot;&gt;here&lt;/a&gt; and for linux you can check &lt;a href=&quot;https://phoenixnap.com/kb/docker-memory-and-cpu-limit&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To deploy Airflow on Docker Compose, you should fetch &lt;code&gt;docker-compose.yaml&lt;/code&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;curl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LfO&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;https://airflow.apache.org/docs/apache-airflow/2.2.3/docker-compose.yaml&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This file contains several service definitions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;airflow-scheduler - The scheduler monitors all tasks and DAGs, then triggers the task instances once their dependencies are complete. Behind the scenes, the scheduler spins up a subprocess, which monitors and stays in sync with all DAGs in the specified DAG directory. Once per minute, by default, the scheduler collects DAG parsing results and checks whether any active tasks can be triggered [&lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/concepts/scheduler.html&quot;&gt;ref&lt;/a&gt;].&lt;/li&gt;
&lt;li&gt;airflow-webserver - The webserver is available at &lt;a href=&quot;http://localhost:8080&quot;&gt;http://localhost:8080&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;airflow-worker - The worker that executes the tasks given by the scheduler.&lt;/li&gt;
&lt;li&gt;airflow-init - The initialization service.&lt;/li&gt;
&lt;li&gt;flower - The &lt;a href=&quot;https://flower.readthedocs.io/en/latest/&quot;&gt;flower&lt;/a&gt; app is a web based tool for monitoring the environment. It is available at &lt;a href=&quot;http://localhost:5555&quot;&gt;http://localhost:5555&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;postgres - The database.&lt;/li&gt;
&lt;li&gt;redis - The &lt;a href=&quot;https://redis.io/&quot;&gt;redis&lt;/a&gt; - broker that forwards messages from scheduler to worker.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these services allow you to run Airflow with &lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/executor/celery.html&quot;&gt;CeleryExecutor&lt;/a&gt;. For more information, see &lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html&quot;&gt;Architecture Overview&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some directories in the container are mounted, which means that their contents are synchronized between your computer and the container.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;./dags - you can put your DAG files here.&lt;/li&gt;
&lt;li&gt;./logs - contains logs from task execution and scheduler.&lt;/li&gt;
&lt;li&gt;./plugins - you can put your custom &lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/plugins.html&quot;&gt;plugins&lt;/a&gt; here. Airflow has a simple plugin manager built-in that can integrate external features to its core by simply dropping files in your $AIRFLOW_HOME/plugins folder.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is the architecture of &lt;code&gt;docker-compose.yaml&lt;/code&gt; file:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;version: &lt;span class=&quot;s1&quot;&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
x-airflow-common:
  &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt;airflow-common
  &lt;span class=&quot;c1&quot;&gt;# In order to add custom dependencies or upgrade provider packages you can use your extended image.&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# and uncomment the &amp;quot;build&amp;quot; line below, Then run `docker-compose build` to build the images.&lt;/span&gt;
  image: &lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;AIRFLOW_IMAGE_NAME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:-&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;apache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;/airflow:&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.2.3&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# build: .&lt;/span&gt;
  environment:
    &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt;airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: &lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: &lt;span class=&quot;s1&quot;&gt;&amp;#39;true&amp;#39;&lt;/span&gt;
    AIRFLOW__CORE__LOAD_EXAMPLES: &lt;span class=&quot;s1&quot;&gt;&amp;#39;true&amp;#39;&lt;/span&gt;
    AIRFLOW__API__AUTH_BACKEND: &lt;span class=&quot;s1&quot;&gt;&amp;#39;airflow.api.auth.backend.basic_auth&amp;#39;&lt;/span&gt;
    _PIP_ADDITIONAL_REQUIREMENTS: &lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;_PIP_ADDITIONAL_REQUIREMENTS&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:-&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
  user: &lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;AIRFLOW_UID&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:-&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;50000&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:0&amp;quot;&lt;/span&gt;
  depends_on:
    &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt;airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy

services:
  postgres:
    ...

  redis:
    ...

  airflow-webserver:
    ...

  airflow-scheduler:
    ...

  airflow-worker:
    ...

  airflow-triggerer:
    ...

  airflow-init:
    ...

  airflow-cli:
    ...

  flower:
    ...

volumes:
  postgres-db-volume:
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The above file uses the latest Airflow image (&lt;a href=&quot;https://hub.docker.com/r/apache/airflow&quot;&gt;apache/airflow&lt;/a&gt;). If you need to install a new Python library or system library, you can build your image.&lt;/p&gt;
&lt;p&gt;When running Airflow locally, you may wish to use an extended image that includes some additional dependencies - for example, you may wish to add new Python packages or upgrade the airflow providers to a newer version. This is accomplished by including a custom Dockerfile alongside your &lt;code&gt;docker-compose.yaml&lt;/code&gt; file. Then, using the &lt;code&gt;docker-compose build&lt;/code&gt; command, you can create your image (you need to do it only once). Additionally, you can add the &lt;code&gt;--build&lt;/code&gt; flag to your &lt;code&gt;docker-compose&lt;/code&gt; commands to automatically rebuild the images when other &lt;code&gt;docker-compose&lt;/code&gt; commands are run. To learn more and see additional examples, visit &lt;a href=&quot;https://airflow.apache.org/docs/docker-stack/build.html&quot;&gt;here&lt;/a&gt; [&lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html&quot;&gt;Airflow docs&lt;/a&gt;].&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Ingest-Data-to-GCS-and-BigQuery-using-Airflow&quot;&gt;Ingest Data to GCS and BigQuery using Airflow&lt;a class=&quot;anchor-link&quot; href=&quot;#Ingest-Data-to-GCS-and-BigQuery-using-Airflow&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;First, there are some pre-requisites. For the sake of standardization across this tutorial's config, rename your gcp-service-accounts-credentials file to &lt;code&gt;google_credentials.json&lt;/code&gt; and store it in your &lt;code&gt;$HOME&lt;/code&gt; directory:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~ &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; mkdir -p ~/.google/credentials/
mv &amp;lt;path/to/your/service-account-authkeys&amp;gt;.json ~/.google/credentials/google_credentials.json
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In order to use airflow with GCP, we have changed the &lt;code&gt;docker-compose.yaml&lt;/code&gt; file in this course as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;instead of using the official airflow image as the base image, we use a custom docker file to build and start from.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;build:
    context: .
    dockerfile: ./Dockerfile
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;disable loading the DAG examples that ship with Airflow. It’s good to get started, but you probably want to set this to False in a production environment&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AIRFLOW__CORE__LOAD_EXAMPLES&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;false&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;add GCP environment variables (you need to use your own gcp project id and the gcs bucket you created in the previous week)&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;GOOGLE_APPLICATION_CREDENTIALS: /.google/credentials/google_credentials.json
AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: &lt;span class=&quot;s1&quot;&gt;&amp;#39;google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json&amp;#39;&lt;/span&gt;
GCP_PROJECT_ID: &lt;span class=&quot;s1&quot;&gt;&amp;#39;pivotal-surfer-336713&amp;#39;&lt;/span&gt;
GCP_GCS_BUCKET: &lt;span class=&quot;s2&quot;&gt;&amp;quot;dtc_data_lake_pivotal-surfer-336713&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;add the folder we created at the beginning of the post for google credentials.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~/.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;google&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;credentials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;google&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;credentials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ro&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here is the beginning of the file after our modifications:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;build:
    context: .
    dockerfile: ./Dockerfile
  environment:
    &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt;airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: &lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: &lt;span class=&quot;s1&quot;&gt;&amp;#39;true&amp;#39;&lt;/span&gt;
    AIRFLOW__CORE__LOAD_EXAMPLES: &lt;span class=&quot;s1&quot;&gt;&amp;#39;false&amp;#39;&lt;/span&gt;
    AIRFLOW__API__AUTH_BACKEND: &lt;span class=&quot;s1&quot;&gt;&amp;#39;airflow.api.auth.backend.basic_auth&amp;#39;&lt;/span&gt;
    _PIP_ADDITIONAL_REQUIREMENTS: &lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;_PIP_ADDITIONAL_REQUIREMENTS&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:-&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;
    GOOGLE_APPLICATION_CREDENTIALS: /.google/credentials/google_credentials.json
    AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: &lt;span class=&quot;s1&quot;&gt;&amp;#39;google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json&amp;#39;&lt;/span&gt;
    GCP_PROJECT_ID: &lt;span class=&quot;s1&quot;&gt;&amp;#39;pivotal-surfer-336713&amp;#39;&lt;/span&gt;
    GCP_GCS_BUCKET: &lt;span class=&quot;s2&quot;&gt;&amp;quot;dtc_data_lake_pivotal-surfer-336713&amp;quot;&lt;/span&gt;

  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ~/.google/credentials/:/.google/credentials:ro
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Following is the custom Dockerfile whcich is placed inside the &lt;code&gt;airflow&lt;/code&gt; folder.
The Dockerfile has the custom packages to be installed. The one we'll need the most is &lt;code&gt;gcloud&lt;/code&gt; to connect with the GCS bucket/Data Lake.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# First-time build can take upto 10 mins.&lt;/span&gt;

FROM apache/airflow:2.2.3

ENV &lt;span class=&quot;nv&quot;&gt;AIRFLOW_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/airflow

USER root
RUN apt-get update -qq &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get install vim -qqq
&lt;span class=&quot;c1&quot;&gt;# git gcc g++ -qqq&lt;/span&gt;

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt


&lt;span class=&quot;c1&quot;&gt;# Ref: https://airflow.apache.org/docs/docker-stack/recipes.html&lt;/span&gt;

SHELL &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;/bin/bash&amp;quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&amp;quot;-o&amp;quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&amp;quot;pipefail&amp;quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&amp;quot;-e&amp;quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&amp;quot;-u&amp;quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&amp;quot;-x&amp;quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&amp;quot;-c&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;

ARG &lt;span class=&quot;nv&quot;&gt;CLOUD_SDK_VERSION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;322&lt;/span&gt;.0.0
ENV &lt;span class=&quot;nv&quot;&gt;GCLOUD_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/google-cloud-sdk

ENV &lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;GCLOUD_HOME&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/bin/:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;

RUN &lt;span class=&quot;nv&quot;&gt;DOWNLOAD_URL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLOUD_SDK_VERSION&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;-linux-x86_64.tar.gz&amp;quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;TMP_DIR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;mktemp -d&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; curl -fL &lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;DOWNLOAD_URL&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt; --output &lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;TMP_DIR&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/google-cloud-sdk.tar.gz&amp;quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; mkdir -p &lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;GCLOUD_HOME&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; tar xzf &lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;TMP_DIR&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/google-cloud-sdk.tar.gz&amp;quot;&lt;/span&gt; -C &lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;GCLOUD_HOME&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt; --strip-components&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;GCLOUD_HOME&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/install.sh&amp;quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
       --bash-completion&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
       --path-update&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
       --usage-reporting&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
       --quiet &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; rm -rf &lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;TMP_DIR&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; gcloud --version

WORKDIR &lt;span class=&quot;nv&quot;&gt;$AIRFLOW_HOME&lt;/span&gt;

USER &lt;span class=&quot;nv&quot;&gt;$AIRFLOW_UID&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;code&gt;requirements.txt&lt;/code&gt; file in the Dockerfile which contains the required pyton packages is as follows:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;apache-airflow-providers-google
pyarrow
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In case you don't want to see so many services as it is done in the above &lt;code&gt;docker-compose.yaml&lt;/code&gt; file, you can use the following one which is placed in the &lt;code&gt;week_2_data_ingestion/airflow/extras&lt;/code&gt; folder in the course github repo:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;version: &lt;span class=&quot;s1&quot;&gt;&amp;#39;3.7&amp;#39;&lt;/span&gt;
services:
    webserver:
        container_name: airflow
        build:
            context: ..
            dockerfile: ../Dockerfile
        environment:
            - &lt;span class=&quot;nv&quot;&gt;PYTHONPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/airflow
            &lt;span class=&quot;c1&quot;&gt;# airflow connection with SQLAlchemy container&lt;/span&gt;
            - &lt;span class=&quot;nv&quot;&gt;AIRFLOW__CORE__SQL_ALCHEMY_CONN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;sqlite:///&lt;span class=&quot;nv&quot;&gt;$AIRFLOW_HOME&lt;/span&gt;/airflow.db
            - &lt;span class=&quot;nv&quot;&gt;AIRFLOW__CORE__EXECUTOR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;LocalExecutor
            &lt;span class=&quot;c1&quot;&gt;# disable example loading&lt;/span&gt;
            - &lt;span class=&quot;nv&quot;&gt;AIRFLOW__CORE__LOAD_EXAMPLES&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;FALSE

        volumes:
            - ./dags:/home/airflow/dags
        &lt;span class=&quot;c1&quot;&gt;# user: &amp;quot;${AIRFLOW_UID:-50000}:0&amp;quot;&lt;/span&gt;
        ports:
            - &lt;span class=&quot;s2&quot;&gt;&amp;quot;8080:8080&amp;quot;&lt;/span&gt;
        command: &amp;gt;  &lt;span class=&quot;c1&quot;&gt;# airflow db upgrade;&lt;/span&gt;
            bash -c &lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;s2&quot;&gt;                airflow scheduler -D;&lt;/span&gt;
&lt;span class=&quot;s2&quot;&gt;                rm /home/airflow/airflow-scheduler.*;&lt;/span&gt;
&lt;span class=&quot;s2&quot;&gt;                airflow webserver&amp;quot;&lt;/span&gt;
        healthcheck:
            test: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;CMD-SHELL&amp;quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&amp;quot;[ -f /home/airflow/airflow-webserver.pid ]&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
            interval: 30s
            timeout: 30s
            retries: &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We will not use this file to avoid any confusion.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There is also another lightweight and less memory-intensive docker-compose file in the &lt;a href=&quot;https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_2_data_ingestion/airflow&quot;&gt;github repo&lt;/a&gt; which can be used.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Before starting Airflow for the first time, You need to prepare your environment, i.e. create the necessary files, directories and initialize the database.&lt;/p&gt;
&lt;p&gt;On Linux, the quick-start needs to know your host user id and needs to have group id set to &lt;code&gt;0&lt;/code&gt;. Otherwise the files created in &lt;code&gt;dags&lt;/code&gt;, &lt;code&gt;logs&lt;/code&gt; and &lt;code&gt;plugins&lt;/code&gt; will be created with &lt;code&gt;root&lt;/code&gt; user. You have to make sure to configure them for the docker-compose: (run it inside the &lt;code&gt;airflow&lt;/code&gt; folder where the &lt;code&gt;docker-compose.yaml&lt;/code&gt; file is placed) [&lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html&quot;&gt;Airflow docs&lt;/a&gt;]&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mkdir -p ./dags ./logs ./plugins
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; -e &lt;span class=&quot;s2&quot;&gt;&amp;quot;AIRFLOW_UID=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;id -u&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt; &amp;gt; .env
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;For other operating systems, you will get warning that &lt;code&gt;AIRFLOW_UID&lt;/code&gt; is not set, but you can ignore it. You can also manually create the &lt;code&gt;.env&lt;/code&gt; file in the same folder your &lt;code&gt;docker-compose.yaml&lt;/code&gt; is placed with this content to get rid of the warning:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AIRFLOW_UID&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Read more about environment variables for compose &lt;a href=&quot;https://docs.docker.com/compose/environment-variables/&quot;&gt;here&lt;/a&gt;. It seems that when we run &lt;code&gt;docker-compose up&lt;/code&gt;, it looks for &lt;code&gt;.env&lt;/code&gt; file in the same directory and uses the variables in that file.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Then we need to initialize the database. On all operating systems, you need to run database migrations and create the first user account. To do it, run.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker-compose build
docker-compose up airflow-init
docker-compose up
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;You may also some error, but you can ignore them as they are for some services in the official docker compose file that we do not use.&lt;/p&gt;
&lt;p&gt;You can check which services are up using:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker-compose ps
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;For me, the output is as follows:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;s2&quot;&gt;&amp;quot;/usr/bin/dumb-init …&amp;quot;&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;running&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;healthy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;mi&quot;&gt;8080&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tcp&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triggerer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;s2&quot;&gt;&amp;quot;/usr/bin/dumb-init …&amp;quot;&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triggerer&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;running&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;healthy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;mi&quot;&gt;8080&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tcp&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;webserver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;s2&quot;&gt;&amp;quot;/usr/bin/dumb-init …&amp;quot;&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;webserver&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;running&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;healthy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8080&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8080&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tcp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8080&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8080&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tcp&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;worker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;      &lt;span class=&quot;s2&quot;&gt;&amp;quot;/usr/bin/dumb-init …&amp;quot;&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;worker&lt;/span&gt;      &lt;span class=&quot;n&quot;&gt;running&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;healthy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;mi&quot;&gt;8080&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tcp&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flower&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;              &lt;span class=&quot;s2&quot;&gt;&amp;quot;/usr/bin/dumb-init …&amp;quot;&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;flower&lt;/span&gt;              &lt;span class=&quot;n&quot;&gt;running&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;healthy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5555&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5555&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tcp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5555&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5555&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tcp&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;postgres&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;            &lt;span class=&quot;s2&quot;&gt;&amp;quot;docker-entrypoint.s…&amp;quot;&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;postgres&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;running&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;healthy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;mi&quot;&gt;5432&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tcp&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;redis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;               &lt;span class=&quot;s2&quot;&gt;&amp;quot;docker-entrypoint.s…&amp;quot;&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;redis&lt;/span&gt;               &lt;span class=&quot;n&quot;&gt;running&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;healthy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;mi&quot;&gt;6379&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tcp&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There are several ways to interact with it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;by running &lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/usage-cli.html&quot;&gt;CLI commands&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;via a browser using the &lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/ui.html&quot;&gt;web interface&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;using the &lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html&quot;&gt;REST API&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the web interface, you can go to this address: &lt;code&gt;http://0.0.0.0:8080/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The airflow UI will be like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w2/11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The account created has the login &lt;code&gt;airflow&lt;/code&gt; and the password &lt;code&gt;airflow&lt;/code&gt;. After log in you will see two generated dags from the &lt;code&gt;week_2_data_ingestion/airflow/dags&lt;/code&gt; folder.

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/9ksX9REfL8w&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;A Workflow has the following components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;DAG&lt;/code&gt;: Directed acyclic graph, specifies the dependencies between a set of tasks with explicit execution order, and has a beginning as well as an end. (Hence, “acyclic”)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;DAG Structure&lt;/code&gt;: DAG Definition, Tasks (eg. Operators), Task Dependencies (control flow: &lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt; or &lt;code&gt;&amp;lt;&amp;lt;&lt;/code&gt; )&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Task&lt;/code&gt;: a defined unit of work (aka, operators in Airflow). The Tasks themselves describe what to do, be it fetching data, running analysis, triggering other systems, or more.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Common Types: Operators (used in this workshop), Sensors, TaskFlow decorators&lt;/li&gt;
&lt;li&gt;Sub-classes of Airflow's BaseOperator&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;DAG Run&lt;/code&gt;: individual execution/run of a DAG&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;scheduled or triggered&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Task Instance&lt;/code&gt;: an individual run of a single task. Task instances also have an indicative state, which could be “running”, “success”, “failed”, “skipped”, “up for retry”, etc.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ideally, a task should flow from &lt;code&gt;none&lt;/code&gt;, to &lt;code&gt;scheduled&lt;/code&gt;, to &lt;code&gt;queued&lt;/code&gt;, to &lt;code&gt;running&lt;/code&gt;, and finally to &lt;code&gt;success&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let's look at how to use Airflow to ingest data into GCP. To do so, we'll need to create a DAG object. One thing to remember is that this Airflow Python script is really just a configuration file that specifies the DAG's structure as code. The tasks defined here will be executed in a context distinct from that of this script. This script cannot be used to cross-communicate between tasks because different tasks run on different workers at different times. Note that we have a more advanced feature called &lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html&quot;&gt;XComs&lt;/a&gt; that can be used for this purpose [&lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html&quot;&gt;Airflow docs&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;People mistakenly believe that the DAG definition file is where they can do actual data processing - this is not the case! The goal of the script is to create a DAG object. It must evaluate quickly (seconds, not minutes) because the scheduler will run it on a regular basis to reflect any changes [&lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html&quot;&gt;Airflow docs&lt;/a&gt;].&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The structure of a DAG file is as follows:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Imports&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;airflow&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DAG&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;default_args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DAG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;s1&quot;&gt;&amp;#39;tutorial&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;default_args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;default_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;A simple tutorial DAG&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;schedule_interval&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timedelta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;days&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;start_date&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2021&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;catchup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;example&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# t1, t2 and t3 are examples of tasks created by instantiating operators&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BashOperator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;task_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;print_date&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bash_command&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;t3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;## defining task dependencies&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;An Airflow pipeline is just a Python script that happens to define an Airflow DAG object.&lt;/li&gt;
&lt;li&gt;We have the choice to explicitly pass a set of arguments to each task’s constructor (which would become redundant), or (better!) we can define a dictionary of default parameters that we can use when creating tasks.&lt;/li&gt;
&lt;li&gt;We’ll need a DAG object to nest our tasks into. Here we pass a string that defines the dag_id, which serves as a unique identifier for your DAG. We also pass the default argument dictionary that we just defined and define a schedule_interval of 1 day for the DAG.&lt;/li&gt;
&lt;li&gt;Tasks are generated when instantiating operator objects. An object instantiated from an operator is called a task. The first argument task_id acts as a unique identifier for the task.&lt;/li&gt;
&lt;li&gt;Then we need to define dependencies between tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can check more tutorials and examples &lt;a href=&quot;https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now let's see out own DAG file for ingesting &lt;code&gt;yellow_tripdata_2021-01.csv&lt;/code&gt; dataset into GCP which is place in &lt;code&gt;week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py&lt;/code&gt;. First let's check the structure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;importing python libraries &lt;ul&gt;
&lt;li&gt;in-built airflow like &lt;code&gt;BashOperator&lt;/code&gt; and &lt;code&gt;PythonOperator&lt;/code&gt;. There is also &lt;code&gt;DockerOperator&lt;/code&gt; to run docker in docker!&lt;/li&gt;
&lt;li&gt;&lt;code&gt;storage&lt;/code&gt; from google cloud library to interact with GCS.&lt;/li&gt;
&lt;li&gt;BigQuery from google airflow provider to interact with BigQuery and create an external table.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pyarrow&lt;/code&gt; library for converting dataset type to &lt;code&gt;parquet&lt;/code&gt; before uploading it to GCS. &lt;code&gt;parquet&lt;/code&gt; is used more in production and is faster to upload it and also uses less space on GCS.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;setting some variables&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GCP_PROJECT_ID&lt;/code&gt;, &lt;code&gt;GCP_GCS_BUCKET&lt;/code&gt; which we set in the &lt;code&gt;docker-compose.yaml&lt;/code&gt; file under &lt;code&gt;x-airflow-common&lt;/code&gt; section.&lt;/li&gt;
&lt;li&gt;info about dataset url&lt;/li&gt;
&lt;li&gt;airflow local folder path&lt;/li&gt;
&lt;li&gt;name of the desired parquet file&lt;/li&gt;
&lt;li&gt;BigQuery dataset which can be found from &lt;code&gt;variables.tf&lt;/code&gt; from terraform folder of week 1 (in &lt;code&gt;week_1_basics_n_setup/1_terraform_gcp/terraform/variables.tf&lt;/code&gt;). I think the name was &lt;code&gt;BQ_DATASET&lt;/code&gt; there, but the value is the same &lt;code&gt;trips_data_all&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Some python functions which will be attached to &lt;code&gt;PythonOperator&lt;/code&gt;s like &lt;code&gt;format_to_parquet()&lt;/code&gt; and &lt;code&gt;upload_to_gcs()&lt;/code&gt;. Their names describe their functionality.&lt;/li&gt;
&lt;li&gt;Default arguments which will be used in DAG definition. &lt;/li&gt;
&lt;li&gt;Then the DAG declaration with tasks and their dependencies&lt;ul&gt;
&lt;li&gt;&lt;code&gt;download_dataset_task&lt;/code&gt;to download the dataset using a bash command.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;format_to_parquet_task&lt;/code&gt; which call the &lt;code&gt;format_to_parquet()&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;local_to_gcs_task&lt;/code&gt; which call the &lt;code&gt;upload_to_gcs()&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bigquery_external_table_task&lt;/code&gt; to extract schema and create a BigQuery table form the file uploaded to GCS. You can easily run SQL queries on this table.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Then the workflow for direction of tasks: &lt;code&gt;download_dataset_task &amp;gt;&amp;gt; format_to_parquet_task &amp;gt;&amp;gt; local_to_gcs_task &amp;gt;&amp;gt; bigquery_external_table_task&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;logging&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;airflow&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DAG&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;airflow.utils.dates&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;days_ago&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;airflow.operators.bash&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BashOperator&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;airflow.operators.python&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PythonOperator&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;google.cloud&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;storage&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;airflow.providers.google.cloud.operators.bigquery&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BigQueryCreateExternalTableOperator&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyarrow.csv&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pv&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyarrow.parquet&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pq&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;PROJECT_ID&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;GCP_PROJECT_ID&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;BUCKET&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;GCP_GCS_BUCKET&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;yellow_tripdata_2021-01.csv&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset_url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;https://s3.amazonaws.com/nyc-tlc/trip+data/&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset_file&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;path_to_local_home&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;AIRFLOW_HOME&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;/opt/airflow/&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;parquet_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;.csv&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;.parquet&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;BIGQUERY_DATASET&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;BIGQUERY_DATASET&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;trips_data_all&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;format_to_parquet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;endswith&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;.csv&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;logging&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Can only accept source files in CSV format, for the moment&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;.csv&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;.parquet&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# NOTE: takes 20 mins, at an upload speed of 800kbps. Faster if your internet has a better upload speed&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;upload_to_gcs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bucket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;object_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Ref: https://cloud.google.com/storage/docs/uploading-objects#storage-upload-object-python&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    :param bucket: GCS bucket name&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    :param object_name: target path &amp;amp; file-name&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    :param local_file: source path &amp;amp; file-name&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    :return:&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# WORKAROUND to prevent timeout for files &amp;gt; 6 MB on 800 kbps upload speed.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# (Ref: https://github.com/googleapis/python-storage/issues/74)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;storage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_MAX_MULTIPART_SIZE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 5 MB&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;storage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_DEFAULT_CHUNKSIZE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 5 MB&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# End of Workaround&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;storage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bucket&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bucket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bucket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;blob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bucket&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;object_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;blob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;upload_from_filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;local_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;default_args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;owner&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;airflow&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;start_date&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;days_ago&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;depends_on_past&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;retries&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# NOTE: DAG declaration - using a Context Manager (an implicit way)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DAG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dag_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;data_ingestion_gcs_dag&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;schedule_interval&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;@daily&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;default_args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;default_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;catchup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;max_active_runs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;dtc-de&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;download_dataset_task&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BashOperator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;task_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;download_dataset_task&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bash_command&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;curl -sS &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset_url&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; &amp;gt; &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path_to_local_home&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset_file&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;format_to_parquet_task&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PythonOperator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;task_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;format_to_parquet_task&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;python_callable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format_to_parquet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;op_kwargs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s2&quot;&gt;&amp;quot;src_file&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path_to_local_home&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset_file&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# TODO: Homework - research and try XCOM to communicate output values between 2 tasks/operators&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;local_to_gcs_task&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PythonOperator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;task_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;local_to_gcs_task&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;python_callable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;upload_to_gcs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;op_kwargs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s2&quot;&gt;&amp;quot;bucket&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BUCKET&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s2&quot;&gt;&amp;quot;object_name&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;raw/&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parquet_file&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s2&quot;&gt;&amp;quot;local_file&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path_to_local_home&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parquet_file&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;bigquery_external_table_task&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BigQueryCreateExternalTableOperator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;task_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;bigquery_external_table_task&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;table_resource&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s2&quot;&gt;&amp;quot;tableReference&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;s2&quot;&gt;&amp;quot;projectId&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PROJECT_ID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s2&quot;&gt;&amp;quot;datasetId&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BIGQUERY_DATASET&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s2&quot;&gt;&amp;quot;tableId&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;external_table&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
            &lt;span class=&quot;s2&quot;&gt;&amp;quot;externalDataConfiguration&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;s2&quot;&gt;&amp;quot;sourceFormat&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;PARQUET&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s2&quot;&gt;&amp;quot;sourceUris&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;gs://&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BUCKET&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/raw/&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parquet_file&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;download_dataset_task&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;format_to_parquet_task&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local_to_gcs_task&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigquery_external_table_task&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Let's run it. First go to &lt;code&gt;localhost:8080&lt;/code&gt; and use &lt;code&gt;airflow&lt;/code&gt; and &lt;code&gt;airflow&lt;/code&gt; as username and password to log in. Then switch on the &lt;code&gt;data_ingestion_gcs_dag&lt;/code&gt; and click on that to open and be able to see the tree. You can also swith to graph using the toolbar on top of the page.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w2/12.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w2/13.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Then click on the play button on top-right part of the page and select &lt;code&gt;Trigger DAG&lt;/code&gt;. Note that after running &lt;code&gt;docker-compose ps&lt;/code&gt; everything should be in the &lt;code&gt;healthy&lt;/code&gt; mode.&lt;/p&gt;
&lt;p&gt;In case any of the tasks fails, you can check the logs as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w2/2.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;If the tasks are completed successfully, then you will see the uploaded &lt;code&gt;parquet&lt;/code&gt; file in GCS bucket and also the table in BigQuery.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: All the &lt;code&gt;PythonOperator&lt;/code&gt; codes (functions called by that) are executed in airflow-workers (which is a container) and files (datasets) are saved there and not in your local machine. If you use &lt;code&gt;DockerOperator&lt;/code&gt;, you are actually running a docker inside another docker (airflow-worker).&lt;/p&gt;
&lt;p&gt;On finishing your run or to shut down the container/s:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker-compose down
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To stop and delete containers, delete volumes with database data, and download images, run:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker-compose down --volumes --rmi all
or
docker-compose down --volumes --remove-orphans
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Ingest-Data-to-Postgres-using-Airflow&quot;&gt;Ingest Data to Postgres using Airflow&lt;a class=&quot;anchor-link&quot; href=&quot;#Ingest-Data-to-Postgres-using-Airflow&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In order to see another step-by-step tutorial on ingesting data to local postgres using airflow, you can check the following video.

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/s2U8MWJH5xA&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Before, we uploaded data to GCS using the &lt;code&gt;upload_to_gcs&lt;/code&gt; function. But here, a s we want to ingest data into postgres (which we will run using another docker-compose), we need to use another function. All the steps are the same (check the above video) and we just check the DAG file and ingestion script here which is a modified version of what we used in week 1.&lt;/p&gt;
&lt;p&gt;Let's see the &lt;code&gt;week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py&lt;/code&gt; file first:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;airflow&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DAG&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;airflow.operators.bash&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BashOperator&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;airflow.operators.python&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PythonOperator&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;ingest_script&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ingest_callable&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;AIRFLOW_HOME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;AIRFLOW_HOME&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;/opt/airflow/&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;PG_HOST&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;PG_HOST&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;PG_USER&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;PG_USER&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;PG_PASSWORD&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;PG_PASSWORD&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;PG_PORT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;PG_PORT&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;PG_DATABASE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;PG_DATABASE&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;local_workflow&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DAG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;LocalIngestionDag&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;schedule_interval&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;0 6 2 * *&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;start_date&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2021&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;URL_PREFIX&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;https://s3.amazonaws.com/nyc-tlc/trip+data&amp;#39;&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;URL_TEMPLATE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;URL_PREFIX&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;/yellow_tripdata_.csv&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;OUTPUT_FILE_TEMPLATE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AIRFLOW_HOME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;/output_.csv&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;TABLE_NAME_TEMPLATE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;yellow_taxi_&amp;#39;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local_workflow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;wget_task&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BashOperator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;task_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;wget&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bash_command&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;curl -sSL &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;URL_TEMPLATE&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; &amp;gt; &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;OUTPUT_FILE_TEMPLATE&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;ingest_task&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PythonOperator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;task_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;ingest&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;python_callable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ingest_callable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;op_kwargs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PG_USER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PG_PASSWORD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PG_HOST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PG_PORT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PG_DATABASE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;table_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TABLE_NAME_TEMPLATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;csv_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;OUTPUT_FILE_TEMPLATE&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;wget_task&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ingest_task&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;And the ingestion function which is imported in the above script is as follows:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sqlalchemy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_engine&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ingest_callable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;table_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;execution_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;execution_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;postgresql://&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;connection established successfully, instering data...&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;t_start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_iter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csv_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chunksize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_dropoff_datetime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_dropoff_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;if_exists&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;replace&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;if_exists&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;append&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;t_end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;inserted the first chunk, took &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.3f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; second&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t_end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
        &lt;span class=&quot;n&quot;&gt;t_start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;ne&quot;&gt;StopIteration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;completed&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_dropoff_datetime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_dropoff_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;if_exists&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;append&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;t_end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;inserted another chunk, took &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.3f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; second&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t_end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Now we have two &lt;code&gt;docker-compose.yaml&lt;/code&gt; files: one for this week which runs airflow stuff and one for week 1 for postgres and pgadmin. Let's see how we can connect them.&lt;/p&gt;
&lt;p&gt;When we run the &lt;code&gt;docker-compose.yaml&lt;/code&gt; file for airflow, it creates a network called &lt;code&gt;airflow_default&lt;/code&gt; which we will use as an external network to connect the docker-compose for postgres and pgadmin to [&lt;a href=&quot;https://stackoverflow.com/questions/38088279/communication-between-multiple-docker-compose-projects&quot;&gt;ref&lt;/a&gt;].&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# docker-compose.yaml of week 1&lt;/span&gt;
services:
    pgdatabase:
        image: postgres:13
        environment:
            - &lt;span class=&quot;nv&quot;&gt;POSTGRES_USER&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;root
            - &lt;span class=&quot;nv&quot;&gt;POSTGRES_PASSWORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;root
            - &lt;span class=&quot;nv&quot;&gt;POSTGRES_DB&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ny_taxi
        volumes:
            - &lt;span class=&quot;s2&quot;&gt;&amp;quot;./ny_taxi_postgres_data:/var/lib/postgresql/data:rw&amp;quot;&lt;/span&gt;
        ports:
            - &lt;span class=&quot;s2&quot;&gt;&amp;quot;5432:5432&amp;quot;&lt;/span&gt;
        networks:
            - airflow
    pgadmin:
        image: dpage/pgadmin4
        environment:
            - &lt;span class=&quot;nv&quot;&gt;PGADMIN_DEFAULT_EMAIL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;admin@admin.com
            - &lt;span class=&quot;nv&quot;&gt;PGADMIN_DEFAULT_PASSWORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;root
        ports:
            - &lt;span class=&quot;s2&quot;&gt;&amp;quot;8080:80&amp;quot;&lt;/span&gt;

networks:
  airflow:
    external:
      name: airflow_default
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We will have two postgres databases, one for airflow  that stores its own metadata and one for uplading our dataset into.&lt;/p&gt;
&lt;p&gt;First run &lt;code&gt;docker-compose up&lt;/code&gt; in week 1 folder, and then test if you can connect using &lt;code&gt;pgcli -h localhost -p 5432 -U root -d ny_taxi&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;Then we can go to the airflow worker container and see if we can connect to the postgres database that we ran above:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; -it &amp;lt;container id&amp;gt; bash
&lt;span class=&quot;c1&quot;&gt;# then type python to open python &lt;/span&gt;
&amp;gt; &amp;gt; from sqlalchemy import create_engine
&amp;gt;&amp;gt; &lt;span class=&quot;nv&quot;&gt;engine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; create_engine&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;postgresql://root:root@pgdatabase:5432/ny_taxi&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&amp;gt;&amp;gt; engine.connect&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# no error here&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;If you don't see any error by following the above procedure, it shows that you can connect to the postgres from airflow worker container. So calling the ingestion script from the &lt;code&gt;PythonOperator&lt;/code&gt; in the DAG file should work. Now you can run both docker-compose files and go to airflow web interface to see the DAGs running.&lt;/p&gt;
&lt;p&gt;On finishing your run or to shut down the container/s:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker-compose down
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To stop and delete containers, delete volumes with database data, and download images, run:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker-compose down --volumes --rmi all
or
docker-compose down --volumes --remove-orphans
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Transfer-Service-in-GCP&quot;&gt;Transfer Service in GCP&lt;a class=&quot;anchor-link&quot; href=&quot;#Transfer-Service-in-GCP&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Until now we have used airflow DAGs to download dataset and then push it into GCS. We can also use a service from GCP called transfer service to do this task directly from other cloud providers or local storages. &lt;a href=&quot;https://cloud.google.com/storage-transfer/docs&quot;&gt;Storage transfer service&lt;/a&gt; is a secure, low-cost services for transferring data from cloud, like AWS or Azure, or on-premises sources. If you search for the transfer service in GCP, there are two separate services, one for cloud and one for on-premises.&lt;/p&gt;
&lt;p&gt;To activate a job, you can use terraform or the UI in GCP. Check the following video to learn how to do it via GCP.

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/rFOFTfD1uGk&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To learn how to do it via terraform, please check the following video:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/VhmmbqpIzeI&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The reviewed procedure in this post for Airflow is not really suitable for production. Usually a combination of Airflow &amp;amp; Kubernetes &amp;amp; Git can be used. You can check the following resources to learn more. Maybe I write a blog post on that in future.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://medium.com/@olivertosky/deploying-apache-airflow-to-google-kubernetes-engine-a72c7db912ee&quot;&gt;Deploying Apache Airflow to Google Kubernetes Engine&lt;/a&gt;

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/rSkIa0lREUc&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/3VDeKmxHWYA&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/QgzkB1hcq5s&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" /><media:content medium="image" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Data Engineering - Week 1</title><link href="https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/18/data-engineering-w1.html" rel="alternate" type="text/html" title="Data Engineering - Week 1" /><published>2022-01-18T00:00:00-06:00</published><updated>2022-01-18T00:00:00-06:00</updated><id>https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/18/data-engineering-w1</id><content type="html" xml:base="https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/18/data-engineering-w1.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-01-18-data-engineering-w1.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The goal of this course is to build a data pipeline around a dataset like &lt;em&gt;TLC Trip Record Data&lt;/em&gt; which is about pickups and drop offs in New York City.&lt;/p&gt;
&lt;p&gt;Here is the architecture of what we want to do in this course:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w1/arch_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;We will take this data, process it, then upload it into Google cloud storage, and then from there we will upload it to Google BigQuery, and then we will use BigQuery to do Analytics engineering and building transformation using spark and so on. In the other part, we will pretend that this data is coming in real time and we will build a system around processing this data using kafka.&lt;/p&gt;
&lt;p&gt;HEre is the list of technologist will be used in this course:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Google Cloud Platform (GCP): Cloud-based auto-scaling platform by Google&lt;/li&gt;
&lt;li&gt;Google Cloud Storage (GCS): Data Lake&lt;/li&gt;
&lt;li&gt;BigQuery: Data Warehouse&lt;/li&gt;
&lt;li&gt;Terraform: Infrastructure-as-Code (IaC)&lt;/li&gt;
&lt;li&gt;Docker: Containerization&lt;/li&gt;
&lt;li&gt;SQL: Data Analysis &amp;amp; Exploration&lt;/li&gt;
&lt;li&gt;Airflow: Pipeline Orchestration&lt;/li&gt;
&lt;li&gt;DBT: Data Transformation&lt;/li&gt;
&lt;li&gt;Spark: Distributed Processing&lt;/li&gt;
&lt;li&gt;Kafka: Streaming&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The course will be around 10 weeks:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w1/3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Here is the video of this week which will introduce teachers of the course and overview what you can expect from the course in each week:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/bkJZDmreIpA&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;strong&gt;Note&lt;/strong&gt;: You can also find the github repo for the course &lt;a href=&quot;https://github.com/DataTalksClub/data-engineering-zoomcamp&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can also find the playlist of videos of the course &lt;a href=&quot;https://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the first week, we will cover the following topics:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w1/4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Let's get started.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Google-Cloud-Platform&quot;&gt;Google Cloud Platform&lt;a class=&quot;anchor-link&quot; href=&quot;#Google-Cloud-Platform&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;We will use some services from Google Cloud Platform (GCP). Here is a very short introduction of its services:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/18jIzE41fJ4&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;First you need to create a new account &lt;a href=&quot;https://cloud.google.com/&quot;&gt;google cloud&lt;/a&gt;. You can also use your account if you have one, but with a new account, you will get 300 dollars credit for free. Will see more on GCP in next sections.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Docker&quot;&gt;Docker&lt;a class=&quot;anchor-link&quot; href=&quot;#Docker&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;
&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/EYNwNlOrpr0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; is a set of platform as a service products that use OS-level virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries, and configuration files; they can communicate with each other through well-defined channels [wikipedia].&lt;/p&gt;
&lt;p&gt;The main goal is to get data (in csv format for example) and process it and then push it into postgres database:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w1/data-pipeline.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Let's write a Dockerfile and build an image to run a simple python script:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# pipeline.py&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sys&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;day&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# some fancy stuff with pandas&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;job finished successfully for day = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;day&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Dockerfile&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;python&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.9&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#the base image to start from&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;RUN&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#run a command to install python packages&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;WORKDIR&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#change the working directory - it&amp;#39;s like cd command in linux &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;COPY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# copy the file from current folder in the host machine to the working directory&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ENTRYPOINT&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;python&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;pipeline.py&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# run the python pipeline.py command when we use docker run command&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;use the following command to build the image from Dockerfile in the current directory&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;docker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;PostgreSQL&quot;&gt;PostgreSQL&lt;a class=&quot;anchor-link&quot; href=&quot;#PostgreSQL&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;
&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/2JM-ziJt0WI&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;PostgreSQL, also known as Postgres, is a free and open-source relational database management system emphasizing extensibility and SQL compliance [wikipedia].&lt;/p&gt;
&lt;p&gt;Now let's see how we can run a PostgreSQL database with docker and push some data into that.&lt;/p&gt;
&lt;p&gt;Run &lt;em&gt;postgres:13&lt;/em&gt; image database with some environment commands (specified by -e), mapping local folder from host machine to a path in docker container (using -v flag), and on port 5432 which will be used for connecting to the database from outside (our python code for example).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;docker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; \
 &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;POSTGRES_USER&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;root&amp;quot;&lt;/span&gt; \
 &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;POSTGRES_PASSWORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;root&amp;quot;&lt;/span&gt; \
 &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;POSTGRES_DB&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;ny_taxi&amp;quot;&lt;/span&gt; \
 &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny_taxi_postgres_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;postgresql&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; \
 &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5432&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5432&lt;/span&gt; \
 &lt;span class=&quot;n&quot;&gt;postgres&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Download data from &lt;a href=&quot;https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page&quot;&gt;here&lt;/a&gt; and under &lt;code&gt;2021 &amp;gt; January &amp;gt; Yellow Taxi Trip Records&lt;/code&gt;. The file name is &lt;em&gt;yellow_tripdata_2021-01.csv&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Using the following codes you can load and visualize and import data to postgres.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# import libraries&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sqlalchemy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_engine&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# create engine and set the root as postgresql://user:password@host:port/database&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;postgresql://root:root@localhost:5432/ny_taxi&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df_iter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;yellow_tripdata_2021-01.csv&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chunksize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#iterate and read chunks of data and append it to the table&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_dropoff_datetime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_dropoff_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;yellow_taxi_data&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;if_exists&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;append&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Then we need to connect to the postgres database. &lt;code&gt;pgcli&lt;/code&gt; is a python package and a command line interface to quickly look at data and we can use it for connecting to the database and do whatever we want with the data.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;pip install pgcli
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pgcli&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;localhost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5432&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;root&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ny_taxi&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Then using &lt;code&gt;\dt&lt;/code&gt; command, we can list tables of the database.&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;\d yellow_taxi_data&lt;/code&gt; command to see the imported data schema:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+-----------------------+-----------------------------+-----------+&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Column&lt;/span&gt;                &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt;                        &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Modifiers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|-----------------------+-----------------------------+-----------|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;                 &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigint&lt;/span&gt;                      &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VendorID&lt;/span&gt;              &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigint&lt;/span&gt;                      &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timestamp&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;without&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zone&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tpep_dropoff_datetime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timestamp&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;without&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zone&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;passenger_count&lt;/span&gt;       &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigint&lt;/span&gt;                      &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trip_distance&lt;/span&gt;         &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RatecodeID&lt;/span&gt;            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigint&lt;/span&gt;                      &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;store_and_fwd_flag&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;                        &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PULocationID&lt;/span&gt;          &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigint&lt;/span&gt;                      &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DOLocationID&lt;/span&gt;          &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigint&lt;/span&gt;                      &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;payment_type&lt;/span&gt;          &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigint&lt;/span&gt;                      &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fare_amount&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;extra&lt;/span&gt;                 &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mta_tax&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tip_amount&lt;/span&gt;            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tolls_amount&lt;/span&gt;          &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;improvement_surcharge&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_amount&lt;/span&gt;          &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;congestion_surcharge&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We can also write any query on imported tables in the database. For example:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@localhost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny_taxi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_amount&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yellow_taxi_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;                                                                            
&lt;span class=&quot;o&quot;&gt;+---------------------+---------------------+---------+&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;                 &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;                 &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|---------------------+---------------------+---------|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2021&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;02&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;22&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;52&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2008&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;31&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;7661.28&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;+---------------------+---------------------+---------+&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.204&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Now let's write our data ingestion pipeline to download the data and put it into postgres and then dockerize it.

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/B1WwATwf-vY&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

here is the &lt;code&gt;ingest_data.py&lt;/code&gt; file:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# coding: utf-8&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;argparse&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sqlalchemy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_engine&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;password&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;host&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;port&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;port&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;table_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table_name&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;csv_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;output.csv&amp;#39;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;system&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;wget &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; -O &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csv_name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;postgresql://&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;df_iter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csv_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chunksize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_dropoff_datetime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_dropoff_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;if_exists&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;replace&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;if_exists&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;append&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
        &lt;span class=&quot;n&quot;&gt;t_start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_pickup_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_dropoff_datetime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpep_dropoff_datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;if_exists&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;append&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;t_end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;inserted another chunk, took &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.3f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; second&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t_end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;vm&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ArgumentParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Ingest CSV data to Postgres&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;--user&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;user name for postgres&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;--password&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;password for postgres&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;--host&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;host for postgres&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;--port&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;port for postgres&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;--db&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;database name for postgres&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;--table_name&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;name of the table where we will write the results to&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;--url&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;url of the csv file&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;And now we can write a docker file for data ingestion:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;python&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.9&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;RUN&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;apt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wget&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;RUN&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqlalchemy&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;psycopg2&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;WORKDIR&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;COPY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ingest_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ingest_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;ENTRYPOINT&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;python&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;ingest_data.py&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;docker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;taxi_ingest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v001&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;You can check the following video to learn more about how to use jupyter and connect to pgcli:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/3IkfkTwqHx4&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;PgAdmin&quot;&gt;PgAdmin&lt;a class=&quot;anchor-link&quot; href=&quot;#PgAdmin&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;pgAdmin is the leading Open Source management tool, the world’s most advanced Open Source database. The pgAdmin package is a free and open-source graphical user interface (GUI) administration tool for PostgreSQL, which is supported on many computer platforms.
pgAdmin 4 is designed to meet the needs of both novice and experienced Postgres users alike, providing a powerful graphical interface that simplifies the creation, maintenance and use of database objects [wikipedia, pgAdmin documentation]. &lt;a href=&quot;https://www.pgadmin.org/docs/pgadmin4/latest/index.html&quot;&gt;Here&lt;/a&gt; is the documentation.&lt;/p&gt;
&lt;p&gt;Instead of pgcli which was a command line interface, we can use pgAdmin which is GUI-based and more convenient to work with the database.&lt;/p&gt;
&lt;p&gt;We can use a docker image that contains both postgres and pgadmin.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;docker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; \
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PGADMIN_DEFAULT_EMAIL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;admin&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@admin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;com&lt;/span&gt; \
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PGADMIN_DEFAULT_PASSWORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt; \
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8080&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt; \
&lt;span class=&quot;n&quot;&gt;dpage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pgadmin4&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;And then go to this address in your browser: &lt;code&gt;http://localhost:8080/&lt;/code&gt; and use the email and password you used above to log in.&lt;/p&gt;
&lt;p&gt;Then right click on &lt;code&gt;Servers&lt;/code&gt; in the left side of the page and then &lt;code&gt;create &amp;gt; server...&lt;/code&gt;. Then in the &lt;code&gt;General&lt;/code&gt; tab set a name and in the &lt;code&gt;connection&lt;/code&gt; tab use &lt;code&gt;localhost&lt;/code&gt; and &lt;code&gt;root&lt;/code&gt; and &lt;code&gt;root&lt;/code&gt; for host, user, and password. But it doesn't work and it cannot find postgres in its localhost (it cannot see it because there is no connection between these two containers).&lt;/p&gt;
&lt;p&gt;If we want to connect two containers of postgres and pgAdmin to see each other, we need to put them in one network. Then pgAdmin will be able to connect to postgress. We can create a nettwork using &lt;code&gt;docker network create &amp;lt;some-name  for example pg-network&amp;gt;&lt;/code&gt; and then when we want to run each container, we need to tell it that this container needs to be run on this network using &lt;code&gt;--network=pg-network&lt;/code&gt; flag. Also we need to set the &lt;code&gt;--name=&amp;lt;some-name for example pg-database/pgadmin&amp;gt;&lt;/code&gt; for the postgres/pgadmin container so that the pgAdmin/postgres can find it by name.&lt;/p&gt;
&lt;p&gt;Then again log in to pgadmin and use &lt;code&gt;pg-database&lt;/code&gt; name in the &lt;code&gt;Host name/address&lt;/code&gt; in the connection tab of creating server.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;docker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# terminal1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;docker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; \
  &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;POSTGRES_USER&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;root&amp;quot;&lt;/span&gt; \
  &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;POSTGRES_PASSWORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;root&amp;quot;&lt;/span&gt; \
  &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;POSTGRES_DB&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;ny_taxi&amp;quot;&lt;/span&gt; \
  &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny_taxi_postgres_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;postgresql&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; \
  &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5432&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5432&lt;/span&gt; \
  &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt; \
  &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;database&lt;/span&gt; \
  &lt;span class=&quot;n&quot;&gt;postgres&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#terminal2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;docker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; \
  &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PGADMIN_DEFAULT_EMAIL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;admin@admin.com&amp;quot;&lt;/span&gt; \
  &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PGADMIN_DEFAULT_PASSWORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;root&amp;quot;&lt;/span&gt; \
  &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8080&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt; \
  &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt; \
  &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pgadmin&lt;/span&gt; \
  &lt;span class=&quot;n&quot;&gt;dpage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pgadmin4&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Let's run the container that we had for data ingestion on the network to ingest data into postgres. Note that as we have donwloaded the dataset and do not want to download it again (we can if we want by providing the url from the website), we use python to make the address (folder in our local machine) that contains the csv dataset as a server and download the csv from that. For this, we can use the following python command in the folder address (&lt;code&gt;week_1_basics_n_setup/2_docker_sql&lt;/code&gt;:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;URL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;http://http://192.168.43.156:8000/:8000/yellow_tripdata_2021-01.csv&amp;quot;&lt;/span&gt;
&lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;URL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv&amp;quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;docker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; \
  &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt; \
  &lt;span class=&quot;n&quot;&gt;taxi_ingest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v001&lt;/span&gt; \
    &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt; \
    &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt; \
    &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;database&lt;/span&gt; \
    &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5432&lt;/span&gt; \
    &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny_taxi&lt;/span&gt; \
    &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow_taxi_trips&lt;/span&gt; \
    &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;URL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;in real world, instead of using this network (&lt;code&gt;pg-network&lt;/code&gt;) and host (&lt;code&gt;pg-databse&lt;/code&gt;), we will have a database in the cloud and a url to a database that can be used for connection. Also, instead of docker, this data ingestion can be done using kubernetes or airflow. We will see how to do it using airflow, but not kubernetes in the next weeks.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Now let's use docker-compose and put everything in one yaml file instead of creating network and run two containers in two terminals. Install docker-compose from &lt;a href=&quot;https://docs.docker.com/compose/install/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Then we can create a &lt;code&gt;docker-compose.yaml&lt;/code&gt; file:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;services&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pgdatabase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;postgres&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;environment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;POSTGRES_USER&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;POSTGRES_PASSWORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;POSTGRES_DB&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny_taxi&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;./ny_taxi_postgres_data:/var/lib/postgresql/data:rw&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;5432:5432&amp;quot;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pgadmin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dpage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pgadmin4&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;environment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PGADMIN_DEFAULT_EMAIL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;admin&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@admin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;com&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PGADMIN_DEFAULT_PASSWORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;8080:80&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;And then:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;docker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compose&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;up&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Check the following video for more explaination on docker-compose, network, and ports:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/tOr4hTsHOzU&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This will run two dockers on a network and we do not need to create a separate network. Note that the network created automatically by docker compose has changed (&lt;code&gt;2_docker_sql_default&lt;/code&gt;) and also the host name (&lt;code&gt;pgdatabase&lt;/code&gt;). You can find the network name by &lt;code&gt;docker network ls&lt;/code&gt; command.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;URL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;http://192.168.43.156:8000/yellow_tripdata_2021-01.csv&amp;quot;&lt;/span&gt;
&lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;URL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv&amp;quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;docker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; \
  &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_docker_sql_default&lt;/span&gt; \
  &lt;span class=&quot;n&quot;&gt;taxi_ingest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v001&lt;/span&gt; \
    &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt; \
    &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt; \
    &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pgdatabase&lt;/span&gt; \
    &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5432&lt;/span&gt; \
    &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny_taxi&lt;/span&gt; \
    &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yellow_taxi_trips&lt;/span&gt; \
    &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;URL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Then we can go to &lt;code&gt;localhost:8080&lt;/code&gt; and create a server with a name like &lt;code&gt;localDocker&lt;/code&gt; and connections info: host:&lt;code&gt;pgdatabase&lt;/code&gt;, username:&lt;code&gt;root&lt;/code&gt;, password:&lt;code&gt;root&lt;/code&gt;. The table is in &lt;code&gt;databases &amp;gt; ny_taxi &amp;gt; schemas &amp;gt; tables &amp;gt; yellow_taxi_trips&lt;/code&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Terraform&quot;&gt;Terraform&lt;a class=&quot;anchor-link&quot; href=&quot;#Terraform&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;
&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Hajwnmj0xfQ&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/dNkEgO-CExg&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In this section we will go for an introduction to Terraform and how to setup GCP infrastructure using Terraform.&lt;/p&gt;
&lt;p&gt;Terraform is an infrastructure as code (IaC) tool that allows you to build, change, and version infrastructure safely and efficiently. This includes low-level components such as compute instances, storage, and networking, as well as high-level components such as DNS entries, SaaS features, etc. Terraform can manage both existing service providers and custom in-house solutions [&lt;a href=&quot;https://www.terraform.io/intro&quot;&gt;terraform docs&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w1/6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;To deploy infrastructure with Terraform [&lt;a href=&quot;https://learn.hashicorp.com/tutorials/terraform/infrastructure-as-code?in=terraform/gcp-get-started&quot;&gt;terraform docs&lt;/a&gt;]:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scope&lt;/strong&gt; - Identify the infrastructure for your project.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Author&lt;/strong&gt; - Write the configuration for your infrastructure.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Initialize&lt;/strong&gt; - Install the plugins Terraform needs to manage the infrastructure.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plan&lt;/strong&gt; - Preview the changes Terraform will make to match your configuration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Apply&lt;/strong&gt; - Make the planned changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/data-engineering-w1/5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We need to first install Terraform client from &lt;a href=&quot;https://www.terraform.io/downloads&quot;&gt;this link&lt;/a&gt; based on your OS type.&lt;/p&gt;
&lt;p&gt;After setting up your GCP account, you need to create a project (follow the video). Then you need to create a service account from &lt;code&gt;IAM &amp;amp; Admin &amp;gt; Services &amp;gt; Create service account&lt;/code&gt; and then fill the name and other stuff to create it. Then grant &lt;code&gt;Viewer&lt;/code&gt; role to begin with.&lt;/p&gt;
&lt;p&gt;A service account is a special kind of account used by an application or compute workload, such as a Compute Engine virtual machine (VM) instance, rather than a person. Applications use service accounts to make authorized API calls, authorized as either the service account itself, or as Google Workspace or Cloud Identity users through domain-wide delegation.
For example, a service account can be attached to a Compute Engine VM, so that applications running on that VM can authenticate as the service account. In addition, the service account can be granted IAM roles that let it access resources. The service account is used as the identity of the application, and the service account's roles control which resources the application can access.
A service account is identified by its email address, which is unique to the account [&lt;a href=&quot;https://cloud.google.com/iam/docs/service-accounts&quot;&gt;GCP docs&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;After creating the service account, click on three dots in front of it and then &lt;code&gt;manage keys &amp;gt; add key&amp;gt; create a new key &amp;gt; json&lt;/code&gt;. You can download and save the key on your machine.&lt;/p&gt;
&lt;p&gt;Then you need Google SDK which is a CLI tool for you to interact with google cloud services. Cloud SDK is a set of tools that you can use to manage resources and applications hosted on Google Cloud. These tools include the gcloud, gsutil, and bq command-line tools [&lt;a href=&quot;https://cloud.google.com/sdk/docs&quot;&gt;gcp docs&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;You can install the SDK following the instructions &lt;a href=&quot;https://cloud.google.com/sdk/docs/install#deb&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To test if it is installed correctly, you can use the &lt;code&gt;gcloud -v&lt;/code&gt; command:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isaac&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@isaac&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;$&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gcloud&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Google&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Cloud&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SDK&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;369.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2022.01&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2022.01&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bq&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;72&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;core&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2022.01&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gsutil&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.6&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Then set environment variable to point to your downloaded GCP keys:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GOOGLE_APPLICATION_CREDENTIALS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;your&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;account&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;authkeys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Refresh token, and verify authentication&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gcloud&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auth&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;application&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;default&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;login&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Let's now create the infrastructure for our project in GCP:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Google Cloud Storage (GCS): Data Lake&lt;/li&gt;
&lt;li&gt;BigQuery: Data Warehouse&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A data lake is a centralized repository designed to store, process, and secure large amounts of structured, semistructured, and unstructured data. It can store data in its native format and process any variety of it, ignoring size limits [&lt;a href=&quot;https://cloud.google.com/learn/what-is-a-data-lake#section-1&quot;&gt;gcp docs&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;A data lake provides a scalable and secure platform that allows enterprises to: ingest any data from any system at any speed—even if the data comes from on-premises, cloud, or edge-computing systems; store any type or volume of data in full fidelity; process data in real time or batch mode; and analyze data using SQL, Python, R, or any other language, third-party data, or analytics application [&lt;a href=&quot;https://cloud.google.com/learn/what-is-a-data-lake#section-1&quot;&gt;gcp docs&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Today’s enterprises rely on the effective collection, storage, and integration of data from disparate sources for analysis and insights. These data analytics activities have moved to the heart of revenue generation, cost containment, and profit optimization. As such, it’s no surprise that the amounts of data generated and analyzed, as well as the number and types of data sources, have exploded.
Data-driven companies require robust solutions for managing and analyzing large quantities of data across their organizations. These systems must be scalable, reliable, and secure enough for regulated industries, as well as flexible enough to support a wide variety of data types and use cases. The requirements go way beyond the capabilities of any traditional database. That’s where the data warehouse comes in.
BigQuery is the Google Cloud’s modern and serverless data warehousing solution [&lt;a href=&quot;https://cloud.google.com/learn/what-is-a-data-warehouse&quot;&gt;gcp docs&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;A data warehouse is an enterprise system used for the analysis and reporting of structured and semi-structured data from multiple sources, such as point-of-sale transactions, marketing automation, customer relationship management, and more. A data warehouse is suited for ad hoc analysis as well custom reporting. A data warehouse can store both current and historical data in one place and is designed to give a long-range view of data over time, making it a primary component of business intelligence [&lt;a href=&quot;https://cloud.google.com/learn/what-is-a-data-warehouse&quot;&gt;gcp docs&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;More explanation about GCS and BigQuery will come in next lectures.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Now let's add permissions for our service account. From &lt;code&gt;IAM &amp;amp; Admin &amp;gt; IAM&lt;/code&gt; in the &lt;code&gt;Permissions&lt;/code&gt; tab select the permission we created and edit it. In order to give Terraform access to go and create buckets and objects in GCS, we need to add two new roles called &lt;code&gt;storage admin&lt;/code&gt; and &lt;code&gt;storage object admin&lt;/code&gt;. Note that in production you may want to create custom rules to limit the access and not use Admin version which gives full access.
The ideal case would be to create one service account for Terrafrom and assign its permissions, and then a different service for data pipeline with its own permissions.&lt;/p&gt;
&lt;p&gt;In addition to the above two roles, we need to add &lt;code&gt;BigQuery admin&lt;/code&gt; too for BigQuery to be able to interact with GCS.&lt;/p&gt;
&lt;p&gt;We also need to enable APIs. The idea is that when the local environment interacts with the cloud environment, it doesn't interact directly with the resource. These APIs are the enablers of this communication. We need to enable these APIs for our project:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://console.cloud.google.com/apis/library/iam.googleapis.com?project=caramel-aria-318622&quot;&gt;Identity and Access Management (IAM) API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com?project=caramel-aria-318622&quot;&gt;IAM Service Account Credentials API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here is the &lt;code&gt;main.tf&lt;/code&gt; file (check &lt;a href=&quot;https://learn.hashicorp.com/tutorials/terraform/google-cloud-platform-build?in=terraform/gcp-get-started&quot;&gt;here&lt;/a&gt; to learn about different parts and how ro write the config file):&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;terraform&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;required_version&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;&amp;gt;= 1.0&amp;quot;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;local&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Can change from &amp;quot;local&amp;quot; to &amp;quot;gcs&amp;quot; (for google) or &amp;quot;s3&amp;quot; (for aws), if you would like to preserve your tf-state online&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;required_providers&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;google&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;hashicorp/google&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;provider&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;google&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;project&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;project&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;region&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;region&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;credentials&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;credentials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Use this if you do not want to set env-var GOOGLE_APPLICATION_CREDENTIALS&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Data Lake Bucket&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Ref: https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/storage_bucket&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;resource&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;google_storage_bucket&amp;quot;&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;data-lake-bucket&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;          &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;$&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{local.data_lake_bucket}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;_$&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{var.project}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Concatenating DL bucket &amp;amp; Project name for unique naming&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;location&lt;/span&gt;      &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;region&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;# Optional, but recommended settings:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;storage_class&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;storage_class&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;uniform_bucket_level_access&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;true&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;versioning&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;enabled&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;lifecycle_rule&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Delete&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;condition&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;age&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;days&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;force_destroy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;true&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# DWH&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Ref: https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_dataset&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;resource&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;google_bigquery_dataset&amp;quot;&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;dataset&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;dataset_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BQ_DATASET&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;project&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;project&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;location&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;region&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;And the &lt;code&gt;variables.tf&lt;/code&gt; is as follows:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;locals&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;data_lake_bucket&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;dtc_data_lake&amp;quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;variable&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;project&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Your GCP Project ID&amp;quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;variable&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;region&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Region for GCP resources. Choose as per your location: https://cloud.google.com/about/locations&amp;quot;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;default&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;europe-west6&amp;quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;variable&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;storage_class&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Storage class type for your bucket. Check official docs for more info.&amp;quot;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;default&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;STANDARD&amp;quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;variable&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;BQ_DATASET&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;BigQuery Dataset that raw data (from GCS) will be written to&amp;quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;default&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;trips_data_all&amp;quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We can then do the following commands for terraform:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;terraform&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Initializes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;configures&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;installs&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plugins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;providers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;checks&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;an&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;existing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;configuration&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;version&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control&lt;/span&gt; 
&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;terraform&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plan&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Matches&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preview&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slocal&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;changes&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;against&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remote&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proposes&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;an&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Execution&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Plan&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;terraform&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Asks&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;approval&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proposed&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;applies&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;changes&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cloud&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;terraform&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;destroy&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Removes&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;your&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Cloud&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;That's it for this week. The following video shows how to create a VM instance, add ssh key and connect to it, install docker and docker-compose, run pgAdmin and postgres on the VM, how to connect to the database from our local machine, and how to install terraform and do initialization.

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/ae-CV2KfoN0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To refresh your SQL knowledge, check the following videos:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/QEcps_iskgg&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The questions for homework can be found &lt;a href=&quot;https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/homework.md&quot;&gt;here&lt;/a&gt;. The following video shows the solution:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/HxHqH2ARfxM&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;You can also learn more about terraform here:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/SLB_c_ayRMo&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" /><media:content medium="image" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Self-Supervised Monocular Depth Estimation in Autonomous Driving</title><link href="https://kargarisaac.github.io/blog/deep%20learning/autonomous%20driving/jupyter/2022/01/17/monocular-depth-estimation.html" rel="alternate" type="text/html" title="Self-Supervised Monocular Depth Estimation in Autonomous Driving" /><published>2022-01-17T00:00:00-06:00</published><updated>2022-01-17T00:00:00-06:00</updated><id>https://kargarisaac.github.io/blog/deep%20learning/autonomous%20driving/jupyter/2022/01/17/monocular-depth-estimation</id><content type="html" xml:base="https://kargarisaac.github.io/blog/deep%20learning/autonomous%20driving/jupyter/2022/01/17/monocular-depth-estimation.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-01-17-monocular-depth-estimation.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;It is easy for humans to estimate depth in a scene, but what about machines? Typically, robots and self-driving cars use LiDAR sensors to gauge the depth of a scene. However, LiDAR is an expensive sensor that is beyond the reach of many personal vehicles. Robo-Taxis may be reasonable in business models that provide service across a city, but not for personal vehicles. As a result, some companies are using camera-only approaches to infer depth information from monocular images. I will discuss some of the state-of-the-art approaches for monocular depth estimation in this post.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2003.06620.pdf&quot;&gt;Several approaches are usually used for depth estimation&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Geometry-based methods: Geometric constraints are used to recover 3D structures from images. In 3D reconstruction and Simultaneous Localization and Mapping (SLAM) , structure from motion (SfM) is an effective method of estimating 3D structures from a series of 2D image sequences. The accuracy of depth estimation depends heavily on exact feature matching and high-quality image sequences. SfM suffers from monocular scale ambiguity as well. Stereo vision matching is also capable of recovering 3D structures of scenes from two viewpoints. It simulates the way human eyes work by using two cameras, and a cost function is used to calculate disparity maps of images. Due to the calibration of the transformation between the two cameras, the scale factor is incorporated into depth estimation during stereo vision matching.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sensor-based methods: This approach uses sensors such as RGB-D and LiDAR. There are sevaral disadvantages for this method such as cost, power consumption, size of sensor.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deep learning-based methods: The pixel-level depth map can be recovered from a single image in an end-to-end manner based on deep learning. It can be done in supervised, semi-supervised, or self-supervised manner.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post, we just consider the self-supervised methods in which the geometric constraints between frames are regarded as the supervisory signal during the training process. There are several types of self-supervised learning methods for estimating depth using images, such as stereo-based and monocular videos. Using methods based on monocular videos presents its own challenges. Along with estimating depth, the model also requires estimating ego-motion between pairs of temporal images during training. The process involves training a pose estimation network, which takes a finite sequence of frames as input and outputs the corresponding camera transformations. Stereo data, however, make the camera-pose estimation a one-time offline calibration but may introduce occlusion and texture-copy artifacts.&lt;/p&gt;
&lt;p&gt;One of the interesting use cases for depth estimation is to use it as an auxiliary task for end-to-end policy learning. It can lead to better representation learning and help the policy to learn some information about the geometric and the depth of the scene. Other tasks, such as optical flow, semantic segmentation, object detection, motion prediction, etc, can also be used to improve representation learning. For example, the following image shows a model from Wayve.ai, a self-driving car company in the UK working on end-to-end autonomous driving, which tries to use multi-task learning to improve representation learning and driving policy learning.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/1.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://wayve.ai/blog/driving-intelligence-with-end-to-end-deep-learning/&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There are also some works that try to learn multiple tasks jointly. Sometimes there is some information in other related tasks that help the networks to learn better. For example, the following work tries to learn optical flow, motion segmentation, camera motion estimation, and depth estimation together:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/2.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Ranjan_Competitive_Collaboration_Joint_Unsupervised_Learning_of_Depth_Camera_Motion_Optical_CVPR_2019_paper.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Or the following work that tries to learn optical flow and depth together:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/3.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_ECCV_2018/papers/Yuliang_Zou_DF-Net_Unsupervised_Joint_ECCV_2018_paper.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here in this post, we just want to understand how depth estimation works, and then it would be more straightforward to mix it with other tasks. Maybe in the future, I write other blog posts on other techniques and tasks. In the rest of this post, I will review two related papers for self-supervised monocular depth estimation which use monocular videos and stereo images methods. Let’s get started!&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Unsupervised-Learning-of-Depth-and-Ego-Motion-from-Video&quot;&gt;Unsupervised Learning of Depth and Ego-Motion from Video&lt;a class=&quot;anchor-link&quot; href=&quot;#Unsupervised-Learning-of-Depth-and-Ego-Motion-from-Video&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/4.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.07813.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The human brain is very good at detecting ego-motion and 3D structures in scenes. Observing thousands of scenes and forming consistent models of what we see in the past has provided us with a deep, structural understanding of the world. We can apply this knowledge when perceiving a new scene, even from a single monocular image, since we have accumulated millions of observations about the world’s regularities — roads are flat, buildings are straight, cars are supported by roads, etc.&lt;/p&gt;
&lt;p&gt;The self-supervised methods based on the monocular image sequences and the geometric constraints are built on the projection between neighboring frames:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/5.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;where $p_n$ stands for the pixel on image $I_n$, and $p_{n−1}$ refers to the corresponding pixel of $p_n$ on the image $I_{n−1}$. K is the camera intrinsics matrix, which is known. $D_n(p_n)$ denotes the depth value at pixel $p_n$, and $T_{n→n−1}$ represents the spatial transformation between $I_n$ and $I_{n−1}$. Hence, if $D_n(p_n)$ and $T_{n→n−1}$ are known, the correspondence between the pixels on different images ($I_n$ and $I_{n−1}$) are established by the projection function.&lt;/p&gt;
&lt;p&gt;This work tries to estimate the $D$ and $T$ in the above equation by training an end-to-end model in an unsupervised manner to observe sequences of images and to explain its observations by predicting the ego-motion (parameterized as 6-DoF transformation matrices) and the underlying scene structure (parameterized as per-pixel depth maps under a reference view).&lt;/p&gt;
&lt;p&gt;They propose two CNNs in this work that are trained jointly: a single-view depth estimation and a camera pose estimation from unlabeled video sequences.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/6.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.07813.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;They use the view synthesis idea as the supervision signal for depth and pose prediction CNNs: given one input view of a scene, synthesize a new image of the scene seen from a different camera pose. This synthesis process can be implemented in a fully differentiable manner with CNNs as the geometry and pose estimation modules.&lt;/p&gt;
&lt;p&gt;Let’s consider $&amp;lt;I_1, …, I_N&amp;gt;$ as a training image sequence with one of the frames I_t as the target view and the rest as the source views $I_s(1 ≤ s ≤ N, s≠t)$. The view synthesis objective can be formulated as:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/7.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.07813.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;where p indexes over pixel coordinates and $Iˆ_s$ is the source view $I_s$ warped to the target coordinate frame based on a depth image-based rendering module (described in the following), taking the predicted depth $Dˆ_t$, the predicted 4×4 camera transformation matrix $Tˆ_{t→s}$ and the source view I_s as input.&lt;/p&gt;
&lt;p&gt;The differentiable depth image-based renderer reconstructs the target view $I_t$ by sampling pixels from a source view $I_s$ based on the predicted depth map $Dˆ_t$ and the relative pose $Tˆ_{t→s}$.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/8.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.07813.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let $p_t$ denote the homogeneous coordinates of a pixel in the target view, and $K$ denote the camera intrinsics matrix. The $p_t$’s projected coordinates onto the source view $p_s$ (which is a continuous value) can be obtained by:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/9.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.07813.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Then a differentiable bilinear sampling mechanism is used to linearly interpolate the values of the 4-pixel neighbors (top-left, top-right, bottom-left, and bottom-right) of $p_s$ to approximate $I_s(p_s)$.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/10.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.07813.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;where $w^{ij}$ is linearly proportional to the spatial proximity between $p_s$ and $p^{ij}_s$, and $sum(w^{ij})=1$.&lt;/p&gt;
&lt;p&gt;The above view synthesis formulation implicitly assumes 1) the scene is static without moving objects; 2) there is no occlusion/disocclusion between the target view and the source views; 3) the surface is Lambertian so that the photo-consistency error is meaningful. To improve the robustness of the learning pipeline to these factors, an explainability prediction network (jointly and simultaneously with the depth and pose networks) is trained that outputs a per-pixel soft mask $Eˆ_s$ for each target-source pair, indicating the network’s belief in where direct view synthesis will be successfully modeled for each target pixel.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/11.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.07813.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Since there is no direct supervision for $Eˆ_s$, training with the above loss would result in predicting $Eˆ_s$ to be zero. To prevent this, a regularization term $L_{reg}(Eˆ_s)$ is considered that encourages nonzero predictions.&lt;/p&gt;
&lt;p&gt;The other issue is that the gradients are mainly derived from the pixel intensity difference between $I(p_t)$ and the four neighbors of $I(p_s)$, which would inhibit training if the correct $p_s$ (projected using the ground-truth depth and pose) is located in a low-texture region or far from the current estimation. To alleviate this problem, they use an explicit multi-scale and smoothness loss (the $L1$ norm of the second-order gradients for the predicted depth maps) that allows gradients to be derived from larger spatial regions directly.&lt;/p&gt;
&lt;p&gt;The final loss function they used for training is as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/12.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.07813.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;And finally, the results are as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/13.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.07813.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/14.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.07813.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/1.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://github.com/tinghuiz/SfMLearner&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;paper: &lt;a href=&quot;https://arxiv.org/pdf/1704.07813.pdf&quot;&gt;https://arxiv.org/pdf/1704.07813.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;code: &lt;a href=&quot;https://github.com/tinghuiz/SfMLearner&quot;&gt;https://github.com/tinghuiz/SfMLearner&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Presentation at NeurIPS 2017:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/HWu39YkGKvI&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Digging-Into-Self-Supervised-Monocular-Depth-Estimation&quot;&gt;Digging Into Self-Supervised Monocular Depth Estimation&lt;a class=&quot;anchor-link&quot; href=&quot;#Digging-Into-Self-Supervised-Monocular-Depth-Estimation&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;This paper uses both monocular videos and stereo pairs for depth estimation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/15.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1806.01260.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The problem is formulated as the minimization of a photometric reprojection error at training time. The relative pose for each source view $I_t'$, with respect to the target image $I_t$’s pose, is shown as $T_{t→t’}$. Then a dense depth map $D_t$ is predicted in which it minimizes the photometric reprojection error $L_p$:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/16.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1806.01260.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here $p_e$ is a photometric reconstruction error, e.g. the $L1$ distance in pixel space; $proj()$ is the resulting 2D coordinates of the projected depths $D_t$ in $I_t’$ and $&amp;lt;&amp;gt;$ is the sampling operator. Bilinear sampling is used to sample the source images, which is locally sub-differentiable, and $L1$ and SSIM are used to make the photometric error function $p_e$:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/17.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1806.01260.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;where α = 0.85. They also used edge-aware smoothness:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/18.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1806.01260.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;where $d*_t$ is the mean-normalized inverse depth to discourage shrinking of the estimated depth.&lt;/p&gt;
&lt;p&gt;In stereo training, the source image $I_t’$ is the second view in the stereo pair to $I_t$, which has a known relative pose. But for monocular sequences, this pose is not known and a neural network is trained jointly with the depth estimation network to minimize $L_p$ and to do pose estimation, $T_{t→t’}$.&lt;/p&gt;
&lt;p&gt;For monocular training, two frames temporally adjacent to $I_t$ are used as source frames, i.e. $I_t’ ∈ {I_{t−1}, I_{t+1}}$. In mixed training (MS), $I_t’$ includes the temporally adjacent frames and the opposite stereo view.&lt;/p&gt;
&lt;p&gt;The image below shows the overview of the proposed approach and in this paper.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/19.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1806.01260.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;They also propose three architectural and loss innovations which, when combined, lead to large improvements in monocular depth estimation when training with monocular video, stereo pairs, or both:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A novel appearance matching loss to address the problem of occluded pixels that occur when using monocular supervision.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A novel and simple auto-masking approach to ignore pixels where no relative camera motion is observed in monocular training.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A multi-scale appearance matching loss that performs all image sampling at the input resolution, leading to a reduction in depth artifacts&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We just explained the main part of the method. To read more about the details of the above innovations, please read the paper. The final loss function is as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/20.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1806.01260.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;And the results are as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/21.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1806.01260.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/monocular-depth/2.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://github.com/nianticlabs/monodepth2&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;paper: &lt;a href=&quot;https://arxiv.org/pdf/1806.01260.pdf&quot;&gt;https://arxiv.org/pdf/1806.01260.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;code: &lt;a href=&quot;https://github.com/nianticlabs/monodepth2&quot;&gt;https://github.com/nianticlabs/monodepth2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;video:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/sIN1Tp3wIbQ&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" /><media:content medium="image" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Active Learning, Data Selection, Data Auto-Labeling, and Simulation in Autonomous Driving</title><link href="https://kargarisaac.github.io/blog/deep%20learning/autonomous%20driving/jupyter/2021/01/14/Active-Learning,-Data-Selection,-Data-Auto-Labeling,-and-Simulation-in-Autonomous-Driving.html" rel="alternate" type="text/html" title="Active Learning, Data Selection, Data Auto-Labeling, and Simulation in Autonomous Driving" /><published>2021-01-14T00:00:00-06:00</published><updated>2021-01-14T00:00:00-06:00</updated><id>https://kargarisaac.github.io/blog/deep%20learning/autonomous%20driving/jupyter/2021/01/14/Active-Learning,-Data-Selection,-Data-Auto-Labeling,-and-Simulation-in-Autonomous-Driving</id><content type="html" xml:base="https://kargarisaac.github.io/blog/deep%20learning/autonomous%20driving/jupyter/2021/01/14/Active-Learning,-Data-Selection,-Data-Auto-Labeling,-and-Simulation-in-Autonomous-Driving.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-01-14-Active Learning, Data Selection, Data Auto-Labeling, and Simulation in Autonomous Driving.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: This blog post is very long. Use the table of content if you want to jump to a specific part:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Deep Learning is going to be in every module of the Self-Driving Car software stack. These deep learning models use data as their learning material, and the quality of the data they are trained on determines their performance. As a result, choosing the optimum training dataset is just as crucial as designing the model itself.&lt;/p&gt;
&lt;p&gt;Furthermore, there is always a case with autonomous driving where we don't have data and our model doesn't know what to do. As a result, the models must be trained on as much and diverse data as possible. As a consequence, they'd be equipped to deal with unusual situations in the actual world.&lt;/p&gt;
&lt;p&gt;The issue we'd like to look into is that there's a lot of data collected by a fleet of many cars moving around the world, and it's impossible to manually annotate and review all of the data points to find the most useful ones. As a result, we're looking for approaches that assist us to choose or subsample the most informative and valuable data points from live streaming data, such as in self-driving cars, or from unlabeled datasets, and labeling data in an automated and cost-effective manner. In addition, we want to see how we can use simulation environments to generate required data.&lt;/p&gt;
&lt;p&gt;In this blog post, I will walk through the automatic data collection, data selection, data labeling pipelines, and simulation tools of multiple companies, which I gleaned through numerous speeches and blog posts, as well as review some cutting-edge research articles. The auto labeling definition is obvious, labeling data using some trained deep learning models (also called pseudo-labeling), but let's see what do we mean by active learning.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Active-Learning&quot;&gt;Active Learning&lt;a class=&quot;anchor-link&quot; href=&quot;#Active-Learning&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Assume that you have an unlabeled dataset and have a budget (money or time) to spend on the labeling task. How can we select and label the most interesting and informative examples from that dataset in which if we train a model on those, it will give us the best possible performance? &lt;/p&gt;
&lt;p&gt;The data selection procedure is called Active Learning and the idea is as the following diagram:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/al.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=oYUkAvhBNsg&amp;amp;list=PLog3nOPCjKBl8s3Ia4ZtmOEniuYM3pVGQ&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We start with a labeled dataset and use it to train a model. The model is then applied to new unlabeled data points, producing scores based on various metrics for each sample. The data points can then be selected based on the associated scores. After that, a human annotator will annotate these selected samples and add them to the labeled dataset. The model will then be trained on that dataset once more, and the procedure will be repeated until the results are satisfactory. &lt;/p&gt;
&lt;p&gt;Some resources consider the following main categories for active learning techniques: &lt;strong&gt;diversity-based sampling, and uncertainty-based sampling&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/al2.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://medium.com/pytorch/https-medium-com-robert-munro-active-learning-with-pytorch-2f3ee8ebec&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The right column above represents Active Learning: the Known Unknowns that can be addressed with Uncertainty Sampling and the Unknown Unknowns that can be addressed with Diversity Sampling. Some examples of the two mentioned categories can be found &lt;a href=&quot;https://towardsdatascience.com/https-towardsdatascience-com-diversity-sampling-cheatsheet-32619693c304&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://towardsdatascience.com/uncertainty-sampling-cheatsheet-ec57bc067c0b&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here I list some of the techniques I found which can fall into one or multiple mentioned categories. Let's check some of them (mostly from &lt;a href=&quot;https://www.youtube.com/watch?v=oYUkAvhBNsg&amp;amp;list=PLog3nOPCjKBl8s3Ia4ZtmOEniuYM3pVGQ&quot;&gt;this lecture&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Random Sampling&lt;/strong&gt;: One way to select some samples from the dataset is to do random sampling. But there is no guarantee that selected samples are informative enough or maybe they are similar to each other.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.lightly.ai/post/data-selection-for-computer-vision-in-5-steps&quot;&gt;Metadata-based Sampling&lt;/a&gt;&lt;/strong&gt;: The metadata distribution of the dataset refers to parameters that describe the image's content (such as luminance and color channels) or the conditions in which the image was captured (time, temperature, and location). The primary task is to determine what the dataset should ideally represent in these proportions. One sampling technique for data selection is data querying based on metadata information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.lightly.ai/post/data-selection-for-computer-vision-in-5-steps&quot;&gt;Diversity-based sampling&lt;/a&gt;&lt;/strong&gt;: Diversity-based sampling, which is based on the distance between samples in embedding space, is a more comprehensive and efficient method of data selection. Its goal is to select a diverse dataset that covers all possible scenarios, ensuring that the model is trained even on edge or corner cases. By using a diverse training subset, redundant samples and over-represented data that can lead to overfitting are removed, improving model accuracy. It should be noted that some of the other approaches mentioned here may also fall into this or other categories. For example, the following method, which employs an ensemble of models, can be used to assess the diversity of a new data point.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2018/papers/Beluch_The_Power_of_CVPR_2018_paper.pdf&quot;&gt;Ensemble of models&lt;/a&gt;&lt;/strong&gt;: Using an ensemble of trained models and then feeding the data point into all of them and voting on their outputs is one technique to choose a data point. If the models agree on the outcome, it means the data point is similar to the data used to train the model and is hence uninformative. If models disagree on the data, it implies that the models lack sufficient information about the sample, and there is something to be learned from this data point.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/al3.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/4563068&quot;&gt;Entropy&lt;/a&gt;&lt;/strong&gt;: The other approach is to use Entropy, which shows the uncertainty, as a score for selecting samples. For example, for a classification task, the entropy for the distribution over different classes can be calculated. If the probability for one class is much higher than others, the calculated entropy would be low which means the model is certain about that sample. And if the probabilities for all classes or several classes are almost the same, the calculated entropy would be high and this shows the model is uncertain about the data point and it will be selected to be labeled. To summarize, the idea behind uncertainty-based sampling is that algorithms ensure that data, where the model is uncertain, is added to the training set.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ensemble + Entropy&lt;/strong&gt;: It is also possible to combine the two mentioned approaches. First, use an ensemble of models to get outputs (distribution over different classes for classification task for example) and then combine the outputs (take the mean of those distributions for example) and then calculate the entropy for the resulting distribution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2018/papers/Beluch_The_Power_of_CVPR_2018_paper.pdf&quot;&gt;Monte Carlo Dropout&lt;/a&gt;&lt;/strong&gt;: The other technique is to use Monte Carlo Dropout. The idea is to train a model with dropout but does not turn the dropout off in the inference phase. Then feeding the data point into the model with dropout several times. It would be somehow similar to the ensemble idea but just needs to train one neural network. The entropy idea can be used here too similar to the previous case.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.00489.pdf&quot;&gt;Geometric Approach or Core-Set&lt;/a&gt;&lt;/strong&gt;: The idea is to pick a small set of points (blue points) whose neighbors (red points) are within a certain distance of these blue points so that if we train a model on these blue points (selected points), the model's performance will be as close to that of a model trained on the entire dataset as possible (all blue and red data points). They define a loss function to select these data points that you can check the paper for more detail. It is also possible to use clustering algorithms to find clusters and select the centroid of the cluster as the blue point.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/al4.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.00489.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Yoo_Learning_Loss_for_Active_Learning_CVPR_2019_paper.pdf&quot;&gt;Learning the Loss&lt;/a&gt;&lt;/strong&gt;: the idea is to train the model to predict the cross-entropy loss as an output. So during the training phase, we can teach the model to do the loss prediction and use it on unlabeled data points. Then, the predicted loss can be used as a score for each data point to be selected for labeling or not.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/al5.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Yoo_Learning_Loss_for_Active_Learning_CVPR_2019_paper.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=MANuneF0DMw&quot;&gt;Consistency-based sampling&lt;/a&gt;&lt;/strong&gt;: Entropy-based and consistency-based methods are complementary. Entropy-based methods perform well on samples coming from easy classes (classes that the model performs well in general but not for some cases) and not very well on hard classes (classes that the model does not perform well in general). As a result, active learning using uncertainty improves the performance over the classes that the model works already well. But for hard classes, the scores by the uncertainty methods are not reliable. Here is an example from a paper called &quot;Not All Labels Are Equal&quot;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/al6.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;http://128.84.4.18/pdf/2106.11921&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The other problem with uncertainty-based methods is that by focusing only on the samples that the model is uncertain about, the selected samples will be just the hard samples which will have a different distribution than the main data and the task we want the model to perform on.  &lt;/p&gt;
&lt;p&gt;On the other hand, consistency-based methods perform well on samples coming from hard classes. The idea in consistency-based methods is that during training, both labeled and unlabeled data are used for model optimization, with cross-entropy loss encouraging correct predictions for labeled samples and consistency-based loss encouraging consistent outputs between unlabeled samples and their augmentations. During sample selection, the unlabeled samples and their augmentations are evaluated by the model developed during the training stage. The consistency-based metric is used to evaluate their outputs. Samples with low consistency scores are chosen for labeling and sent to the labeled pool. Here is an illustration of the proposed framework at t_th active learning cycle in a paper called &quot;Consistency-based Semi-supervised Active Learning&quot;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/al7.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1910.07153.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Combining uncertainty and consistency methods (e.g. multiplying the scores) can improve the performance over both. The paper called &quot;Not All Labels Are Equal&quot; proposes a unified strategy to choose which samples to manually label and which samples can be automatically labeled. They consider the object detection task in their paper and try to build a more general acquisition function to score data points which is not only based on entropy but also based on consistency. The idea is to use again both labeled and unlabeled data and pass them through the detector. There are two losses in their method: one is the supervised loss for labeled data and the other one is the consistency loss which is for both labeled and unlabeled data. Then they have the scoring function, which will select which samples go to the human annotators, and also which samples are easy samples and need to go for the pseudo labeling. So the valuable time of the annotator will not be spent on easy samples. Here is the diagram of their proposed method:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/al8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;For the consistency loss, they have two terms: one for class inconsistency and one for bounding box inconsistency.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/al9.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=MANuneF0DMw&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There is a lot more to say about active learning, but we'll stop here. Now, let's look at what big companies do in the real world.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;NVIDIA&quot;&gt;NVIDIA&lt;a class=&quot;anchor-link&quot; href=&quot;#NVIDIA&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;NVIDIA proposes to use &lt;a href=&quot;https://burrsettles.com/pub/settles.activelearning.pdf&quot;&gt;pool-based active learning&lt;/a&gt; and an acquisition function based on a disagreement between several trained models (the core of their system is an ensemble of object detectors providing potential bounding boxes and probabilities for each class of interest) to select the frames which are most informative to the model. Here are the steps in their proposed approach:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Train: Train N models initialized with different random parameters on all currently labeled training data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Query: Select examples from the unlabeled pool using the acquisition function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Annotate: Annotate selected data by a human annotator.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Append: Append newly labeled data to training data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Go back to 1.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/nvidia1.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/2004.04699.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;They assume the object detector generates a 2D probability map for each class (bicycle, person, car, etc.). Each position in this map relates to a pixel patch in the input image, and the probability indicates whether an object of that class has a bounding box centered there. This type of output map is commonly found in one-stage object detectors like SSD or YOLO. They use the following scoring functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Entropy&lt;/strong&gt;: They compute the entropy of the Bernoulli random variable at each position in the probability map of a specific class:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/nvidia22.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;where p_c represents probability at position p for class c.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mutual Information (MI)&lt;/strong&gt;: This method makes use of an ensemble E of models to measure disagreement. First, the average probability over all ensemble members for each position p and class c is computed as:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/nvidia2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;where |E| is the cardinality of E. Then, the mutual information is computed as:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/nvidia3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;MI encourages the selection of uncertain samples with high disagreement among ensemble models during the data acquisition process.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Gradient of the output layer (Grad)&lt;/strong&gt;: This function computes the model's uncertainty based on the magnitude of &quot;hallucinated&quot; gradients. The predicted label is specifically assumed to be the ground-truth label. The gradient for this label can then be calculated, and its magnitude can be used as a proxy for uncertainty. Small gradients will be produced by samples with low uncertainty. Then a single score based on the maximum or mean variance of the gradient vectors is assigned to each image.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Bounding boxes with confidence (Det-Ent)&lt;/strong&gt;: The predicted bounding boxes of the detector have an associated probability. This allows computing the uncertainty in the form of entropy for each bounding box.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/nvidia4.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/2004.04699.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To aggregate the scores obtained via the mentioned techniques above, two approaches can be used: taking the maximum or the average. Taking the maximum can lead to outliers (because the probability at a single position determines the final score), whereas taking the average favors images with a high number of objects (since an image with a single high uncertainty object might get a lower score than an image with many low uncertainty objects). For the maximum, the score is defined as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/nvidia5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Then the top N images with the highest scores can be selected to be annotated by human annotators. It is also possible to first calculate a feature vector for each image using for example a CNN network, and then using some techniques using k-means++, Core-set, or sparse-modeling (OMP), which combines diversity and uncertainty, to select samples.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/nvidia6.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/2004.04699.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here is a short video of the performance of their approach:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/4aq13pB9s7c&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;p&gt;In addition to the mentioned active learning approach, NVIDIA has developed another solution to deal with the challenges of data collection and labeling. They discussed this challenge at &lt;a href=&quot;https://www.youtube.com/watch?v=pFUEP5xaskc&quot;&gt;CES2022&lt;/a&gt;, stating that the most interesting data that we are interested in labeling is also the most challenging. Here are some examples of dark, blurry, or hazy scenes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/nvidia7.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=pFUEP5xaskc&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There are also scenes that are difficult to comprehend, such as occluded vehicles or pedestrians. Moreover, there are some scenes not often seen, like construction zones. Due to these issues, autonomous vehicle developers need a mix of real and synthetic data. NVIDIA built &lt;a href=&quot;https://blogs.nvidia.com/blog/2021/11/09/drive-sim-replicator-synthetic-data-generation/&quot;&gt;DRIVE Sim replicator&lt;/a&gt; to deal with all of these labeling challenges. DRIVE Sim is a simulation tool built on &lt;a href=&quot;https://www.nvidia.com/en-us/omniverse/&quot;&gt;Omniverse&lt;/a&gt; that takes advantage of the platform's features. Engineers can generate hard-to-label ground truth data from this replicator and use them to train deep neural networks that make up autonomous vehicle perception systems. As a result of synthetic data, developers have more control over data development, allowing them to tailor it to their specific needs and collect the data they want even before any real-world data collection for those situations. &lt;/p&gt;
&lt;p&gt;As collecting and labeling real-world data is difficult, taking synthetic data and augmenting it with real-world data removes this bottleneck, allowing engineers to take a data-driven approach to develop autonomous driving systems. This improves real-world results and significantly speeds up AV development.&lt;/p&gt;
&lt;p&gt;Another problem is the gap between the simulator world and the real world. The gap may be pixel-level or content-level. Omniverse Replicator is designed to narrow the appearance and content gaps. Read more about the capabilities of this amazing simulator &lt;a href=&quot;https://blogs.nvidia.com/blog/2021/11/09/drive-sim-replicator-synthetic-data-generation/&quot;&gt;here&lt;/a&gt;.

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/gPaFgNEF82Q&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Waymo&quot;&gt;Waymo&lt;a class=&quot;anchor-link&quot; href=&quot;#Waymo&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Waymo uses active learning too, obviously. In &lt;a href=&quot;https://www.youtube.com/watch?v=Q0nGo2-y0xY&amp;amp;t=1325s&quot;&gt;this talk&lt;/a&gt;, Drago Anguelov explains about the ML factory used at Waymo:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waymo1.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Q0nGo2-y0xY&amp;amp;t=1325s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The lifecycle is almost similar to what we saw for NVIDIA. Most of the data come from some common scenarios and does not have enough information for the model to learn. So it is essential to know how to select the data. They have data mining and active learning pipelines to find rare cases and situations where the models are uncertain or inconsistent over time and label those cases. Then this labeled data will go for model training. They also have auto-labels in their system. When you collect data, you also see the future for many objects. This knowledge about the past and the future will help annotate data better, go back to the model that does not know the future, and replicate it with the model.&lt;/p&gt;
&lt;p&gt;Waymo also released the &lt;a href=&quot;https://waymo.com/intl/en_us/dataset-motion/&quot;&gt;Open Motion Dataset&lt;/a&gt; and had a competition at CVPR 2021. The dataset is labeled using a deep learning model in offline mode published in CVPR 2021: &lt;a href=&quot;https://arxiv.org/abs/2103.05073&quot;&gt;Offboard 3D Object Detection from Point Cloud Sequences&lt;/a&gt;. Running the model in offline mode is not limited by latency constraints on the vehicle and also benefits from seeing the future, as it has access to the full scene and can go backward and forward in time. This labeling approach in offline mode can be used to label a lot of data and then train deep learning models on that data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waymo2.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://waymo.com/intl/en_us/dataset-motion/&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The offboard 3D object detection paper presents new techniques for automatically labeling the point clouds created by lidar sensors. Taking advantage of the fact that different image frames capture complementary views of the same object, this team has developed a labeling system that includes multi-frame object detection over time. Here is their 3D auto-labeling pipeline (the pipeline is explained in the image caption):&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waymo3.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/2103.05073.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The 3D Auto Labeling pipeline. Given a point cloud sequence as input, the pipeline first leverages a 3D object detector to localize objects in each frame. Then object boxes at different frames are linked through a multi-object tracker. Object track data (its point clouds at every frame as well as its 3D bounding boxes) are extracted for each object and then go through the object-centric auto labeling (with a divide-and-conquer for static and dynamic tracks) to generate the final &quot;auto labels&quot;, i.e. refined 3D bounding boxes.*&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And here is its performance compared to state-of-the-art:&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waymo4.png&quot; alt=&quot;&quot; /&gt;&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/2103.05073.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Waymo, like NVIDIA, has a &lt;a href=&quot;https://blog.waymo.com/2021/06/SimulationCity.html&quot;&gt;simulator&lt;/a&gt; called SimulationCity. The goal is to gain a better understanding of how the Waymo Driver responds to the full range of behaviors that it will encounter in the real world.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waymo3.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://blog.waymo.com/2021/06/SimulationCity.html&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Assume we simulate a scenario of tailgating at an intersection. To evaluate the Waymo Driver's behavior, we want to understand as many possible outcomes as possible and their likelihood of occurring. If we chose a random tailgating scenario, the tailgater would almost certainly brake in time. However, it is critical to assess how the Waymo Driver behaves when the tailgater fails to brake in time, for example, when the tailgater is distracted or inattentive. As more variations of the same scenario are simulated, we observe a convergence of the distribution of outcomes between what we observe in simulation and the real world. Additionally, SimulationCity enables us to investigate rare events in order to create risky scenarios that the Driver has never encountered before, but that have been proven to be both realistic and extremely useful.&lt;/p&gt;
&lt;p&gt;Having a large and high-quality dataset, as well as the simulation required to generate the required data, is critical for deep learning models and autonomous driving to operate safely and handle rare cases such as the following:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waymo4.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=oJ96bgmSaW0&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This scenario depicts a car passing a red traffic light and entering an intersection when the green light is for the ego car, but the vehicle is capable of handling the situation and yielding to the crazy car before proceeding. This demonstrates that Waymo is performing an excellent job of data collection, data selection, and corner case analysis, as well as simulation, to train their models.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Tesla&quot;&gt;Tesla&lt;a class=&quot;anchor-link&quot; href=&quot;#Tesla&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;In &lt;a href=&quot;https://www.youtube.com/watch?t=7520&amp;amp;v=Ucp0TTmvqOE&amp;amp;feature=youtu.be&quot;&gt;this talk&lt;/a&gt; on Tesla AI Day in 2019, Andrej Karpathy explains the active learning procedure at Tesla, which they call the Data Engine. For example, in an object detection task and for a bike attached to the back of a car, the neural network should detect just one object (car) for downstream tasks such as decision-making and planning. Check the following image:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla1.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Ucp0TTmvqOE&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;They find a few images that show this pattern and use a machine learning mechanism to search for similar examples in their fleet to fix this problem. The returned images from the fleet can be as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla2.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Ucp0TTmvqOE&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Then human annotators will annotate these examples as single cars, and the neural network will be trained on these new examples. So, in the future, the object detector will understand that it is just an attached bike to a car and consider that as just a single car. They do this all the time for all the rare cases. So their model will become more and more accurate over time.&lt;/p&gt;
&lt;p&gt;After collecting some initial data, the models are trained. Then, wherever the model is uncertain, or there is human intervention or disagreement between the human behavior and the model output, which is running in shadow mode, the data will be selected to be annotated by humans, and the model will be trained on that data. For example, if the model for lane line detection does not work very well in tunnels, they will notice a problem in tunnels. So they use the explained mechanism to find similar images, annotate those, and train the model on those.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla3.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Ucp0TTmvqOE&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Andrej also talks about their automated mechanism to do data labeling in addition to expensive human annotators, called Fleet Learning. &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla3.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Ucp0TTmvqOE&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As an example, he talks about automatic cut-in detection. You drive on a highway in a lane, and someone from the left or right lane cuts into your lane. Their fleet is capable of detecting this scenario.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla4.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Ucp0TTmvqOE&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This is not a hard-coded procedure to write rules or codes to identify this event. It is a learning process. They ask the fleet to find scenarios where a car from the left or right lane transitions to the ego lane. Then they go backward in time and automatically annotate that scenario and train neural networks on it. So the neural network will pick up many of these patterns and learn from them. As Andrej says, the neural network may also learn about the left or right blinkers, which show lane change internally from these examples without any hard-coding. After the whole training procedure, they can deploy the model in shadow mode. So, in shadow mode, the network always makes predictions about the events and acts as a trigger to find mispredictions. These false positives and false negatives detected freely by the cut-in network will be added to the training dataset and go into the explained Data Engine and training procedure. After several rounds of the same process for the cut-in network, and when they are happy with the performance, the trained model can be turned on and take control of the car instead of being in shadow mode.&lt;/p&gt;
&lt;p&gt;Here are some of the triggers they use:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla5.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=NSDTZQdo6H8&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;On &lt;a href=&quot;https://www.youtube.com/watch?v=j0z4FweCy4M&amp;amp;t=5072s&quot;&gt;Tesla AI day in 2021&lt;/a&gt;, they discuss the Autopilot software stack in more detail. They also talk about data collection and auto-labeling. Andrej talks about collecting clean and diverse data for training neural networks. Instead of image space, they go for vector space, a 3D representation of everything you need for driving. It is the 3D position of lines, edges, curbs, traffic signs, traffic lights, cars, their positions, orientations, depth, velocity, etc. It gets raw image data and outputs the vector space for the scene. Part of the vector space is shown on the Tesla car screen in the following image:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla6.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=j0z4FweCy4M&amp;amp;t=5288s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Andrej also talks about data labeling in vector space instead of image space. The human annotator labels data in the 3D vector space, and it will be projected into image space automatically as shown in the GIF below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla1.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=j0z4FweCy4M&amp;amp;t=5288s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;But even this improved labeling procedure is not scalable. Active learning and auto-labeling can help here. Here is an example of how they label a clip. It has data from different sensors, such as cameras, IMUs, GPS, etc., and can last from 45 seconds to 1 minute.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla7.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=j0z4FweCy4M&amp;amp;t=5288s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Tesla engineering cars or customer cars can upload the clips to Tesla servers. Following this, many neural networks are run in offline mode to produce intermediate results, such as segmentation masks, depth, and point matching. Then a lot of robotics and AI algorithms are then utilized to create the final set of labels needed to train the neural networks.&lt;/p&gt;
&lt;p&gt;Road surface labeling (segmentation) is one of the tasks they discuss. Typically, we can represent a road surface with splines or meshes, but due to topological restrictions, they are not differentiable and not appropriate for deep learning. Also, segmenting in each image space and for each camera view and then joining them together to represent the scene does not work very well. Instead, they use Neural Radiance Fields or NeRF and an implicit representation to represent the road surface (I LOVED this idea). You can check &lt;a href=&quot;https://www.youtube.com/watch?v=CRlN-cYFxTk&quot;&gt;this&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=Q5g3p9Zwjrk&quot;&gt;this&lt;/a&gt;, and &lt;a href=&quot;https://www.youtube.com/watch?v=HfJpQCBTqZs&quot;&gt;this&lt;/a&gt; to learn more about NeRF.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla8.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=j0z4FweCy4M&amp;amp;t=5288s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Then they can query an (x, y) point on the ground, and the network can predict the height of the ground surface and the semantic class for that point, such as lane line, curb, asphalt, crosswalk, etc. By having the input (x, y) and the predicted height (z), we will have a 3D point that can be projected to all camera views. Making millions of these queries and projecting the resulting 3D point into all camera views can achieve semantic segmentation, such as the top-right picture in the above image. Then the resulting images from all camera views can be compared to the semantic segmentation results from image space and by a joint optimization for all camera views across space and time can produce an excellent reconstruction:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla2.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=j0z4FweCy4M&amp;amp;t=5288s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Each car that drives on some roads does the labeling. By collecting this labeled data from one or many cars in the same area, they can bring them all together and align them using various features such as road edges, lane lines, etc., which all agree with each other and also with the image space observations and can generate the whole scene. This produces an effective way of labeling the road surface. After the labeling process, human annotators can check them and clean up any noise in the data, or maybe add metadata to that to make it even more prosperous.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla3.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=j0z4FweCy4M&amp;amp;t=5288s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;They also talk about more data collection such as static objects, walls, barriers, etc. We don't go into more details here but you got the idea.&lt;/p&gt;
&lt;p&gt;In addition to the auto-labeling procedure, explained above, Tesla has a simulator too and uses it for data generation and labeling. Here is their talk at Tesla AI Day 2021:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/j0z4FweCy4M&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;p&gt;They mention that using their vector space, they can produce the scenes they want very quickly! The simulator can be used in different cases:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla9.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=j0z4FweCy4M&amp;amp;t=5288s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To have such a good simulator, it needs accurate sensor simulation, photorealistic rendering, Diverse actors and locations, scalable scenario generation, real-world scenario reconstruction.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla10.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=j0z4FweCy4M&amp;amp;t=5288s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla11.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=j0z4FweCy4M&amp;amp;t=5288s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla12.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=j0z4FweCy4M&amp;amp;t=5288s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla13.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=j0z4FweCy4M&amp;amp;t=5288s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla14.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=j0z4FweCy4M&amp;amp;t=5288s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/tesla15.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=j0z4FweCy4M&amp;amp;t=5288s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;They also mentioned doing Reinforcement Learning using their simulator, which is my favorite topic and I'm doing my Ph.D. on that.&lt;/p&gt;
&lt;p&gt;I think that's enough for Tesla. Let's go for another company.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Cruise&quot;&gt;Cruise&lt;a class=&quot;anchor-link&quot; href=&quot;#Cruise&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Cruise also makes use of active learning. They refer to it as the Continuous Learning Machine (CLM).&lt;/p&gt;
&lt;p&gt;Consider the task of prediction. The motion prediction model must be capable of handling both the nominal and longtail cases well. Here is the end-to-end motion prediction model which Cruise uses and announced in the &lt;em&gt;Cruise Under the Hood&lt;/em&gt; event recently:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise4.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=uJWN0K26NxQ&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It is critical to note that while these longtail events do occur in the data collected on the road, they are extremely rare and infrequent. As a result, we concentrate on identifying the needle in the haystack of daily driving and use upsampling to teach the models about these events.&lt;/p&gt;
&lt;p&gt;A naive approach to identifying rare events would be to manually engineer &quot;detectors&quot; for each of these longtail situations to assist in data sampling. For instance, we could create a &quot;u-turn&quot; detector that generates sample scenarios whenever it is triggered. This approach would enable us to collect targeted data, but quickly fails when scaling up, as it is impossible to write a detector that is sufficiently specific for each unique longtail situation.&lt;/p&gt;
&lt;p&gt;Self-supervised learning is a viable option for the prediction task at hand. In each scenario, we can compare our model's prediction to the ground truth future trajectory of each car, and if they differ, we can label that scenario and train our model on that. These error situations can be automatically identified and mined. The labeling does not require human annotations and can be done automatically using the logged future trajectory of the car. Following that, these longtail events should be upsampled. The auto-labeled approach ensures maximum coverage of the dataset by identifying and mining all model errors, ensuring that no valuable data is missed. Additionally, it keeps the dataset as lean as possible by ensuring that no additional data for already-solved scenarios is added to the dataset.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise1.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://medium.com/cruise/cruise-continuous-learning-machine-30d60f4c691b&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Drives&lt;/strong&gt;: The CLM starts with the fleet navigating in the city.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Error Mining&lt;/strong&gt;: Active learning is used to automatically identify error cases, and only scenarios with a significant difference between prediction and reality are added to the dataset. This enables highly targeted data mining, ensuring that we add only valuable data and avoid bloating the datasets with easy and uninformative scenarios.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Labeling&lt;/strong&gt;: All of our data is automatically labeled by the self-supervised framework, which uses future perception output as the &lt;em&gt;ground truth&lt;/em&gt; for all prediction scenarios. While the core CLM structure is applicable to other machine learning problems where a human annotator is required, fully automating this step within prediction enables significant scale, cost, and speed improvements, allowing this approach to span the entire longtail.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Model Training and Evaluation&lt;/strong&gt;: The final step is to train a new model, run it to rigorous testing, and finally deploy it to the road. The testing and metrics pipelines ensure that a new model outperforms its predecessor and generalizes well to the nearly infinite variety of scenarios found in the test suites. Cruise has made significant investments in the machine learning infrastructure, which enables the automation of a variety of time-consuming tasks. As a result, they are capable of creating an entire CLM loop without human intervention.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Let's review some examples.&lt;/p&gt;
&lt;p&gt;U-turn is one of the longtail scenarios which happens very rarely. The following image shows different trajectories (red ones) in an intersection starting from the black point.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise2.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://medium.com/cruise/cruise-continuous-learning-machine-30d60f4c691b&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As demonstrated in the image, the majority of the dataset consists of drivers traveling straight with few left turns, even fewer lane changes, and only two U-turn trajectories. Another example of the uncommon mid-block u-turn can be seen below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise3.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://medium.com/cruise/cruise-continuous-learning-machine-30d60f4c691b&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;When CLM principles are applied, the initial deployment of the model may underpredict U-turn situations. As a result, when we sample data, we frequently encounter error situations involving U-turns. Over time, the dataset gradually increases its representation of U-turns until the model is capable of sufficiently predicting them and the AV is capable of accurately navigating these scenarios.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise1.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://medium.com/cruise/cruise-continuous-learning-machine-30d60f4c691b&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;K-turn is the other longtail scenario. The K-turn is a three-point maneuver that requires the driver to move forward and backward in order to complete the turn in the opposite direction. These are uncommon and are most frequently used when the street is too narrow for a U-turn.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise2.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://medium.com/cruise/cruise-continuous-learning-machine-30d60f4c691b&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Cut-in is another rare scenario that we need to be able to predict in order to handle the situation and yield for the car if needed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise3.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://medium.com/cruise/cruise-continuous-learning-machine-30d60f4c691b&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Another kind of interesting scenario is the one with the interaction between agents. For example:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise4.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=uJWN0K26NxQ&amp;amp;t=4660s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Cruise employs an interaction-centric architecture with an agent-to-agent graph and an attention mechanism for detecting agent interaction. For instance, in the previous scenario, the ego car and a bicycle are driving alongside one another, and the parked car wants to slightly come back. The car understands the interaction and anticipates that the cyclist will nudge to the left to avoid it. As a result, the self-driving car slows and yields to the cyclist.&lt;/p&gt;
&lt;p&gt;Additionally, they have an interaction auto-labeler that can determine whether or not a pair of agents interacts. And, if that is the case, who wins the interaction? Then, as additional self-supervision, this interaction auto-labeler can mine scenarios and define auxiliary tasks for interaction detection and resolution.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Not only the future is uncertain, but also the world behind occlusions. Therefore, they designed their AI system to understand which part of the world is occluded and proactively anticipate other agents before even seeing them. For example, when a door pops open, their system can anticipate a pedestrian coming out of the door. So the car slows down immediately and steers further away from it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise5.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=uJWN0K26NxQ&amp;amp;t=4660s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The other example is when a large garbage truck obscures the driver's view; even though the driver cannot see anything behind the truck, the system imagines a pedestrian attempting to cross the street.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise6.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=uJWN0K26NxQ&amp;amp;t=4660s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The other example is that prior to driving through an intersection, even if the system does not see any cross-traffic due to occlusion, the system imagines a car crossing from the right and slowing down; if it does see a car approaching, the autonomous car can stop in time.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise7.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=uJWN0K26NxQ&amp;amp;t=4660s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Thus, regardless of what occurs in the future, the car will always be prepared to make prudent choices. All of this is due to the high-quality data that was used to train the model.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Additionally, they developed a few-shot active learned classifier for the purpose of mining-specific behaviors. For instance, if we want to train a model to predict when bikes will make a u-turn and want to find similar trajectories, a naive similarity search using embedding features would return left-turn scenarios. Because the two behaviors are somewhat similar and left-turning is a much more common occurrence than u-turning. However, with the assistance of human supervision, we can train a classifier with much fewer data and a higher degree of accuracy and return a variety of true positive u-turn examples.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise8.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=uJWN0K26NxQ&amp;amp;t=4660s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Cruise also employs Reinforcement Learning (RL) to develop a safe policy. This application of RL was one of the aspects of the &lt;em&gt;Cruise Under the Hood&lt;/em&gt; event that I enjoyed the most. Take the following example:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise5.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=uJWN0K26NxQ&amp;amp;t=4660s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This scenario is possible on a regular basis, but we may not have it in the dataset. To deal with these situations, they use reinforcement learning to train an offline policy to understand what happens when pedestrians are extremely close to the ego car. They simulate decades of data in order to develop a cautious policy. As illustrated in the image, there are two representations of how a policy appears. Let us begin with the left one. If the pedestrian is in the yellow zone, they are in a very dangerous stage; they can run towards the vehicle or to the side, but if the vehicle is traveling at a high initial velocity, a collision is possible. Thus, the learned and safe policy would be to exercise caution if the pedestrian enters the yellow zone, as we have no idea what they will do.&lt;/p&gt;
&lt;p&gt;Now, let us discuss the right one. They can train policy offline using a simulated latency, in this case 400 milliseconds, and as can be seen in the image, the yellow area is significantly larger and extends to the vehicle's sides. Because of the system's latency, we must exercise more cautiously.&lt;/p&gt;
&lt;p&gt;Additionally, they use simulation to learn policies offline for complex interactions involving multiple actors. For instance, the following video demonstrates two vehicles attempting to park. The same technique can be applied to the learning of a wide variety of behaviors both offline and in a simulator. Additionally, it can be used to generate data that they do not have or that is difficult to obtain.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise9.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=uJWN0K26NxQ&amp;amp;t=4660s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The other reason for using simulation in Cruise, which they refer to as &lt;em&gt;Morpheus&lt;/em&gt;, is for safety. Simulation can be used to practice handling longtails and gradually reduce the reliance on real-world testing. Because longtails occur only once every thousands of road miles, testing the model in those scenarios will take a long time and is not scalable. Cruise has developed a system for exploring large-scale parameter spaces in order to generate test scenarios on a scalable basis. They can begin their simulation by searching for a specific location on the map. The following video demonstrates how they can generate a large and diverse test suit specific to a given situation. Beginning 15 meters before a left turn and then adding a straight road intersection with the turn and maybe adding an unlimited number of other parameters. It's astounding!&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise10.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=uJWN0K26NxQ&amp;amp;t=4660s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The next step is to introduce additional agents into the scene. They accomplish this through the process of converting real-world events into editable simulation scenarios dubbed &lt;em&gt;road to sim&lt;/em&gt;. It combines perception data with heuristics developed over millions of real-world miles to recreate a complete simulation environment from road data. The following video depicts a simulation of a recreated scenario (bottom one) from the real world (top one). Then, while the scenario is running in simulation, we can experiment with various parameters and attributes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise11.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=uJWN0K26NxQ&amp;amp;t=4660s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To increase the realism of the simulation, they use an artificial intelligence system called NPC (non-player character - a term borrowed from video games) AI to simulate the complex multi-agent behaviors of all agents in the scene except the autonomous car. The following are two variations on a single simulated environment in which an NPC is used to provide life to other agents:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise12.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=uJWN0K26NxQ&amp;amp;t=4660s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Along with the previously mentioned technologies, Cruise employs another called World-Gen to expand their business into new cities. It is capable of procedurally generating an entire city, with roads, curbs, sidewalks, lane markings, street lights, traffic signs, buildings, automobiles, and pedestrians. Here is the automatically generated &lt;em&gt;Alamo Square Park in San Francisco&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise13.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=uJWN0K26NxQ&amp;amp;t=4660s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Additionally, they create high-quality simulations of various sensors and use them to generate synthetic data for the perception module and also collect data for instances such as emergency vehicles, which are uncommon and difficult to collect in the real world, and we need to detect them extremely precisely:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/cruise6.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=uJWN0K26NxQ&amp;amp;t=4660s&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, the simulation can be used to evaluate the algorithms' comfort and safety.&lt;/p&gt;
&lt;p&gt;There is a lot of cool stuff they announced at the event. I highly encourage you to watch the event below:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/uJWN0K26NxQ&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Waabi&quot;&gt;Waabi&lt;a class=&quot;anchor-link&quot; href=&quot;#Waabi&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;In a recent &lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;workshop on self-supervised learning for autonomous driving at ICCV 2021&lt;/a&gt;, Raquel Urtasun talked about their labeling mechanisms at Waabi. She mentions that there is no need for humans in the labeling loop and it is possible to make the entire loop automatic.&lt;/p&gt;
&lt;p&gt;Here is the Autonomy workflow used at Waabi:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi1.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We have access to a fleet of vehicles as well as data collection platforms. So while we can collect a large amount of data, labeling it all is prohibitively expensive. Change and evolution of datasets, on the other hand, is necessary and occurs frequently in industry and the real world. However, because the world is changing as we drive to different cities, seeing different scenes and situations, and the city changing due to, for example, constructions, we need to change our datasets and train our models on them in order to be able to handle the situations that we see and cannot handle. Annotating these datasets is costly and the solution for that can be data curation.&lt;/p&gt;
&lt;p&gt;In order to select samples in data to label, there are several techniques. &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi2.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Interesting&quot;&gt;Interesting&lt;a class=&quot;anchor-link&quot; href=&quot;#Interesting&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We can choose a data point that we believe is interesting, exciting, and would be beneficial to learn about, whether for training or testing purposes.&lt;/p&gt;
&lt;p&gt;They have some measures in place to select data from the logs and data collected by each of the vehicles. They accomplish this by using an intermediate process to tag logs with various properties, which they can then use to generate various notions of what might be interesting. They then rank and select the best examples. Automation is critical in this process.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi3.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;While humans are capable of tagging, the process must also be automated. They've devised two distinct methods for automatic tagging. The first is modular tagging, which involves running the perception system in offline mode on data to perform detection, tracking, prediction, and so on, and then determining whether the data is interesting and also determining the scene's complexity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi4.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The other approach is to use a learning-based approach. Basically, you can learn to tag through the use of a sophisticated neural network. Then you can have sophisticated tags about what is happening in the scene and then use those for complexity measures.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi5.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here we focus on the modular tagging approach.&lt;/p&gt;
&lt;p&gt;Each scenario, which is a few seconds of driving, has tags, HDMap, information about the self-driving car, and all other traffic participants. They then calculate complexity measures and combine them to arrive at a single value for the scene's complexity. Then, all of the scenarios will be ranked according to their complexity, and the top ones will be chosen.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi6.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So, how can we say a scenario is complex and interesting for us to be selected? It can be many things: &lt;strong&gt;map complexity measures, actor complexity measures&lt;/strong&gt;, and &lt;strong&gt;Self-Driving Vehicle (SDV) complexity measures&lt;/strong&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;For the map complexity case, scenarios can be selected based on the following items ( In the following images, left is more complex and right is less complex):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Motion paths&lt;/strong&gt;: like high curvature roads or roads with odd shapes can be more interesting and have higher complexity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi7.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lane-crossing&lt;/strong&gt;: for example, an intersection where there are many lanes crossing each other and the car can go from different lanes to other lanes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi8.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Crosswalks&lt;/strong&gt;: Scenarios with more crosswalks that might have more pedestrians can be more complex than others.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi9.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Intersections&lt;/strong&gt;: scenarios with more complex intersections can be interesting too. The left intersection has a more complex topology compared to the right one which is a very common and classic intersection.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi10.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Traffic-controls&lt;/strong&gt;: Intersections with many traffic lights that control many different things are more interesting. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi11.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Map slope&lt;/strong&gt;: scenarios with different slopes of the map and ground can be interesting and you need to make sure that your self-driving car can handle those too.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;For actor complexity measures, the following cases can be considered:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dynamic or static actors&lt;/strong&gt;: scenarios with more actors can be more complex and interesting and our self-driving car needs to be able to handle them.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi12.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Class diversity&lt;/strong&gt;: scenarios with various classes such as bicycles, pedestrians, animals, vehicles, etc., are more complex.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi13.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Spatial diversity&lt;/strong&gt;: scenarios that actors are in different locations in the scene can be interesting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi14.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Velocity&lt;/strong&gt;: scenarios with actors with diverse velocities can be more complex.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi15.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Path&lt;/strong&gt;: variability of the path in the scenario can make it complex too.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi16.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The other case that can be considered for complexity measurement is the SDV complexity measure. The following items can be important in this case:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Path/Velocity diversity&lt;/strong&gt;: it is similar to the previous two items in the actor complexity measures case.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Route&lt;/strong&gt;: route of the SDV can make a scenario more complex. For example, in an intersection, turning is more complex than going straight.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi17.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Interaction with traffic lights&lt;/strong&gt;: scenarios that the SDV needs to deal with traffic lights can be more complex.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi18.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Interaction with actors&lt;/strong&gt;: we can also look at how do we interact with other actors. Is somebody cutting in front of us? Is somebody slowing down behind us? Is somebody entering our line? These are pretty interesting scenarios that we need to make sure our car can handle. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi19.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;For more details on the mentioned measures, check the paper &lt;a href=&quot;https://arxiv.org/pdf/2101.06554.pdf&quot;&gt;&quot;Diverse Complexity Measures for Dataset Curation in Self-driving&quot;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After selecting interesting and complex scenarios based on the mentioned measures, we can label them. What is important to note is that depending on what is your goal and what are you interested in, for example, object detection or motion forecasting, or motion planning, certain things would be more interesting than others. The way to handle this would be very simple. We can compute the weighted sum of these complexity measures in order to project this high-dimensional vector into a single number. &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi20.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In summary, the complexity of each scenario based on each one of the above-mentioned items will be calculated, and based on the task, the weight vector will be selected and multiplied by these complexity measures to calculate a single number for that scenario.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Diversity/Coverage&quot;&gt;Diversity/Coverage&lt;a class=&quot;anchor-link&quot; href=&quot;#Diversity/Coverage&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In addition to the scenarios that are interesting, we should consider the dataset's diversity and coverage. At the end of the day, we are attempting to develop vehicles capable of operating within our operational domain, and we must ensure that they are trained to handle all possible situations.&lt;/p&gt;
&lt;p&gt;One way to handle this is that as soon as you have decided which scenarios you want to label so far, you can look at what do they miss. What are areas in your space that your current selected scenarios do not cover and the new scenario is far from them? It is also important to add geographical diversity here. Then, we can iteratively select examples that are far away from our selected scenarios and label them. We repeat this procedure until the selected dataset has enough diversity and there is no new data point far enough from the selected points.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi21.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Model-Improvement&quot;&gt;Model Improvement&lt;a class=&quot;anchor-link&quot; href=&quot;#Model-Improvement&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The other factor for selecting scenarios is model improvement. We can select data points that aid the model's performance improvement. So, the scenarios will be selected based on their expected improvement in the model performance or how much the model is uncertain about that scenario. Human intervention on the road can also be a notion of model failure in that scenario and shows that the scenario can be used to learn something from it. Basically, the active learning techniques that we mentioned at the beginning f this post can be used here. &lt;/p&gt;
&lt;p&gt;One point that needs to be taken into account is that, in addition to prioritizing some scenarios over others, we can think of more important regions in a scene. Some regions are more important than others and as we pay annotators per each click, it is important to select and label parts of the scene that are informative and reach and not all the scene. For example, in the following scene, the cars in the middle of the image are more important than the parked cars on the left road. &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi22.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here is the flowchart they propose in their paper called &lt;a href=&quot;https://arxiv.org/pdf/2104.03956.pdf&quot;&gt;&quot;Just Label What You Need: Fine-Grained Active Selection for Perception and Prediction through Partially Labeled Scenes&quot;&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi23.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.03956.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Scoring the unlabeled examples can be based on the mentioned scores such as Entropy. And here is their proposed algorithm:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi24.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.03956.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;After selecting scenarios based on their interestingness, diversity/coverage, and model improvement we need to label them. &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi25.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The good news is that we don't need to do this auto-labeling in online mode. So we can use more sophisticated models in offline mode without worrying about latency and other online inference bottlenecks. We also have access to past and future timesteps which can help in the annotation. &lt;/p&gt;
&lt;p&gt;For example, for the task of labeling a trajectory of an object with its bounding boxes, first, we can run the offline model to give us the first estimate of where are all actors in the scene as well as how they are moving, and then we are going to have sophisticated ways of correcting these results to have better trajectories. Here is the method they propose in the paper called &lt;a href=&quot;https://arxiv.org/pdf/2101.06586.pdf&quot;&gt;&quot;Auto4D: Learning to Label 4D Objects from Sequential Point Clouds&quot;&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi26.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://arxiv.org/pdf/2101.06586.pdf&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There are two branches in this approach. The first is to get the initially estimated trajectory and fix the size of the object. Then the second branch gets the trajectory with fixed size and also the point cloud of the object across time frames and fuses them and outputs a refined trajectory. Here is the result with very nice and smooth trajectories:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi1.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/MANuneF0DMw?t=6945&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;As with previous companies we've reviewed, Waabi makes extensive use of simulation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi27.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=PSZ2Px9PrHg&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Training is conducted using a 3D, real-world, high-fidelity simulation, which enables training on uncommon scenarios and eliminates the need for field data collection. In other words, there is no reason to drive &quot;millions of miles&quot; and create potentially dangerous situations or even collisions.&lt;/p&gt;
&lt;p&gt;Furthermore, having a fleet of hundreds of vehicles on the road is prohibitively expensive, and it can be dangerous. Rather than that, Waabi employs an AI approach that is capable of learning from fewer examples and scaling.&lt;/p&gt;
&lt;p&gt;Rather than requiring humans to design 3D assets by hand and engineers to implement rule-based simulation behavior, Waabi generates virtual worlds and multi-agent systems automatically based on observed human driving behavior.&lt;/p&gt;
&lt;p&gt;Here are the components of their closed-loop simulation system:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi28.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=PSZ2Px9PrHg&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The first component is the &lt;em&gt;Virtual World Creation&lt;/em&gt;. Generating the background, cars, pedestrians and animating them are done in this component.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi2.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=PSZ2Px9PrHg&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The next component is &lt;em&gt;Scenario Generation&lt;/em&gt; to automatically generate realistic and diverse scenarios:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi3.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=PSZ2Px9PrHg&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here are some generated super realistic scenarios in simulation:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi4.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=PSZ2Px9PrHg&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi5.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=PSZ2Px9PrHg&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Additionally, Waabi models sensor noise using AI and physics, resulting in perception outputs that behave similarly in both simulation and the real world. The following videos show the simulated LiDAR and Camera sensor data in comparison to real data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi6.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=PSZ2Px9PrHg&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi8.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=PSZ2Px9PrHg&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Some objects in the above video are fake and generated to make the scene more complex!&lt;/p&gt;
&lt;p&gt;If you want to know more details about their LiDAR simulation, check their &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2020/papers/Manivasagam_LiDARsim_Realistic_LiDAR_Simulation_by_Leveraging_the_Real_World_CVPR_2020_paper.pdf&quot;&gt;LiDARsim paper&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Then they use the simulator to test simple scenarios or those that occur frequently, as well as those that occur infrequently. Also, they can create safety critical cases and test their models there.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/waabi7.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=PSZ2Px9PrHg&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;That's it for Waabi! Let's go for the last company.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Aurora&quot;&gt;Aurora&lt;a class=&quot;anchor-link&quot; href=&quot;#Aurora&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Aurora, on the other hand, takes a slightly different approach. Rather than blindly pushing for increased mileage, they've maintained a focus on collecting high-quality real-world data and extracting the maximum value from each data point. For instance, they amplify the impact of real-world experience by identifying interesting or novel events and incorporating them into their &lt;a href=&quot;https://aurora.tech/blog/virtual-testing-the-invisible-accelerator&quot;&gt;Virtual Testing Suite&lt;/a&gt;, where they are used to continuously improve the Aurora driver. &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/aurora1.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://youtu.be/qeFcm7kfnWY?t=116&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Aurora DriverHowever, not all real-world events are amenable to simulation in virtual environments. For instance, the exhaust of a vehicle may be of interest to an object detection system. Thus, using real-world data from such scenes to train the perception system to recognize and ignore exhaust can result in a more enjoyable driving experience.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/aurora2.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://aurora.tech/blog/online-to-offline&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The on-road events that they  turn into virtual tests come from two sources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Copilot annotations&lt;/strong&gt;: Vehicle operator copilots, who provide support from the passenger's seat, routinely flag experiences that are interesting, uncommon, or novel. They frequently recreate these in their Virtual Testing Suite to familiarize the Aurora Driver with a variety of road conditions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Disengagements&lt;/strong&gt;: when their vehicle operators proactively retook control when they suspected an unsafe situation was about to occur or when they disliked the way the vehicle was driving.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Their Virtual Testing Suite is a complementary collection of tests that evaluate the software's functionality at every level. As a result, they transform real-world events into one or more of the following virtual tests (read more here):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Perception tests&lt;/strong&gt;: Consider the following scenario: a bicyclist passes one of their vehicles. Specialists review the event's log footage and then label items such as the object category (bicyclist), the velocity (3 mph), and so on. They can then use this &quot;absolute truth&quot; to assess the ability of new versions of perception software to accurately determine what occurred on the road. Here is an example of the labeling procedure:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/aurora3.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://aurora.tech/blog/virtual-testing-the-invisible-accelerator&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Manual driving evaluations&lt;/strong&gt;: They compare the planned trajectory of the Aurora Driver to the actual trajectory of the vehicle operator and test whether their motion planning software can accurately forecast what a trained, expert driver would do in complex situations: (comparing a vehicle operator's trajectory (blue) to the intended trajectory of the Aurora Driver (green) during a right turn).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/aurora4.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://aurora.tech/blog/virtual-testing-the-invisible-accelerator&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Simulations&lt;/strong&gt;: Simulations are virtual representations of the real world in which the Aurora Driver can be tested in numerous permutations of the same situation. Additionally, simulations enable them to simulate a wide variety of interactions between the Aurora Driver and other virtual world actors. For instance, how will a jaywalker react when the Aurora Driver comes to a halt? And then, how do the simulation's other actors react when the jaywalker crosses the street?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/aurora5.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://aurora.tech/blog/virtual-testing-the-invisible-accelerator&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;By utilizing their online-to-offline pipeline, they convert on-road events into virtual tests:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/aurora1.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://aurora.tech/blog/online-to-offline&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let's review this process by an example of a disengagement that helped the Aurora Driver learn how to nudge (when the Aurora Driver adjusts its trajectory to move around obstacles).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;On-Road Event&lt;/strong&gt;: Vehicle operators annotate disengagements and highlight scenes that are unusual, novel, or interesting. The Aurora Driver hesitates in this situation to nudge around a vehicle that abruptly veers off the roadway and into an on-street parking space. To avoid causing traffic disruptions, the trained vehicle operators quickly take control and drive around the parked vehicle.

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/jemB4LFol-k&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Triage&lt;/strong&gt;: The triage team conducts an examination of on-road incidents and provides an initial diagnosis. For instance, &lt;em&gt;AV must nudge a vehicle to pull over and come to a complete stop. #motionplanning&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Create virtual tests&lt;/strong&gt;: They develop one or more simulated tests, which may include perception tests, manual driving evaluations, and/or simulations. They used it as the basis for 50 new nudging simulations, which included a recreation of the exact scene from the disengagement log footage and variations created by altering variables such as the speed of the vehicle in front of the ego car.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/copied_from_nb/images/activelearning/aurora6.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://aurora.tech/blog/online-to-offline&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Iterate&lt;/strong&gt;: The diverse Virtual Testing Suite enables engineers to fine-tune new and existing capabilities. The engineering team fine-tuned the Aurora Driver's ability to nudge using the nudging simulations inspired by this disengagement, as well as numerous other codebase tests (unit and regression), perception tests, and manual driving evaluations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Test on the road&lt;/strong&gt;: They put enhancements to the test in the real world and continue to collect useful data. Here is the Aurora Driver nudging gracefully through a complex situation:

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/L3mKw-RQyGw&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Conclusion&quot;&gt;Conclusion&lt;a class=&quot;anchor-link&quot; href=&quot;#Conclusion&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;We investigated a number of active learning techniques and the approaches taken by various companies to collect, select, label, and simulate useful and informative data and scenarios to train their algorithms to handle a variety of real-world situations.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" /><media:content medium="image" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Model Based Reinforcement Learning (MBRL)</title><link href="https://kargarisaac.github.io/blog/reinforcement%20learning/mbrl/jupyter/2020/10/26/mbrl.html" rel="alternate" type="text/html" title="Model Based Reinforcement Learning (MBRL)" /><published>2020-10-26T00:00:00-05:00</published><updated>2020-10-26T00:00:00-05:00</updated><id>https://kargarisaac.github.io/blog/reinforcement%20learning/mbrl/jupyter/2020/10/26/mbrl</id><content type="html" xml:base="https://kargarisaac.github.io/blog/reinforcement%20learning/mbrl/jupyter/2020/10/26/mbrl.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-10-26-mbrl.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This post is a summary (almost!) of the model-based RL tutorial at ICML-2020 by &lt;a href=&quot;https://twitter.com/IMordatch&quot;&gt;Igor Mordatch&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/jhamrick&quot;&gt;Jess Hamrick&lt;/a&gt;. You can find the videos &lt;a href=&quot;https://sites.google.com/view/mbrl-tutorial&quot;&gt;here&lt;/a&gt;. The pictures are from the slides in the talk.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Introduction-and-Motivation&quot;&gt;Introduction and Motivation&lt;a class=&quot;anchor-link&quot; href=&quot;#Introduction-and-Motivation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Having access to a world model, and using it for decision-making is a powerful idea. 
There are a lot of applications of MBRL in different areas like robotics (manipulation- what will happen by doing an action), 
self-driving cars (having a model of other agents decisions and future motions and act accordingly),
games (AlphaGo- search over different possibilities), Science ( chemical use-cases),
and operation research and energy applications (allocate renewable energy at different points in time to meet the demand).&lt;/p&gt;
&lt;h2 id=&quot;Problem-Statement&quot;&gt;Problem Statement&lt;a class=&quot;anchor-link&quot; href=&quot;#Problem-Statement&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In sequential decision making, the agent will interact with the world by doing action $a$ and getting the next state $s$ and reward $r$.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/rl.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We can write this problem as a Markov Decision Process (MDP) as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;States $S \epsilon R^{d_S}$&lt;/li&gt;
&lt;li&gt;Actions $A \epsilon R^{d_A}$&lt;/li&gt;
&lt;li&gt;Reward function $R: S \times A \rightarrow R$&lt;/li&gt;
&lt;li&gt;Transition function $T: S \times A \rightarrow S$&lt;/li&gt;
&lt;li&gt;Discount $\gamma \epsilon (0,1)$&lt;/li&gt;
&lt;li&gt;Policy $\pi: S \rightarrow A$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The goal is to find a policy which maximizes the sum of discounted future rewards:
$$
\text{argmax}_{\pi} \sum_{t=0}^\infty \gamma^t R(s_t, a_t)
$$
subject to
$$
a_t = \pi(s_t) , s_{t+1}=T(s_t, a_t)
$$&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;How to solve this optimization problem?!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Collect data $D= \{ s_t, a_t, r_{t+1}, s_{t+1} \}_{t=0}^T$.&lt;/li&gt;
&lt;li&gt;Model-free: learn policy directly from data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
$$ D \rightarrow \pi \quad \text{e.g. Q-learning, policy gradient}$$
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model-based: learn model, then use it to &lt;strong&gt;learn&lt;/strong&gt; or &lt;strong&gt;improve&lt;/strong&gt; a policy &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
$$ D \rightarrow f \rightarrow \pi$$
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-is-a-model?&quot;&gt;What is a model?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-a-model?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;a model is a representation that explicitly encodes knowledge about the structure of the environment and task.&lt;/p&gt;
&lt;p&gt;This model can take a lot of different forms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A transition/dynamic model: $s_{t+1} = f_s(s_t, a_t)$&lt;/li&gt;
&lt;li&gt;A model of rewards: $r_{t+1} = f_r(s_t, a_t)$&lt;/li&gt;
&lt;li&gt;An inverse transition/dynamics model (which tells you what is the action to take and go from one state to the next state): $a_t = f_s^{-1}(s_t, s_{t+1})$&lt;/li&gt;
&lt;li&gt;A model of distance of two states: $d_{ij} = f_d(s_i, s_j)$&lt;/li&gt;
&lt;li&gt;A model of future returns: $G_t = Q(s_t, a_t)$ or $G_t = V(s_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Typically when someone says MBRL, he/she means the firs two items.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/model.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Sometimes we know the ground truth dynamics and rewards. Might as well use them! Like game environments or simulators like Mujoco, Carla, and so on.&lt;/p&gt;
&lt;p&gt;But we don't have access to the model in all cases, so we need to learn the model. In cases like in robots, complex physical dynamics, and interaction with humans.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;How-to-use-a-model?&quot;&gt;How to use a model?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-use-a-model?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In model-free RL agent, we have a policy and learning algorithm like the figure below:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/rl2.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In model-based RL we can use the model in three different ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;simulating the environment: replacing the environment with a model and use it to generate data and use it to update the policy.&lt;/li&gt;
&lt;li&gt;Assisting the learning algorithm: modify the learning algorithm to use the model to interpret the data it is getting differently. &lt;/li&gt;
&lt;li&gt;Strengthening the policy: allow the agent at test time to use the model to try out different actions before it commits to one of them (taking action in the real world).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbrl.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In general, to compare model-free and model-based:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbrl_vs_mfrl.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;How-to-learn-a-model?&quot;&gt;How to learn a model?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-learn-a-model?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Two different dimensions are useful to pay attention to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;representation of the features for the states that the model is being learned over them&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;representation of the transition between states&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To continue, we take a look at different transition models.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;state-transition-models&quot;&gt;state-transition models&lt;a class=&quot;anchor-link&quot; href=&quot;#state-transition-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We know equations of motion and dynamics in some cases, but we don't know the exact parameters like mass. We can use system identification to estimate unknown parameters like mass. But these sorts of cases require having a lot of domain knowledge about how exactly the system works.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model2.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model3.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In some cases that we don't know the dynamics of motion, we can use an MLP to get a concatenation of $s_t, a_t$, and output the next state $s_{t+1}$.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model4.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In cases that we have some, not perfect, domain knowledge about the environment, we can use graph neural networks (GNNs) to model the agent (robot). For example, in Mujoco, we can model a robot (agent) with nodes as its body parts and edges as joint and learn the physics engine.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model5.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;observation-transition-models&quot;&gt;observation-transition models&lt;a class=&quot;anchor-link&quot; href=&quot;#observation-transition-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In these cases, we don't have access to states (low-level states like joint angles), but we have access to images. The MDP for these cases would be like this:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model6.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;So what can we do with this?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Directly predict transitions between observations (observation-transition models)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model7.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reconstruct observation at every timestep: Using sth like LSTMs. Here we need to reconstruct the whole observation in each timestep. The images can be blurry in these cases.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model8.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model88.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;latent-state-transition-models&quot;&gt;latent state-transition models&lt;a class=&quot;anchor-link&quot; href=&quot;#latent-state-transition-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Another option when we have just access to observation is to instead of making transition between observations we can infere a latent state and then make transitions in that latent space (latent state-transition models) not in the observation space. It would be much faster than reconstructing the observation on every timestep. We take our initial observation or perhaps the last couple of observations and embed them into the latent state and then unroll it in time and do predictions in $z$ instead of $o$.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model9.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Usually we use the observation and reconstruct it during training but at test time we can unroll it very quickly. we can also reconstruct observation at each timestep we want (not necessarily in all timesteps).&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model10.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Structured-latent-state-transition-models&quot;&gt;Structured latent state-transition models&lt;a class=&quot;anchor-link&quot; href=&quot;#Structured-latent-state-transition-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Another thing that you can do if you have a little bit more domain knowledge is to add a little bit of structure into your latent state. For example, if you know that the scene that you are trying to model consists of objects, you can try to actually explicitly detect those objects, segment them out and then learn those transitions between objects.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model11.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Recurrent-value-models&quot;&gt;Recurrent value models&lt;a class=&quot;anchor-link&quot; href=&quot;#Recurrent-value-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The idea is that when you unroll your latent-state, you additionally predict the value of the state at each point of the future, in addition to reward. We can train the model without necessarily needing to train using observations, but just training it by predicting the value progressing toward actual observed values when you roll it out in the real environment.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model12.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Why is this useful? Because some types of planners only need you to predict values rather than predicting states like MCTS (Monte Carlo tree search).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Non-Parametric-models&quot;&gt;Non-Parametric models&lt;a class=&quot;anchor-link&quot; href=&quot;#Non-Parametric-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;So far, we talked about parametric ways of learning the model. We can also use non-parametric methods like graphs.&lt;/p&gt;
&lt;p&gt;For example, the replay buffer that we use in off-policy methods can be seen as an approximation to a type of model, where if you have enough data in your replay buffer, you can sample from the buffer and basically access the density model over your transitions. You can use extra replay to get the same level performances you would get using a model-based method that learns a parametric model.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model13.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We can also use data in the buffer to use data points and learn the transition between them and interpolate to find states between those states in the buffer. Somehow learning distribution and use it to generate new data points.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model14.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Another form of non-parametric transition is a symbolic description popular in the planning community, not in the deep learning community.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model15.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The other form of non-parametric models is gaussian processes, which give us strong predictions using a very small amount of data. PILCO is one example of these algorithms.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/learn_model16.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Model-based-control-and-how-to-use-a-model?&quot;&gt;Model-based control and how to use a model?&lt;a class=&quot;anchor-link&quot; href=&quot;#Model-based-control-and-how-to-use-a-model?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We will be using this landscape of various methods and categories that exist, including some representative algorithms:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc1.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;As we saw earlier, we can use the model in three different ways. In continue, we will see some examples of each case.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Simulating-the-environment&quot;&gt;Simulating the environment&lt;a class=&quot;anchor-link&quot; href=&quot;#Simulating-the-environment&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc2.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;One way is to mix the real data with model-generated experience and then apply traditional model-free algorithms like Q-learning, policy gradient, etc. In these cases, the model offers a larger and augmented training dataset.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dyna-Q&lt;/strong&gt; is an example that uses Q-learning with a learned model. Dyna does the traditional Q-learning updates on real transitions and uses a model to create fictitious imaginary transitions from the real states and perform exactly the same Q-learning updates on those. So it's basically just a way to augment the experience.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc3.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This can also be applied to policy learning. We don't need to perform just a single step but multiple steps according to the &lt;strong&gt;model&lt;/strong&gt; to generate experience even further away from the real data and do policy parameter updates entirely on these fictitious experiences.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc4.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Assisting-the-learning-algorithm&quot;&gt;Assisting the learning algorithm&lt;a class=&quot;anchor-link&quot; href=&quot;#Assisting-the-learning-algorithm&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc5.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;One important way that this can be done is to allow end-to-end training through our models. End-to-end training has recently been very successful in improving and simplifying supervised learning methods in computer vision, NLP, etc.&lt;/p&gt;
&lt;p&gt;The question is, &quot;can we apply the same type of end-to-end approaches to RL?&quot;&lt;/p&gt;
&lt;p&gt;One example is just the policy gradient algorithm. Let's say we want to maximize the sum of the discounted future reward of some parametric policy. We can write the objective function with respect to the policy parameters $\theta$&lt;/p&gt;
$$
 J(\theta) = \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t)  , \quad a_t = \pi_{\theta}(s_t) , \quad s_{t+1} = T(s_t, a_t)
$$&lt;p&gt;Now we need to apply gradient ascent (for maximization) on policy gradient with respect to policy parameters $\theta  \rightarrow  \nabla_{\theta}J$.&lt;/p&gt;
&lt;p&gt;So how can we calculate this $\nabla_{\theta}J$ ?&lt;/p&gt;
&lt;p&gt;Sampling-based methods have been proposed, like REINFORCE, to estimate this gradient. But the problem with them is that they can have very high variance and often require the policy to have some randomness to make decisions. This can be unfavorable.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc6.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The next solution is when we have accurate and smooth models. Accurate and smooth models, aside from imaginary experiences, offer derivatives:&lt;/p&gt;
$$
s_{t+1} = f_s(s_t, a_t) \quad  r_t = f_r(s_t, a_t)
$$$$
\nabla_{s_t}(s_{t+1}), \quad \nabla_{a_t}(s_{t+1}), \quad \nabla_{s_t}(r_t), \quad \nabla_{a_t}(r_t), \quad ...
$$&lt;p&gt;And they are able to answer questions such as: &lt;em&gt;how do small changes in action change next state or reward any of other quantities?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc7.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Why is this useful? This is useful because it will allow us to do this type of end-to-end differentiation algorithms like &lt;strong&gt;back-propagation&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let's rewrite our objective function using models:&lt;/p&gt;
$$
 J(\theta) \approx \sum_{t=0}^{H} \gamma^t r_t  , \quad a_t = \pi_{\theta}(s_t) , \quad s_{t+1} = f_s(s_t, a_t), \quad r_t=f_r(s_t,a_t)
$$&lt;p&gt;So how can we use these derivatives to calculate $\nabla_{\theta}J$ ?&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc8.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The highlighted derivatives are easy to calculate using some libraries like PyTorch or TensorFlow.&lt;/p&gt;
&lt;p&gt;By calculating $\nabla_{\theta}J$ in this way:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;pros&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The policy gradient that we get is actually a deterministic quantity, and there is no variance to it. &lt;/li&gt;
&lt;li&gt;It can support potentially much longer-term credit assignment&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;cons&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is prone to local minima&lt;/li&gt;
&lt;li&gt;Poor conditioning (vanishing/exploding gradients)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are two examples to use model-based back-propagation (derivatives) either along real or model-generated trajectories to do end to end training:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc9.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;real trajectories are safer but need to be from the current policy parameters (so it’s less sample-efficient)&lt;/li&gt;
&lt;li&gt;model-generated trajectories allow larger policy changes without interacting with the real world but might suffer more from model inaccuracies&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Strengthening-the-policy&quot;&gt;Strengthening the policy&lt;a class=&quot;anchor-link&quot; href=&quot;#Strengthening-the-policy&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;So far, we talked about the first two ways of using a model in RL. These two ways are in the category of &lt;strong&gt;Background Planning&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;There is another category based on the &lt;em&gt;Sutton and Barto (2018)- Reinforcement Learning: An Introduction&lt;/em&gt; categorization, called &lt;strong&gt;Decision-Time Planning&lt;/strong&gt;, which is a unique option we have available in model-based settings.&lt;/p&gt;
&lt;h4 id=&quot;What-is-the-difference-between-background-and-decision-time-planning?&quot;&gt;What is the difference between background and decision-time planning?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-the-difference-between-background-and-decision-time-planning?&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In background planning, we can think of it as answering the question, &quot;how do I learn how to act in any possible situation to succeed and reach the goal?&quot;&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc10.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The optimization variables are parameters of a policy or value function or ..., and are trained using expectation over all possible situations.&lt;/li&gt;
&lt;li&gt;Conceptually, we can think of background planning as learning a set of habits that we could reuse.&lt;/li&gt;
&lt;li&gt;We can think of background planning as learning a fast type of thinking.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In decision-time planning, we want to answer the question, &quot;what is the best sequence of actions just for my current situation to succeed or reach the goal?&quot;&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc11.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The optimization parameters are just a sequence of actions or states.&lt;/li&gt;
&lt;li&gt;Conceptually, we can think of decision-time planning as finding our consciously improvising just for the particular situation that we find ourselves in.&lt;/li&gt;
&lt;li&gt;We can think of decision-time planning as learning a slow type of thinking.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Why use one over the other?&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc12.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Act on the most recent state of the world&lt;/em&gt;: decision-time planning is just concerned about the current state in finding the sequence of actions. You can act based on the most recent state of the world. By contrast, in background planning, the habits may be stale and might take a while to get updated as the world's changes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Act without any learning&lt;/em&gt;: decision-time planning allows us to act without any learning at all. There is no need for policy or value networks before we can start making decisions. It is just an optimization problem as long as you have the model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Competent in unfamiliar situations&lt;/em&gt;: if you find yourself in situations that are far away from where you were training, your set of habits or policy network might not have the competence (the ability to do something successfully or efficiently) there. So you don't have any information to act or are very uncertain, or even in the worst case, it will with confidence make decisions that just potentially make no sense. This is out of distribution and generalization problem. In these cases, decision-time planning would be more beneficial.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Independent of observation space&lt;/em&gt;: another advantage of decision-time planning is that it is also independent of the observation space that you decide on. In background methods, we need to consider some encoding or description of the state, joint angles, or pixels or graphs into our policy function. These decisions may play a large role in the total learning performance. When something is not working, you will not really know that is it because of the algorithm or state-space, which doesn't contain enough information. In contrast, decision-time planning avoids this confounded, which in practice can actually be quite useful when you're prototyping new methods.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc13.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Partial observability&lt;/em&gt;: decision-time plannings have some issues with it. They assume that you know the full state of the world when you're making the plan. So it's hard to hide information from decision-time planners. It is possible, but it is more costly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Fast computation at deployment&lt;/em&gt;: decision-time planners require more computation. It is not just evaluating a habit, but it needs more thinking.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Predictability and coherence&lt;/em&gt;: decision-time planners do some actions which are not necessarily predictable or coherent. Because you are consciously thinking about each footstep, you might not have exactly the same plan. So you may have a very chaotic behavior that still succeeds. In contrast, background planning, because it learns a set of habits, it can perform a very regular behavior.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Same for discrete and continuous actions&lt;/em&gt;: background planning has a very unified treatment of discrete and continuous actions, which is conceptually simpler. In decision-time planning, there are different algorithms for discrete and continuous actions. We will see in the following sections more about them.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also mix and match the background and decision-time plannings.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;What-is-the-difference-between-discrete-and-continuous-planning?&quot;&gt;What is the difference between discrete and continuous planning?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-the-difference-between-discrete-and-continuous-planning?&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;It depends on the problem which you want to solve. So it is not a choice that you can make. For example, in controlling a robot, the actions might be the torques for the motors (continuous), or in biomechanical settings, it might be muscle excitations (continuous), or in medical problems, the treatment that should be applied (discrete).&lt;/p&gt;
&lt;p&gt;The distinction between discrete and continuous actions is not significant for background planning methods.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You just learn stochastic policies that sample either from discrete or continuous distributions.&lt;/li&gt;
&lt;/ul&gt;
$$
a \sim \pi(.|s) \quad \leftarrow Gaussian, categorical, ...
$$&lt;ul&gt;
&lt;li&gt;Backpropagation is still possible via some reparametrization techniques. See &lt;em&gt;Jang et al (2016). Categorical reparametrization with Gumbel-Softmax&lt;/em&gt; for an example.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In either of these cases (continuous and discrete in background planning methods), your final objective and optimization problem is still smooth wrt the policy parameters because you are optimizing over expectations.&lt;/p&gt;
$$
J(\theta) = E_{\pi}[\sum_t r_t], \quad a_t \sim \pi(.|s_t, \theta)
$$&lt;p&gt;But for decision-time planning, this distinction leads to specialized methods for discrete and continuous actions: discrete search or continuous trajectory optimization.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Let's see some examples to be able to compare them.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc14.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;MCTS-(monte-carlo-tree-search)&quot;&gt;MCTS (monte carlo tree search)&lt;a class=&quot;anchor-link&quot; href=&quot;#MCTS-(monte-carlo-tree-search)&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;This algorithm is in a discrete action group and is used in alpha-go and alpha-zero. You keep track of Q-value, which is long term reward, for all states and actions that you want to consider. And also the number of times that the state and action have been previously visited.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize $Q_0(s, a) = 0, N_0(s, a)=0, k=0$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc15.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Expansion: Starting from the current situation and expand nodes and selecting actions according to a search policy: &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;
$$\pi_k(s) = Q_k(s,a)$$
&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc16.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Evaluation: When a new node is reached, estimate its long-term value using Monte-Carlo rollouts&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc17.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Backup: Propagate the Q-values to parent nodes:&lt;/li&gt;
&lt;/ol&gt;
$$
Q_{k+1}(s, a) = \frac{Q_k(s,a) N_k(s,a) + R}{N_k(s,a)+1}
$$$$
N_{k+1}(s,a) = N_k(s,a)+1
$$&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc18.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Repeat Steps 2-4 until the search budget is exhausted.
$$
k = k + 1
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc19.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Trajectory-Optimization&quot;&gt;Trajectory Optimization&lt;a class=&quot;anchor-link&quot; href=&quot;#Trajectory-Optimization&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;Instead of keeping track of a tree of many possibilities, you keep track of one possible action sequence.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize $a_0, ..., a_H$ from guess&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc20.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Expansion&lt;/strong&gt;: execute sequence of actions $a = a_0, ..., a_H$ to get a sequence of states $s_1, ..., s_H$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc21.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: get trajectory reward $J(a) = \sum_{t=0}^H r_t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Back-propagation&lt;/strong&gt;: because everything is differentiable, you can just calculate the gradient of the reward via back-propagation using reward model derivatives and transition model derivatives.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
$$
\nabla_a J = \sum_{t=0}^H \nabla_a r_t
$$$$
\nabla_a r_t = \nabla_s f_r(s_t, a_t) \nabla_a s_t + \nabla_a f_r (s_t, a_t)
$$$$
\nabla_a s_t = \nabla_a f_s(s_{t-1}, a_{t-1}) + \nabla_s f_s(s_{t-1}, a_{t-1})\nabla_a s_{t-1}
$$$$
\nabla_a s_{t-1} = ...
$$&lt;ol&gt;
&lt;li&gt;Update all actions via gradient ascent $ a \leftarrow a + \nabla_a J$ and repeat steps 2-5.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc22.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The differences between discrete and continuous actions can be summarized as follows:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc23.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The continuous example we saw above can be categorized in &lt;strong&gt;shooting methods&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&quot;Variety-and-motivations-of-continuous-planning-methods&quot;&gt;Variety and motivations of continuous planning methods&lt;a class=&quot;anchor-link&quot; href=&quot;#Variety-and-motivations-of-continuous-planning-methods&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Why so many variations? They all try to mitigate the issues we looked at like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sensitivity and poor conditioning&lt;/li&gt;
&lt;li&gt;Only reaches local optimum&lt;/li&gt;
&lt;li&gt;Slow convergence&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Addressing each leads to a different class of methods.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc24.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Sensitivity-and-poor-conditioning&quot;&gt;Sensitivity and poor conditioning&lt;a class=&quot;anchor-link&quot; href=&quot;#Sensitivity-and-poor-conditioning&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc24-2.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shooting methods&lt;/strong&gt; that we have seen have this particular issue that small changes in early actions lead to very large changes downstream.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc25.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;By expanding the objective function, this can be understood more clearly.&lt;/p&gt;
$$
\max_{a_0,...,a_H} \sum_{t=0}^H r(s_t, a_t), \quad s_{t+1} = f(s_t, a_t)
$$$$
\sum_{t=0}^H r(s_t, a_t) = r(s_0, a_0) + r(f(s_0, a_0), a_1)+...+r(f(f(...),...), a_H)
$$&lt;p&gt;It means that each state implicitly is dependent on all actions that came before it. This is similar to the exploding/vanishing gradient problem in RNNs that hurts long-term credit assignment. But unlike the RNN training, we cannot change the transition function because it is dictated to us by the environment.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc26.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To address this problem, &lt;strong&gt;Collocation&lt;/strong&gt; is introduced, which is optimizing for states and/or actions &lt;em&gt;directly&lt;/em&gt;, instead of actions only. So we have a different set of parameters that we are optimizing over.&lt;/p&gt;
$$
\max_{s_0,a_0,...,s_H,a_H} \sum_{t=0}^H r(s_t, a_t), \quad ||s_{t+1} - f(s_t, a_t) || = 0 \leftarrow \text{explicit optimization constraint}
$$&lt;p&gt;It is an explicit constrained optimization problem, rather than just beeng satisfied by construction as in shooting methods.&lt;/p&gt;
&lt;p&gt;As a result, you only have pairwise dependencies between variables, unlike the dense activity graph in the previous figure for shooting methods.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc27.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;These methods have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Good conditioning: changing $s_0, a_0$ has a similar effect as changing $s_H, a_H$.&lt;/li&gt;
&lt;li&gt;Larger but easier to optimize search space. It is useful for contact-rich problems such as some robotics applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Only-reaches-local-optimum&quot;&gt;Only reaches local optimum&lt;a class=&quot;anchor-link&quot; href=&quot;#Only-reaches-local-optimum&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc28.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Some approaches try to avoid local optima like sampling-based methods: Cross-Entropy Methods (CEM) and $\text{PI}^2$.&lt;/p&gt;
&lt;p&gt;For example, in CEMs, instead of just maintaining the optimal trajectory, it maintains the optimal trajectory's mean and covariance.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc29.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc30.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc31.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Despite being very simple, this works surprisingly well and has very nice guarantees on performance.&lt;/p&gt;
&lt;p&gt;Why does this work?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Search space of decision-time plans much smaller than space of policy parameters: ex. 30x32 vs 32x644x32&lt;/li&gt;
&lt;li&gt;More feasible plans than policy parameters&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Slow-convergence&quot;&gt;Slow convergence&lt;a class=&quot;anchor-link&quot; href=&quot;#Slow-convergence&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc32.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Gradient descent is too slow to converge, and we need to wait for thousands-millions of iterations to train a policy. But this is too long for a one-time plan that we want to through it away after.&lt;/p&gt;
&lt;p&gt;Can we do something like Newton’s method for trajectory optimization, like non-linear optimization? YES!&lt;/p&gt;
&lt;p&gt;We can approximate transitions with linear functions and rewards with quadratics:&lt;/p&gt;
$$
\max_{a_0,...,a_H} \sum_{t=0}^H r_t, \quad s_{t+1} = f_s(s_t, a_t), \quad r_t=f_r(s_t, a_t)
$$$$
f_s(s_t, a_t) \approx As_t + Ba_t, \quad f_r(s_t, a_t) \approx s_t^TQs_t + a_t^TRa_t
$$&lt;p&gt;Then it becomes the Linear-Quadratic Regulator (LQR) problem and can be solved exactly.&lt;/p&gt;
&lt;p&gt;For iLQR, locally approximate the model around the current solution, solve the LQR problem to update the solution, and repeat.&lt;/p&gt;
&lt;p&gt;For Differential dynamic programming (DDP), it is similar, but with a higher-order expansion of $f_s$.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Model-based-control-in-the-loop&quot;&gt;Model-based control in the loop&lt;a class=&quot;anchor-link&quot; href=&quot;#Model-based-control-in-the-loop&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We want to answer this question of how to both learn the model and act based on that simultaneously?&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc33.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&quot;Gathering-data-to-train-models&quot;&gt;Gathering data to train models&lt;a class=&quot;anchor-link&quot; href=&quot;#Gathering-data-to-train-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;How can we gather data to train the model? this is a chicken or the egg problem. Bad policy leads to a bad experience, leads to a bad model, leads to bad policy ...&lt;/p&gt;
&lt;p&gt;This leads to some training stability issues in practice. There are some recent works in game theory to provide criteria for stability. See &lt;em&gt;Rajeswaran et al (2020). A Game Theoretic Framework for Model Based Reinforcement Learning.&lt;/em&gt; for example.&lt;/p&gt;
&lt;h4 id=&quot;Fixed-off-line-datasets&quot;&gt;Fixed off-line datasets&lt;a class=&quot;anchor-link&quot; href=&quot;#Fixed-off-line-datasets&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Another way to address this in the loop issues is to see if we can actually train from a fixed experience that is not related to the policy. Some options that we have are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Human demonstration&lt;/li&gt;
&lt;li&gt;Manually-engineered policy rollouts&lt;/li&gt;
&lt;li&gt;Another (sub-optimal) policy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc34.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This leads to a recent popular topic &lt;em&gt;model-based offline reinforcement learning&lt;/em&gt;. You can see some recent works like &lt;em&gt;Kidambi et al (2020). MOReL: Model-Based Offline Reinforcement Learning.&lt;/em&gt;, 
&lt;em&gt;Yu et al (2020). MOPO: Model-based Offline Policy Optimization.
See also: Levine et al (2020).&lt;/em&gt;, and &lt;em&gt;Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems.&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Data-augmentation&quot;&gt;Data augmentation&lt;a class=&quot;anchor-link&quot; href=&quot;#Data-augmentation&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Another way to generate data is to use the model to generate data to train itself. For example, in &lt;em&gt;Venkatraman et al (2014). Data as Demonstrator&lt;/em&gt;. You might have some trajectory of a real experiment that you got by taking certain actions; then you roll out the model and train to pull its predicted next states to true next states.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc35.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc36.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;There are also some adversarial approaches to generate data to self-audit the model like &lt;em&gt;Lin et al (2020). Model-based Adversarial Meta-Reinforcement Learning.&lt;/em&gt; and &lt;em&gt;Du et al (2019). Model-Based Planning with Energy Models.&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;But even if we do all of these works, models are not going to be perfect. We cannot have experience everywhere, and there will be some approximation errors always. These small errors propagate and compound. We may end up in some states that are a little bit further away from true data, which might be an unfamiliar situation. So it might end up making even bigger errors next time around and so on and so forth that the model rollouts might actually land very far away over time from where you would expect them to be.&lt;/p&gt;
&lt;p&gt;What's worse is that the planner might actually intentionally &lt;em&gt;exploit&lt;/em&gt; these model errors to achieve the goal.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc37.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This leads to a longer model rollouts to be less reliable.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc38.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;You can check &lt;em&gt;Janner et al (2019). When to Trust Your Model:
Model-Based Policy Optimization&lt;/em&gt; for more details.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Acting-under-imperfect-models&quot;&gt;Acting under imperfect models&lt;a class=&quot;anchor-link&quot; href=&quot;#Acting-under-imperfect-models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The question is that &quot;Can we still act with imperfect models?&quot; the answer is yes!&lt;/p&gt;
&lt;h4 id=&quot;Replan-via-model-predictive-control&quot;&gt;Replan via model-predictive control&lt;a class=&quot;anchor-link&quot; href=&quot;#Replan-via-model-predictive-control&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The first approach is to not commit to just one single plan (open-loop control) but continually re-plan as you go along (closed-loop control).&lt;/p&gt;
&lt;p&gt;Let's see one example.&lt;/p&gt;
&lt;p&gt;You might start at some initial state and create an imaginary plan using the trajectory optimization methods like CEM or other methods. Then apply just the first action of this plan. That might take you to some state that might not in the practice match with your model imagined you would end up with. But it's ok! You can just re-plan from this new state, again and again, take the first action and ... and by doing this, there is a good chance to end up near the goal.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc39.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc40.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc41.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc42.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc43.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;By doing this, the errors don't accumulate. So you don't need a perfect model; just one pointing in the right direction is enough. This re-planning might be expensive, but one solution is to reuse solutions from previous steps as initial guesses for the next plan.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc44.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Plan-conservatively&quot;&gt;Plan conservatively&lt;a class=&quot;anchor-link&quot; href=&quot;#Plan-conservatively&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;We have'seen that longer rollouts become more unreliable. One solution would be just to keep the rollouts short. So we don't deviate too far from where we have real data. And as we saw in Dyna, just one single rollout can be also very helpful to improve learning.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc45.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The other option to plan conservatively is to consider a distribution over your models and plan for either the average or worst case wrt distribution over your model or model uncertainty.&lt;/p&gt;
$$
\max_{\theta} E_{f \sim F} [\sum_t \gamma^t r_t], \quad a_t=\pi_{\theta}(s_t), \quad s_{t+1}=f_s(s_t, a_t), \quad r_t=f_r(s_t, a_t)
$$&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc46.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc47.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Another option for conservative planning is to try to stay close to states where the model is certain. There are a couple of ways to do this:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc48.png&quot; alt=&quot;&quot; style=&quot;max-width: 150px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Implicitly: stay close to past policy that generated the real data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Peters et al (2012). Relative Entropy Policy Search&lt;/li&gt;
&lt;li&gt;Levine et al (2014). Guided Policy Search under Unknown Dynamics.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Explicitly: add penalty to reward or cost function for going into unknown region&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kidambi et al (2020). MOReL: Model-Based Offline Reinforcement Learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In the last two options for conservative planning, we need uncertainty. So how do we get this model uncertainty?&lt;/p&gt;
&lt;h3 id=&quot;Estimating-model-uncertainty&quot;&gt;Estimating model uncertainty&lt;a class=&quot;anchor-link&quot; href=&quot;#Estimating-model-uncertainty&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Model uncertainty, if necessary for conservative planning, but it has other applications too that we will see later.&lt;/p&gt;
&lt;p&gt;We consider two sources of uncertainty:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Epistemic uncertainty&lt;ul&gt;
&lt;li&gt;Model's lack of knowledge about the world&lt;/li&gt;
&lt;li&gt;Distribution over beliefs&lt;/li&gt;
&lt;li&gt;Reducible by gathering more experience about the world&lt;/li&gt;
&lt;li&gt;Changes with learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Aleatoric uncertainty/Risk&lt;ul&gt;
&lt;li&gt;World's inherent stochasticity&lt;/li&gt;
&lt;li&gt;Distribution over outcomes&lt;/li&gt;
&lt;li&gt;Irreducible&lt;/li&gt;
&lt;li&gt;Static as we keep learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are multiple approaches to estimate these uncertainties, which are listed as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Probabilistic neural networks that try to model distributions over the outputs of your model.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Model explicitly outputs means and variances (typically Gaussian)&lt;/p&gt;
&lt;p&gt;$$ p(s_{t+1}|s_t, a_t) = N(\mu_{\theta}(s_t, a_t), \sigma_{\theta}(s_t, a_t))$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Simple and reliable (supervised learning)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Only captures aleatoric uncertainty/risk&lt;/li&gt;
&lt;li&gt;No guarantees for reasonable outputs outside of training data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bayesian neural network&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Model has a distribution over neural network weights&lt;/p&gt;
&lt;p&gt;$$ p(s_{t+1}|s_t, a_t) = E_{\theta}[p(s_{t+1}|s_t, a_t, \theta)]$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Captures epistemic and aleatoric uncertainty&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Factorized approximations can underestimate uncertainty&lt;/li&gt;
&lt;li&gt;Can be hard to train (but an active research area)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc49.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gaussian processes &lt;ul&gt;
&lt;li&gt;Captures epistemic uncertainty&lt;/li&gt;
&lt;li&gt;Explicitly control state distance metric&lt;/li&gt;
&lt;li&gt;Can be hard to scale (but an active research area)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pseudo-counts&lt;ul&gt;
&lt;li&gt;Count or hash states you already visited&lt;/li&gt;
&lt;li&gt;Captures epistemic uncertainty&lt;/li&gt;
&lt;li&gt;Can be sensitive to state space in which you count&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mbc50.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ensembles&lt;ul&gt;
&lt;li&gt;Train multiple models independently and combine predictions across models&lt;/li&gt;
&lt;li&gt;Captures epistemic uncertainty&lt;/li&gt;
&lt;li&gt;Simple to implement and applicable in many contexts&lt;/li&gt;
&lt;li&gt;Can be sensitive to state space and network architecture&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For discussion in the context of reinforcement learning, see &lt;em&gt;Osband et al (2018). Randomized Prior Functions for Deep Reinforcement Learning.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Between the above options, Ensembles are currently popular due to simplicity and flexibility.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Combining-planning-and-learning&quot;&gt;Combining planning and learning&lt;a class=&quot;anchor-link&quot; href=&quot;#Combining-planning-and-learning&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We compared these two methods in previous sections and saw that background and decision-time planning have complementary strengths and weaknesses.&lt;/p&gt;
&lt;p&gt;How to combine decision-time planning and background planning methods and get the benefits of both?&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Distillation&quot;&gt;Distillation&lt;a class=&quot;anchor-link&quot; href=&quot;#Distillation&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In this approach, we gather a collection of initial states and run our decision-time planner for each initial state and get a collection of trajectories that succeed at reaching the goal. Once we collected this collection of optimal trajectories, we can use a supervised learning algorithm to train either policy function or any other function to map states to actions. This is similar to Behavioral Cloning (BC).&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb1.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb2.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Some issues that can arise:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What is the learned policies that have compounding errors?&lt;/strong&gt; If we rollout the policy from one of the states, it does something different than what we intended to do.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Create new decision-time plans from these states that have been visited by the policy.&lt;/li&gt;
&lt;li&gt;Add these trajectories (new decision-time plans) to the distillation dataset (expand dataset where policy makes errors)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb3.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This is the idea of Dagger algorithm:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb4.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What if the plans are not consistent?&lt;/strong&gt; There are several ways to achieving a goal, and we've seen that by changing the initial condition only a little bit, the decision-time planner can give us pretty different solutions to reach a single goal. This chaotic behavior might be hard to distill into the policy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb5.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;we can make it so that the policy function that we are learning actually feeds back and influences our planner. &lt;/li&gt;
&lt;li&gt;To do this, we can add an additional term in our cost that says stay close to the policy. $D$ in the below cost function is the distance between actions of the planner, $a_t$, and the policy outputs, $\pi(s_t)$. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb6.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb7.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb8.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Terminal-value-functions-(value-of-the-terminal-state)&quot;&gt;Terminal value functions (value of the terminal state)&lt;a class=&quot;anchor-link&quot; href=&quot;#Terminal-value-functions-(value-of-the-terminal-state)&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;One of the issues with many trajectory optimizations or discrete search approaches is that the planning horizon is typically finite. This may lead to myopic or greedy behavior.&lt;/p&gt;
$$
J^H = \sum_{t=0}^H \gamma^t r_t
$$&lt;p&gt;To solve this problem, we can use the value function at the terminal state and add it to the objective function. This learned value function guides plans to good long-term states. So the objective function would be infinite horizon:&lt;/p&gt;
$$
J^{\infty} = \sum_{t=0}^{\infty} \gamma^t r_t = \sum_{t=0}^H \gamma^t r_t + \gamma^H V(s_H)
$$&lt;p&gt;This is another kind of combining decision-time planning (optimization problem) with background planning (learned value function).&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb9.png&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This can be used in both discrete and continuous action spaces:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb10.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Planning-as-policy-improvement&quot;&gt;Planning as policy improvement&lt;a class=&quot;anchor-link&quot; href=&quot;#Planning-as-policy-improvement&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;So far, we used policy (background) or decision-time planner to make a decision and generate trajectory and actions.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb11.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;But we can combine them and use the planner as policy improvement. We can use the policy to provide some information for the planner. For example, the policy can output its set of trajectories, and the planner can use it as a warm start or initialization to improve upon. We would like to train the policy such that the improvement proposed by the planner has no effect. So the policy trajectory is the best that we can do. I think we can see the planner as a teacher for the policy.&lt;/p&gt;
&lt;p&gt;Some related papers are listed here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Silver et al (2017). Mastering the game of Go without human knowledge.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Levine et al (2014). Guided Policy Search under Unknown Dynamics.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Anthony et al (2017). Thinking Fast and Slow with Deep Learning and Tree Search.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb12.png&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Implicit-planning&quot;&gt;Implicit planning&lt;a class=&quot;anchor-link&quot; href=&quot;#Implicit-planning&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In addition, to use a planner to improve policy trajectory, we can put the planner as a component &lt;em&gt;inside&lt;/em&gt; the policy network and train end-to-end.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb13.png&quot; alt=&quot;&quot; style=&quot;max-width: 400px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The advantage of doing this is that the policy network dictates abstract state/action spaces to plan in. But the downside of this is that it requires differentiating through the planning algorithm. But the good news is that multiple algorithms we've seen have been made differentiable and amenable to integrating into such a planner.&lt;/p&gt;
&lt;p&gt;some examples are as follows:&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb14.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb15.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb16.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;There are also some works that show the planning could &lt;em&gt;emerge&lt;/em&gt; in generic black-box policy network and model-free RL training.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/comb17.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-else-can-models-be-used-for?&quot;&gt;What else can models be used for?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-else-can-models-be-used-for?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Consider we have a model of the world. We can use the model in a lot of different ways like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exploration&lt;/li&gt;
&lt;li&gt;Hierarchical Reasoning&lt;/li&gt;
&lt;li&gt;Adaptivity &amp;amp; Generalization&lt;/li&gt;
&lt;li&gt;Representation Learning&lt;/li&gt;
&lt;li&gt;Reasoning about other agents&lt;/li&gt;
&lt;li&gt;Dealing with partial observability&lt;/li&gt;
&lt;li&gt;Language understanding&lt;/li&gt;
&lt;li&gt;Commonsense reasoning&lt;/li&gt;
&lt;li&gt;and more!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here we're gonna just focus on the first four ways that we can use the model to encourage better behavior.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu1.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Exploration&quot;&gt;Exploration&lt;a class=&quot;anchor-link&quot; href=&quot;#Exploration&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;One of the good things about having a model of the world is that you can reset to any state in the world that you might care about. It's not possible in all environments to reset like a continual learning problem. But if you have the model of the world, you can reset to any state you want.&lt;/p&gt;
&lt;p&gt;We can also consider resetting to intermediate states in the middle of the episode as a starting point. The idea is to keep track of one of the interesting states and does exploration from there. So if you have the world's model, you can again reset to that state and efficiently perform additional explorations.&lt;/p&gt;
&lt;p&gt;You can also reset from the final state rather than the initial state. This can be useful in situations where there is only a single goal state like Rubik's Cube. In this case, there is only one goal but maybe several possible starting states. So it would be useful to reset to the final state and explore backward from there rather than starting from the initial state.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu2.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Another way that models can be used to facilitate exploration is by using &lt;em&gt;intrinsic reward&lt;/em&gt;. In these cases, we want to explore places that we haven't been much so that we can gather data in those locations and learn more about them. One way to identify where we haven't been is to use model prediction error as a proxy. Basically, we learn a world model, then we predict what the next state is going to be and then take action and observe the next state and compare it with the predicted state and calculate the model error. We can then use this prediction error as a signal in the intrinsic reward to encourage the agent to explore the locations we haven't visited often to learn more about them.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu3.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In addition to the above approach, we can also &lt;em&gt;plan to explore&lt;/em&gt;. In &lt;em&gt;POLO&lt;/em&gt; paper, rather than using the error from your prediction model, they use the error across an ensemble of value functions and use it as an intrinsic reward. Actually, at each state, we compute a bunch of different values from our ensemble of value functions, then take softmax over them to give us an optimistic estimate of what the value is going to be. We can use this optimistic value estimate as an intrinsic reward. We plan to maximize this optimistic value estimate, and then this allows us to basically, during planning, identify places that we should direct our behavior towards are more surprising or more interesting.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute intrinsic reward during (decision-time) planning to direct the agent into new regions of state-space&lt;/li&gt;
&lt;li&gt;Intrinsic reward = softmax across an ensemble of value functions&lt;/li&gt;
&lt;/ul&gt;
$$
\hat{V}(s) = log(\sum_{k=1}^K exp(k\hat{V}_{\theta_k}(s)))
$$&lt;p&gt;
&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/003rQ5vUcek?t=3&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Lowrey et al. (2019). Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control. ICLR 2019.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also use the same idea, but instead of using a set of disagreement across on ensemble of value functions, we can compute disagreement across transition functions. Now because we are just using state transitions, this turns into a task agnostic exploration problem. We can then plan where there is a disagreement between our transition functions and direct behavior towards those regions of space to learn a really robust world model. And then use this model of the world to learn new tasks either using zero-shot or few-shot (examples of experience).&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu5.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Finally, another form of exploration is that if we have a model of possible states that we might find ourselves in, not necessarily a transition model but a density model over goals, we can sample possible goals from this density model and then train our agent achieve the goals.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu6.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Hierarchical-reasoning&quot;&gt;Hierarchical reasoning&lt;a class=&quot;anchor-link&quot; href=&quot;#Hierarchical-reasoning&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;A very classic way of doing hierarchical reasoning is what's called &lt;em&gt;task and motion planning (TAMP)&lt;/em&gt; in robotics. You jointly plan symbolically at the task level, and then you also plan in the continuous space and do motion planning at the low-level—you sort of doing these things jointly in order to solve relatively long-horizon and multi-step tasks. For example, in the following figure, to control a robot arm and to get block $A$ and put it in the washer, wash it, and then put it in storage. In order to do this, you first have to move $C$ and $B$ out of the way and put $A$ into the washer, then move $D$ out of the way and then put $A$ into the storage. By leveraging symbolic representation, like PDDL from the beginning of the post, allows you to be able to jointly solve these hierarchical tasks.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu7.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The other example of this is the OpenAI Rubik's Cube solver. The idea is that you use a high-level symbolic algorithm, Kociemba's algorithm, to generate the solution (plan) of high-level actions, for example, which faces should be rotated, and then you have a low-level neural network policy that generates the controls needed to achieve these high-level actions. This low-level control policy is quite challenging to learn.

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/kVmp0uGtShk&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;OpenAI et al. (2019). Solving Rubik's Cube with a Robot Hand. arXiv.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The question that might arise is that where does this high-level state-space come from?&lt;/p&gt;
&lt;p&gt;We don't want to hand-code symbolically on these high-level roles that we want to achieve. Some model-free works try to answer this, but we focus on some MBRL approaches here for this problem.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu8.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&quot;Subgoal-based-approaches&quot;&gt;Subgoal-based approaches&lt;a class=&quot;anchor-link&quot; href=&quot;#Subgoal-based-approaches&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;We can consider that any state you might find yourself in in the world as a subgoal. We don't want to construct a super long sequence of states to go through, but a small sequence. So the idea would be which states do we pick as a subgoal. Rather than learning a forward state transition model, we can learn a universal value function approximator, $V(s, g)$, that tells us the value of going from state $s$ to goal state $g$. We can train these value functions between our subgoals to estimate how good a particular plan of length $k$ is. A plan of length $k$ is then given by maximizing:&lt;/p&gt;
$$
\text{arg}\max_{\{s_i\}_{i=1}^k} (V(s_0, s_1) + V(s_k, s_g) + \sum_{i=1}^{k-1} V(s_i, s_{i+1}))
$$&lt;p&gt;The figure below shows the idea. If you start from state $s_0$ and you want to go to $s_{\infty}$, you can break up this long plan of length one into a plan of length two by inserting a subgoal. You can do this recursively multiple times to end up with a plan of length $k$ or, in this case, a plan of length three.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu9.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;When we use a planner to identify which of these subgoals we should choose in order to maximize the above equation, in the figure below, you see which white subgoal it is considering as subgoal in order to find a path between the green and the blue points.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/maze.gif&quot; alt=&quot;&quot; style=&quot;max-width: 200px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Nasiriany et al. (2019). Planning with Goal-Conditioned Policies. NeurIPS.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Jurgenson et al. (2019). Sub-Goal Trees -- A Framework for Goal-Directed Trajectory Prediction and Optimization. arXiv.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Parascandolo, Buesing, et al. (2020). Divide-and-Conquer Monte Carlo Tree Search For Goal-Directed Planning. arXiv.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Skill-based-approaches&quot;&gt;Skill-based approaches&lt;a class=&quot;anchor-link&quot; href=&quot;#Skill-based-approaches&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Here, rather than identifying discrete states as subgoals that we want to try to achieve, what we want to do is to learn a set of skills that sort of fully parametrize the space of possible trajectories that we might want to execute. So, for example, in the Ant environment, a nice parametrization of skills would be to say a particular direction that you want to get to move in. So the approach taken by this paper is to learn a set of skills those outcomes are both (1) easy to predict, so if you train a dynamics model to predict the outcome of executing the skill, and (2) the skills are diverse from one another. That's why you get this nice diversity of the ant moving in different directions. This works very well for zero-shot adaptation to new sequences of goals. As you can see on the bottom, this is an ant going to a few different locations in space, and it is doing this by just pure planning using this set of skills that it is learned during the unsupervised training phase.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn a set of skills whose outcomes are (1) easy to predict and (2) diverse&lt;/li&gt;
&lt;li&gt;Learn dynamics model over skills, and plan with MPC&lt;/li&gt;
&lt;li&gt;Can solve long-horizon sequences of high-level goals with no additional learning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/ant.gif&quot; alt=&quot;&quot; style=&quot;max-width: 500px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Sharma et al. (2020). Dynamics-Aware Unsupervised Discovery of Skills. ICLR.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;representation-Learning&quot;&gt;representation Learning&lt;a class=&quot;anchor-link&quot; href=&quot;#representation-Learning&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Beyond just using models for prediction, they can be used as regularizers for training other types of representations that then you can train a policy on.&lt;/p&gt;
&lt;p&gt;One way is to learn a model as an &lt;em&gt;auxiliary loss&lt;/em&gt;. For example, if you have an A2C algorithm and add an auxiliary loss to predict the reward it's gonna achieve, in some cases, you can get a large improvement in performance by just adding this auxiliary loss. By considering this loss during training, we are actually forcing it to learn the future and capture the structure of the world, which is useful. We also don't use this learned model in planning and just for representation learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Jaderberg et al. (2017). Reinforcement learning with unsupervised auxiliary tasks. ICLR 2017.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The other same idea is to use a &lt;em&gt;contrastive loss&lt;/em&gt;, like CPC paper (below), that tries to predict what observations it might encounter in the future, and by adding this additional loss during training, we see improvement in performance.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;van den Oord, Li, &amp;amp; Vinyals (2019). Representation Learning with Contrastive Predictive Coding. arXiv.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another idea is &lt;em&gt;plannable representations&lt;/em&gt; that make it much easier to plan in. For example, if we are in a continuous space, we can discretize it in an intelligent way that might make it easy to use some of these discrete search methods, like MCTS, to rapidly come up with a good plan of actions. Or maybe we can come up with a representation for our state space such that moving along a direction in the latent state space corresponds to planning. So you can basically just interpolate between states in order to come up with a plan.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn an embedding of states that is easier to plan in, e.g.&lt;ul&gt;
&lt;li&gt;Discretized&lt;/li&gt;
&lt;li&gt;States that can be transitioned between should be near to each other in latent space!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Related to notions in hierarchical RL (state abstraction)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu10.png&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Corneil et al. (2018). Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation. ICML.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Kurutach et al. (2018). Learning Plannable Representations with Causal InfoGAN. NeurIPS.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Adaptivity-&amp;amp;-generalization&quot;&gt;Adaptivity &amp;amp; generalization&lt;a class=&quot;anchor-link&quot; href=&quot;#Adaptivity-&amp;amp;-generalization&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Models of the world can also be used for fast adaptation and generalization.&lt;/p&gt;
&lt;p&gt;The world can be changed in two different ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Change in rewards&lt;/strong&gt;. So we're being asked to do a new task, but the dynamics are the same.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Change in dynamics&lt;/strong&gt;. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Based on the above changes, we can do different things in response to them.&lt;/p&gt;
&lt;p&gt;In a model-free approach, we just adapt to the policy. But this tends to be relatively slow because it's hard to quickly adapt changes in rewards to the same dynamics and vice versa because they are sort of entangled with each other.&lt;/p&gt;
&lt;p&gt;If we have an explicit model of the world, we can update our behavior differently. One option would be that we can adapt the planner, but we can also adapt the model itself, or we can do both.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mu11.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&quot;Adapting-the-planner-in-new-states&quot;&gt;Adapting the planner in new states&lt;a class=&quot;anchor-link&quot; href=&quot;#Adapting-the-planner-in-new-states&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;A pre-trained policy may not generalize to all states (especially in combinatorial spaces). So some states that we might find ourselves in might be required harder or more reasoning, and others may require less.
We have to try to detect when planning is required, and they adapt the amount of planning depending on the difficulty of the task. For example, in the following gifs, in the upper case, the n-body agent can easily solve the task and reach the center's goal using just a couple of simulations. But in the bottom case, it is much harder to reason about because it starts on one of the planets, which requires many more simulations. We can adaptively change this amount of computation as needed. Save the computation on easy scenes and then spend it more on the hard ones.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/easy.gif&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;

&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/hard.gif&quot; alt=&quot;&quot; style=&quot;max-width: 300px&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Hamrick et al. (2017). Metacontrol for adaptive imagination-based optimization. ICLR 2017.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Pascanu, Li, et al. (2017). Learning model-based planning from scratch. arXiv.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Adapting-the-planner-to-new-rewards&quot;&gt;Adapting the planner to new rewards&lt;a class=&quot;anchor-link&quot; href=&quot;#Adapting-the-planner-to-new-rewards&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Here is another same idea in a life-long learning setup where the reward can suddenly change, and either the agents can observe the change in the reward, or they just have to infer the reward has changed. Because of changes in reward, it needs more planning because the prior policy is less reliable, and more planning allows you to better explore these different options for the reward function. In the video below, as you can see in the bottom agent after the reward is changed, the agent needs to do more planning to have a nice movement compared to the other two agents. 

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/3T3QuKregt0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Lu, Mordatch, &amp;amp; Abbeel (2019). Adaptive Online Planning for Continual Lifelong Learning. NeurIPS Deep RL Workshop.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Adapting-the-model-to-new-dynamics&quot;&gt;Adapting the model to new dynamics&lt;a class=&quot;anchor-link&quot; href=&quot;#Adapting-the-model-to-new-dynamics&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;For the times that the dynamics change, it could be very useful to adapt the model. One way to approach this is to train the model using the meta-learning objective so that during training, you're always training it to adapt  to a slightly different environment around you, and at the test time, you actually see a new unobserved environment that you never saw before, you can take a few gradient steps to adapt the model to deal with these new situations. Here is an example where the agent, half cheetah, has been trained to walk along some terrain, but it's never seen as a little hill before. Therefore, the baseline methods that cannot adapt their model cannot get the agent to go up the hill, where this meta-learning version can get the cheetah to go up the hill. 

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/ejG2nzCNdZ8?t=144&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Nagabandi et al. (2019). Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning. ICLR.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What's-missing-from-model-based-methods?&quot;&gt;What's missing from model-based methods?&lt;a class=&quot;anchor-link&quot; href=&quot;#What's-missing-from-model-based-methods?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Humans are ultimate model-based reasoners and we can learn a lot from how we build and deploy models of the world. - &lt;strong&gt;Motor control&lt;/strong&gt;: forward kinematics models in the cerebellum. We have a lot of motor systems that are making predictions about how our muscles are going to affect the kinematics of our bodies.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Language comprehension&lt;/strong&gt;: we build models of what is being communicated in order to understand.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pragmatics&lt;/strong&gt;: we construct models of listener &amp;amp; speaker beliefs in order to try to understand what is tryingto be communicated.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Theory of mind&lt;/strong&gt;: we construct models of other agents’ beliefs and behavior in order to predict what they are going to do.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decision making&lt;/strong&gt;: model-based reinforcement learning&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Intuitive physics&lt;/strong&gt;: forward models of physical dynamics&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scientific reasoning&lt;/strong&gt;: mental models of scientific phenomena&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Creativity&lt;/strong&gt;: being able to imagine novel combinations of things&lt;/li&gt;
&lt;li&gt;… and much more!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more you can see the following reference:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Markman, Klein, &amp;amp; Suhr (2008). Handbook of Imagination and Mental Simulation.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Abraham (2020). The Cambridge Handbook of the Imagination.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;If you look at the mentioned different domains, where people are engaging a model based reasoning, a few themes emerge that could be really useful in thinking about how to continue to develop our models in MBRL.&lt;/p&gt;
&lt;p&gt;Humans use their models of the world in ways that are compositional, causal, incomplete, adaptive, efficient, and abstract. Taking these ideas and trying to distill them into MBRL enables us to do&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;faster planning&lt;/li&gt;
&lt;li&gt;have systems with higher tolerance to model error&lt;/li&gt;
&lt;li&gt;can be scaled to much much harder problems.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This will lead us to more robust real-world applications and better common sense reasoning.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis1.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Compositionality&quot;&gt;Compositionality&lt;a class=&quot;anchor-link&quot; href=&quot;#Compositionality&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Humans are much much stronger than MBRL algorithms that we have in compositionality.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis2.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&quot;Causality&quot;&gt;Causality&lt;a class=&quot;anchor-link&quot; href=&quot;#Causality&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis3.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&quot;Incompleteness&quot;&gt;Incompleteness&lt;a class=&quot;anchor-link&quot; href=&quot;#Incompleteness&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Another facet of human model based reasoning in the fact that we can reason about incomplete models, but reason about them in very tich ways. This is in contrast to model-based RL which if we have model error, it would be a huge deal and are very far from human capabilities.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis4.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&quot;Adaptivity&quot;&gt;Adaptivity&lt;a class=&quot;anchor-link&quot; href=&quot;#Adaptivity&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The way that we (humans) use our models is also incredibly adaptive. We can rapidly assemble our compositional knowledge into on-the-fly models that are adapted to the current task. Then we quickly solve these models, leveraging both mental simulation &amp;amp; (carefully chosen) real experience&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/adapt.gif&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Allen, Smith, &amp;amp; Tenenbaum (2019). The tools challenge: Rapid trial-and-error learning in physical problem solving. CogSci 2019.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Dasgupta, Smith, Schulz, Tenenbaum, &amp;amp; Gershman (2018). Learning to act by integrating mental simulations and physical experiments. CogSci 2018.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Efficiency&quot;&gt;Efficiency&lt;a class=&quot;anchor-link&quot; href=&quot;#Efficiency&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Humans' model-based reasoning is also very efficient. Figure below illustrates how much of an improvement Alpha-zero was over former state of the art chess engine which requires a tens of millions of moves during simulation. Whereas Alpha-zero only needs tens of thousands. But again it is not comparable to human grandmaster, which only requires hundreds of moves. So we need to continue to develop planners that are able to sort of leverage our models as quickly and as efficiently as possible towards this type of efficiency.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis5.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&quot;Abstraction&quot;&gt;Abstraction&lt;a class=&quot;anchor-link&quot; href=&quot;#Abstraction&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The final feature of humans' ability to use models of the world is abstraction. We go through all of different levels of abstraction as we're planning over multiple timescales, over multiple forms of state abstraction, and we move up and down different forms of abstraction as needed and so we ideally want integrated agents that could do the same.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis6.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Conclusion&quot;&gt;Conclusion&lt;a class=&quot;anchor-link&quot; href=&quot;#Conclusion&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In this tutorial, we discussed what it means to have a model of the world and different types of models that you can learn. We also talked about where the model fits into the RL loop. We talked about landscape of model-based methods and some practical considerations that we care about when integrating models into the loop. We also saw how we can try to improve models by looking towards human cognition.&lt;/p&gt;
&lt;h3 id=&quot;Ethical-and-broader-impacts&quot;&gt;Ethical and broader impacts&lt;a class=&quot;anchor-link&quot; href=&quot;#Ethical-and-broader-impacts&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Because MBRL inherits methods both from model-free RL and model learning in general, it inherits the problems from both of them too.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  
    &lt;img class=&quot;docimage&quot; src=&quot;/blog/images/copied_from_nb/images/mbrl/mis7.png&quot; alt=&quot;&quot; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" /><media:content medium="image" url="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Online Reinforcement Learning</title><link href="https://kargarisaac.github.io/blog/rl/2020/06/15/online-rl.html" rel="alternate" type="text/html" title="Online Reinforcement Learning" /><published>2020-06-15T00:00:00-05:00</published><updated>2020-06-15T00:00:00-05:00</updated><id>https://kargarisaac.github.io/blog/rl/2020/06/15/online-rl</id><content type="html" xml:base="https://kargarisaac.github.io/blog/rl/2020/06/15/online-rl.html">&lt;h1 id=&quot;online-reinforcement-learning&quot;&gt;Online Reinforcement Learning&lt;/h1&gt;

&lt;p&gt;In this post I will overview different single and multi-agent online Reinforcement Learning (RL) algorithms. By &lt;strong&gt;online&lt;/strong&gt; I mean the algorithms that can interact with an environment and collect data, in contrast to offline RL. I will update this post and add algorithms periodically.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/rl-diagram.png&quot; alt=&quot;RL diagram&quot; /&gt; &lt;em&gt;RL diagram&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here are some resources to learn more about RL!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;David Silver’s &lt;a href=&quot;https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&quot;&gt;course&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CS287 at UC Berkeley - Advanced Robotics &lt;a href=&quot;https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNBPJdt8WamRAt4XKc639wF&quot;&gt;course&lt;/a&gt; - Instructor: Pieter Abbeel&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CS 285 at UC Berkeley - Deep Reinforcement Learning &lt;a href=&quot;http://rail.eecs.berkeley.edu/deeprlcourse/&quot;&gt;course&lt;/a&gt; - Instructor: Sergey Levine&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CS234 at Stanford - Reinforcement Learning &lt;a href=&quot;http://web.stanford.edu/class/cs234/index.html&quot;&gt;course&lt;/a&gt; - Instructor: Emma Brunskill&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CS885 at University of Waterloo - Reinforcement Learning &lt;a href=&quot;https://www.youtube.com/playlist?list=PLdAoL1zKcqTXFJniO3Tqqn6xMBBL07EDc&quot;&gt;course&lt;/a&gt; - Instructor: Pascal Poupart&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Arthur Juliani’s &lt;a href=&quot;https://medium.com/@awjuliani&quot;&gt;posts&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Jonathan Hui’s &lt;a href=&quot;https://medium.com/@jonathan_hui/rl-deep-reinforcement-learning-series-833319a95530&quot;&gt;posts&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A Free &lt;a href=&quot;https://simoninithomas.github.io/deep-rl-course/&quot;&gt;course&lt;/a&gt; in Deep Reinforcement Learning from beginner to expert by Thomas Simonni&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;single-agent&quot;&gt;Single agent&lt;/h1&gt;

&lt;h3 id=&quot;dqn&quot;&gt;DQN&lt;/h3&gt;

&lt;p&gt;We will take a look at DQN with experience replay buffer and the target network.&lt;/p&gt;

&lt;p&gt;DQN is a value-based method. It means that we try to learn a value function and then use it to achieve the policy. In DQN we use a neural network as a function approximator for our value function. It gets the state as input and outputs the value for different actions in that state. These values are not limited to be between zero and one, like probabilities, and can have other values based on the environment and the reward function we define.&lt;/p&gt;

&lt;p&gt;DQN is an off-policy method which means that we are using data from old policies, the data that we gather in every interaction with the environment and save it in the experience replay buffer, to sample from it later and train the network. The size of the replay buffer should be large enough to reduce the $i.i.d$ property between data that we sample from it.&lt;/p&gt;

&lt;p&gt;To use DQN, the action should be discrete. We can use it for continuous action spaces by discretizing the action space, but it’s better to use other techniques that can handle continuous action spaces such as Policy Gradients.
First, let’s see the algorithm’s sudo code:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/dqn.png&quot; alt=&quot;DQN algorithm&quot; title=&quot;DQN algorithm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this algorithm, we have experience replay buffer and a target network with a different set of parameters that will be updated every $C$ steps. These tricks help to get a better and more stable method rather than pure DQN. There are a lot of improvements for DQN and we will see some of them in the next posts too.&lt;/p&gt;

&lt;p&gt;First, we initialize the weights of both networks and then start from the initial state s and take action a with epsilon-greedy policy. In the epsilon-greedy policy, we select an action a randomly or using the Q-network. Then we execute the selected action and get the next state, reward, and the done values from the environment and save them in our replay buffer. Then we sample a random batch from the replay buffer and calculate target based on the Bellman equation in the above picture and use MSE loss and gradient descent to update the network weights. We will update the weights of our target network every $C$ steps.&lt;/p&gt;

&lt;p&gt;In the training procedure, we use epsilon decay. It means that we consider a big value for epsilon, such as $1$. Then during the training procedure, as we go forward, we reduce its value to something like $0.02$ or $0.05$, based on the environment. It will help the agent to do more exploration in the first steps and learn more about the environment. It’s better to have some exploration always. That’s a trade-off between exploration-exploitation.
In test time, we have to use a greedy policy. It means we have to select the action with the highest value, not randomly anymore (set epsilon to zero actually).&lt;/p&gt;

&lt;h3 id=&quot;reinforce&quot;&gt;REINFORCE&lt;/h3&gt;

&lt;p&gt;REINFORCE is a Monte-Carlo Policy Gradient (PG) method. In PGs, we try to find a policy to map the state into action directly.&lt;/p&gt;

&lt;p&gt;In value-based methods, we find a value function and use it to find the optimal policy. Policy gradient methods can be used for stochastic policies and continuous action spaces. If you want to use DQN for continuous action spaces, you have to discretize your action space. This will reduce the performance and if the number of actions is high, it will be difficult and impossible. But REINFORCE algorithms can be used for discrete or continuous action spaces. They are on-policy because they use the samples gathered from the current policy.&lt;/p&gt;

&lt;p&gt;There are different versions of REINFORCE. The first one is without a baseline. It is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/reinforce.png&quot; alt=&quot;reinforce algorithm&quot; title=&quot;from Sutton Barto book: Introduction to Reinforcement Learning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this version, we consider a policy (here a neural network) and initialize it with some random weights. Then we play for one episode and after that, we calculate discounted reward from each time step towards the end of the episode. This discounted reward (G in the above sudo code) will be multiplied by the gradient. This G is different based on the environment and the reward function we define. For example, consider that we have three actions. The first action is a bad action and the other two actions are some good actions that will cause more future discounted rewards. If we have three positive G values for three different actions, we are pushing the network towards all of them. Actually, we push the network towards action number one slightly and towards others more. Now consider we have one negative G value for the first action and two G values for the other two actions. Here we are pushing the network far from the first action and towards the other two actions. You see?! the value of G and its sign is important. It guides our gradient direction and its step size. To solve such problems, one way is to use baseline. This will reduce the variance and accelerate the learning procedure. For example, subtract the value of the state from it, or normalize it with the mean and variance of the discounted reward of the current episode. You can see the sudo code for REINFORCE with baseline in the following picture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/reinforce2.png&quot; alt=&quot;reinforce algorithm&quot; title=&quot;from Sutton Barto book: Introduction to Reinforcement Learning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this version, first, we initialize the policy and value networks. It is possible to use two separate networks or a multi-head network with a shared part. Then we play an episode and calculate the discounted reward from every step until the end of the episode (reward to go). Then subtract the value (from the learned neural net) for that state from the discounted reward (REINFORCE with baseline) and use it to update the weights of value and policy networks. Then generate another episode and repeat the loop.&lt;/p&gt;

&lt;p&gt;In the Sutton&amp;amp;Barto book, they do not consider the above algorithm as actor-critic (another RL algorithm that we will see in the next posts). It learns the value function but it is not used as a critic! I think it is because we do not use the learned value function (critic) in the first term of the policy gradient rescaler (for bootstrapping) to tell us how good is our policy or action in every step or in a batch of actions (in A2C and A3C we do the update every t_max step). In REINFORCE we update the network at the end of each episode.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“The REINFORCE method follows directly from the policy gradient theorem. Adding a state-value function as a baseline reduces REINFORCE’s variance without introducing bias. Using the state-value function for bootstrapping introduces bias but is often desirable for the same reason that bootstrapping TD methods are often superior to Monte Carlo methods (substantially reduced variance). The state-value function assigns credit to — critizes — the policy’s action selections, and accordingly the former is termed the critic and the latter the actor, and these overall methods are termed actor–critic methods.
Actor–critic methods are sometimes referred to as advantage actor–critic (“A2C”) methods in the literature.”&lt;/em&gt;
[Sutton&amp;amp;Barto — second edition]&lt;/p&gt;

&lt;p&gt;I think Monte-Carlo policy gradient and Actor-Critic policy gradient are good names as I saw in the slides of David Silver course.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/reinforce3.png&quot; alt=&quot;reinforce algorithm&quot; title=&quot;source: https://www.youtube.com/watch?v=KHZVXao4qXs&amp;amp;list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&amp;amp;index=7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I also saw the following slide from the Deep Reinforcement Learning and Control course (CMU 10703) at Carnegie Mellon University:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/reinforce4.png&quot; alt=&quot;reinforce algorithm&quot; title=&quot;source: https://www.andrew.cmu.edu/course//10-703/slides/Lecture_PG-NatGrad-10-8-2018.pdf&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here they consider every method that uses value function (V or Q) as actor-critic and if you just consider reward to go in the policy gradient rescaler, it is REINFORCE. The policy evaluation by the value function can be TD or MC.&lt;/p&gt;

&lt;p&gt;Summary of the categorization:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Vanilla REINFORCE or Policy gradient → we use G as gradient rescaler.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;REINFORCE with baseline → we use $\frac{G-mean(G)}{std(G)}$ or $(G-V)$ as gradient rescaler. We do not use $V$ in $G$. $G$ is only the reward to go for every step in the episode → $G_t = r_t + \gamma r_{t+1} + … $&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Actor-Critic → we use $V$ in the first term of gradient rescaler and call it Advantage ($A$):&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$A_t = Q(s_t, a_t) - V(s_t)$&lt;/p&gt;

&lt;p&gt;$A_t = r_t + \gamma V_{s_{t+1}} - V_{s_t}$ → for one-step&lt;/p&gt;

&lt;p&gt;$A_t = r_t + \gamma r_{t+1} + \gamma^2 V_{s_{t+2}} - V_{s_t}$ → for 2-step&lt;/p&gt;

&lt;p&gt;and so on.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In Actor-Critics you can do the update each $N$ step based on your task. This $N$ can be less than an episode.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anyway, let’s continue.&lt;/p&gt;

&lt;p&gt;This algorithm can be used for either discrete or continuous action spaces. In discrete action spaces, it will output a probability distribution over action, which means that the activation function of the output layer is a softmax. For exploration-exploitation, it samples from the actions based on their probabilities. Actions with higher probabilities have more chances to be selected.&lt;/p&gt;

&lt;p&gt;In continuous action spaces, the output will not have any softmax. Because the output is a mean for a normal distribution. We consider one neuron for each action and it can have any value. In fact, the policy is a normal distribution and we calculate its mean by a neural network. The variance can be fixed or decrease over time or can be learned. You can consider it as a function of the input state, or define it as a parameter that can be learned by gradient descent. If you want to learn the sigma too, you have to consider the number of actions. For example, if we want to map the front view image of a self-driving car into steering and throttle-brake, we have two continuous actions. So we have to have two mean and two variance for these two actions. During training, we sample from this normal distribution for exploration of the environment, but in the test, we only use the mean as action.&lt;/p&gt;

&lt;h3 id=&quot;a2c&quot;&gt;A2C&lt;/h3&gt;

&lt;h3 id=&quot;a3c&quot;&gt;A3C&lt;/h3&gt;

&lt;h3 id=&quot;ppo&quot;&gt;PPO&lt;/h3&gt;

&lt;h3 id=&quot;ddpg&quot;&gt;DDPG&lt;/h3&gt;

&lt;p&gt;This algorithm is from the &lt;em&gt;“Continuous Control with Deep Reinforcement Learning”&lt;/em&gt; &lt;a href=&quot;https://arxiv.org/pdf/1509.02971.pdf&quot;&gt;paper&lt;/a&gt; and uses the ideas from deep q-learning in the continuous action domain and is a model-free method based on the deterministic policy gradient.&lt;/p&gt;

&lt;p&gt;In Deterministic Policy Gradient (DPG), for each state, we have one clearly defined action to take (the output of policy is one value for action and for exploration we add a noise, normal noise for example, to the action). But in Stochastic Gradient Descent, we have a distribution over actions (the output of policy is mean and variance of a normal distribution) and sample from that distribution to get the action, for exploration. In another term, in stochastic policy gradient, we have a distribution with mean and variance and we draw a sample from that as an action. When we reduce the variance to zero, the policy will be deterministic.&lt;/p&gt;

&lt;p&gt;When the action space is discrete, such as q-learning, we get the max over q-values of all actions and select the best action. But in continuous action spaces, you cannot apply q-learning directly, because in continuous spaces finding the greedy policy requires optimization of $a_t$ at every time-step and would be too slow for large networks and continuous action spaces. Based on the proposed equation in the reference paper, here we approximate &lt;em&gt;max Q(s, a)&lt;/em&gt; over actions with &lt;em&gt;Q(a, µ(s))&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In DDPG, they used function approximators, neural nets, for both action-value function $Q$ and deterministic policy function $\mu$. In addition, DDPG uses some techniques for stabilizing training, such as updating the target networks using soft updating for both $\mu$ and $Q$. It also uses batch normalization layers, noise for exploration, and a replay buffer to break temporal correlations.&lt;/p&gt;

&lt;p&gt;This algorithm is an actor-critic method and the network structure is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/ddpg_post/ddpg_diagram.jpg&quot; alt=&quot;DDPG diagram&quot; title=&quot;DDPG diagram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;First, the policy network gets the state and outputs the action mean vector. This will be a vector of mean values for different actions. For example, in a self-driving car, there are two continuous actions: steering and acceleration&amp;amp;braking (one continuous value between $-x$ to $x$, the negative values are for braking and positive values are for acceleration). So we will have two mean for these two actions. To consider exploration, we can use Ornstein-Uhlenbeck or normal noise and add it to the action mean vector in the training phase. In the test phase, we can use the mean vector directly without any added noise. Then this action vector will be concatenated with observation and fed into the $Q$ network. The output of the $Q$ network will be one single value as a state-action value. In DQN, because it had discrete action space, we had multiple state-action values for each action, but here because the action space is continuous, we feed the actions into the $Q$ network and get one single value as the state-action value.&lt;/p&gt;

&lt;p&gt;Finally, the sudo code for DDPG is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/ddpg_post/ddpg_algorithm.jpg&quot; alt=&quot;DDPG algorithm&quot; title=&quot;DDPG algorithm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To understand the algorithm better, it’s good to try to implement it and play with its parameters and test it in different environments. Here is a good implementation in PyTorch that you can start with &lt;a href=&quot;https://github.com/higgsfield/RL-Adventure-2/blob/master/5.ddpg.ipynb&quot;&gt;this&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also found the Spinningup implementation of DDPG very clear and understandable too. You can find it &lt;a href=&quot;https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ddpg/ddpg.py&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For POMDP problems, it is possible to use LSTMs or any other RNN layers to get a sequence of observations. It needs a different type of replay buffer for sequential data.&lt;/p&gt;

&lt;h3 id=&quot;sac&quot;&gt;SAC&lt;/h3&gt;

&lt;h3 id=&quot;ape-x&quot;&gt;Ape-X&lt;/h3&gt;

&lt;h3 id=&quot;r2d2&quot;&gt;R2D2&lt;/h3&gt;

&lt;h3 id=&quot;impala&quot;&gt;IMPALA&lt;/h3&gt;

&lt;h3 id=&quot;never-give-up&quot;&gt;Never Give-Up&lt;/h3&gt;

&lt;h3 id=&quot;agent57&quot;&gt;Agent57&lt;/h3&gt;

&lt;h1 id=&quot;multi-agent&quot;&gt;Multi-Agent&lt;/h1&gt;

&lt;p&gt;In Multi-Agent Reinforcement Learning (MARL)problems, there are several agents who usually have their own private observation and want to take an action based on that observation. This observation is local and different from the full state of the environment in that time-step. The other problem that we face in such environments is the non-stationary problem because all agents are learning and their behavior would be different during training as they learn to act differently.&lt;/p&gt;

&lt;p&gt;To solve this problem, the most naive approach is to use single-agent RL algorithms for each agent and treat other agents as part of the environment. Some methods like Independent Q-Learning (IQL) work fine in some multi-agent RL problems in practice but there is no guarantee for them to converge. In IQL, each agent has one separate action-value function that gets the agent’s local observation to select its action based on that. It is also possible to use additional inputs like previous actions as input. Usually, in partially observable environments, we use RNNs to consider a history of several sequential observation-actions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/marl1.png&quot; alt=&quot;MARL&quot; title=&quot; source: https://arxiv.org/pdf/1706.05296.pdf &quot; /&gt;&lt;/p&gt;

&lt;p&gt;The other approach is to have a fully centralised method to learn and act in a centralised fashion. We can consider this type as a big single-agent problem. This approach is also valid in some problems that you don’t need decentralised execution. For example for traffic management or traffic light management, it is possible to use such approaches.&lt;/p&gt;

&lt;p&gt;There is one more case that is somewhere between the previous two ones: centralised training and decentralised execution. Usually in the training procedure, because we train agents in a simulation environment or in a lab, we have access to the full state and information in the training phase. So it is better to use this knowledge. On the other hand, the learned policy should be decentralised in some environments and agents cannot have access to the full state during the execution phase. So having algorithms to use the available knowledge in the training phase and learn a policy that is not dependent on the full state in the execution time is necessary. Here we focus on the last case.&lt;/p&gt;

&lt;p&gt;There are several works that try to propose such an algorithm and can be divided into two groups:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;value-based methods like Value Decomposition Networks (VDN) and QMIX&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;actor-critic methods like MADDPG and COMA&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;value-based-methods&quot;&gt;VALUE-BASED METHODS&lt;/h2&gt;

&lt;p&gt;These approaches try to propose a way to be able to use value-based methods like Q-learning and train them in a centralised way and use them for decentralised execution.&lt;/p&gt;

&lt;h3 id=&quot;vdn&quot;&gt;VDN&lt;/h3&gt;

&lt;p&gt;This work proposes a way to have separate action-value functions for multiple agents and learn them by just one shared team reward signal. The joint action-value function is a linear summation of all action-value functions of all agents. Actually, by using a single shared reward signal, it tries to learn decomposed value functions for each agent and use it for decentralised execution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/vdn1.png&quot; alt=&quot;VDN&quot; title=&quot; source: https://arxiv.org/pdf/1706.05296.pdf &quot; /&gt;&lt;/p&gt;

&lt;p&gt;Consider a case with 2 agents, the reward would be:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/vdn2.png&quot; alt=&quot;VDN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then the total $Q$ function is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/vdn3.png&quot; alt=&quot;VDN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is using the same Bellman equation to standard q-learning approach and just replaces $Q$ in that equation with the new $Q$ value.&lt;/p&gt;

&lt;h3 id=&quot;qmix&quot;&gt;QMIX&lt;/h3&gt;

&lt;p&gt;QMIX is somehow an extension to value decomposition networks (VDN) but tries to mix the Q-value of different agents in a nonlinear way. They use global state $s_t$ as input to hypernetworks to generate weights and biases of the mixing network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/qmix1.png&quot; alt=&quot;QMIX&quot; title=&quot; source: https://arxiv.org/pdf/1803.11485.pdf&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here again, the equation to update the weights is the standard Bellman equation in which the $Q$ is replaced with $Q_tot$ in the above figure.&lt;/p&gt;

&lt;h2 id=&quot;actor-critic-based-methods&quot;&gt;ACTOR-CRITIC BASED METHODS&lt;/h2&gt;

&lt;p&gt;This group of methods tries to use actor-critic architecture to do centralised training and decentralised execution. Usually, they use the full state and additional information which are available in the training phase in the critic network to generate a richer signal for the actor.&lt;/p&gt;

&lt;h3 id=&quot;maddpg&quot;&gt;MADDPG&lt;/h3&gt;

&lt;p&gt;Multi-Agent DDPG (MADDPG) is a method to use separate actors and critics for each agent and train the critic in a centralised way and use the actor in execution. So each agent has one actor and one critic. The actor has access to its own action-observation data and is trained by them and the critic has access to observation and action of all agents and is trained by all of them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/maddpg.png&quot; alt=&quot;MADDPG&quot; title=&quot; source: https://arxiv.org/pdf/1706.02275.pdf &quot; /&gt;&lt;/p&gt;

&lt;p&gt;The centralised action-value function for each agent can be written as:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/maddpg2.png&quot; alt=&quot;MADDPG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And the gradient can be written as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/maddpg3.png&quot; alt=&quot;MADDPG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you see, the policy is conditioned on the observation of the agent itself, o_i, and the critic is conditioned on the full state and actions of all agents.
This separate critic for each agent allows us to have agents with different rewards, cooperative or competitive behaviors.&lt;/p&gt;

&lt;h3 id=&quot;coma&quot;&gt;COMA&lt;/h3&gt;

&lt;p&gt;The talk can be found &lt;a href=&quot;https://www.youtube.com/watch?v=3OVvjE5B9LU&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Counterfactual Multi-Agent (COMA) policy gradient is a method for cooperative multi-agent systems and uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents’ policies. In addition, to address the problem of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent’s action, while keeping the other agents’ actions fixed. The idea comes from difference rewards, in which each agent learns from a shaped reward $D_a = r(s, u) − r(s,(u^{-a}, c_a))$ that compares the global reward to the reward received when the action of agent $a$ is replaced with a default action $c_a$.&lt;/p&gt;

&lt;p&gt;COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/coma.png&quot; alt=&quot;COMA&quot; title=&quot; source: https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/17193 &quot; /&gt;&lt;/p&gt;

&lt;p&gt;For each agent $a$, we can then compute an advantage function that compares the Q-value for the current action $u^a$ to a counterfactual baseline that marginalizes out $u^a$, while keeping the other agents’ actions $u^{-a}$ fixed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts_images/rl-series/coma2.png&quot; alt=&quot;COMA&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In contrast to MADDPG, COMA is an on-policy approach and has only one critic network.&lt;/p&gt;</content><author><name></name></author><summary type="html">Online Reinforcement Learning</summary></entry><entry><title type="html">AlphaGo - Mastering the game of Go with deep neural networks and tree search</title><link href="https://kargarisaac.github.io/blog/jupyter/2020/04/12/AlphaGo.html" rel="alternate" type="text/html" title="AlphaGo - Mastering the game of Go with deep neural networks and tree search" /><published>2020-04-12T00:00:00-05:00</published><updated>2020-04-12T00:00:00-05:00</updated><id>https://kargarisaac.github.io/blog/jupyter/2020/04/12/AlphaGo</id><content type="html" xml:base="https://kargarisaac.github.io/blog/jupyter/2020/04/12/AlphaGo.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-12-AlphaGo.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_0.png?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_1.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_2.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_3.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_4.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_5.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_6.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_7.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/kargarisaac/blog/blob/master/_notebooks/my_icons/alphago/alphago_8.jpeg?raw=1&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;That’s it for the first one. In the next post, I will review the AlphaGo Zero paper.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Isaac Kargar</name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://kargarisaac.github.io/blog/_notebooks/my_icons/alphago/alphago_0.png" /><media:content medium="image" url="https://kargarisaac.github.io/blog/_notebooks/my_icons/alphago/alphago_0.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>