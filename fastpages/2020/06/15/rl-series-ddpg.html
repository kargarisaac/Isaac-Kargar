<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>RL Series - DDPG | Isaac Kargar</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="RL Series - DDPG" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this post, I talk about DDPG algorithm which is an off-policy RL algorithm for continous action spaces." />
<meta property="og:description" content="In this post, I talk about DDPG algorithm which is an off-policy RL algorithm for continous action spaces." />
<link rel="canonical" href="https://kargarisaac.github.io/blog/fastpages/2020/06/15/rl-series-ddpg.html" />
<meta property="og:url" content="https://kargarisaac.github.io/blog/fastpages/2020/06/15/rl-series-ddpg.html" />
<meta property="og:site_name" content="Isaac Kargar" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-15T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"In this post, I talk about DDPG algorithm which is an off-policy RL algorithm for continous action spaces.","@type":"BlogPosting","headline":"RL Series - DDPG","url":"https://kargarisaac.github.io/blog/fastpages/2020/06/15/rl-series-ddpg.html","datePublished":"2020-06-15T00:00:00-05:00","dateModified":"2020-06-15T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://kargarisaac.github.io/blog/fastpages/2020/06/15/rl-series-ddpg.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://kargarisaac.github.io/blog/feed.xml" title="Isaac Kargar" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>RL Series - DDPG | Isaac Kargar</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="RL Series - DDPG" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this post, I talk about DDPG algorithm which is an off-policy RL algorithm for continous action spaces." />
<meta property="og:description" content="In this post, I talk about DDPG algorithm which is an off-policy RL algorithm for continous action spaces." />
<link rel="canonical" href="https://kargarisaac.github.io/blog/fastpages/2020/06/15/rl-series-ddpg.html" />
<meta property="og:url" content="https://kargarisaac.github.io/blog/fastpages/2020/06/15/rl-series-ddpg.html" />
<meta property="og:site_name" content="Isaac Kargar" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-15T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"In this post, I talk about DDPG algorithm which is an off-policy RL algorithm for continous action spaces.","@type":"BlogPosting","headline":"RL Series - DDPG","url":"https://kargarisaac.github.io/blog/fastpages/2020/06/15/rl-series-ddpg.html","datePublished":"2020-06-15T00:00:00-05:00","dateModified":"2020-06-15T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://kargarisaac.github.io/blog/fastpages/2020/06/15/rl-series-ddpg.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://kargarisaac.github.io/blog/feed.xml" title="Isaac Kargar" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Isaac Kargar</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">RL Series - DDPG</h1><p class="page-description">In this post, I talk about DDPG algorithm which is an off-policy RL algorithm for continous action spaces.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-06-15T00:00:00-05:00" itemprop="datePublished">
        Jun 15, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>This is part of my <a href="https://medium.com/@kargarisaac/rl-series-implementation-in-pytorch-bbeedb033866">RL-series</a> posts.</p>

<p>This algorithm is from the “Continuous Control with Deep Reinforcement Learning” paper and uses the ideas from deep q-learning in the continuous action domain and is a model-free method based on the deterministic policy gradient.</p>

<p>In Deterministic Policy Gradient (DPG), for each state, we have one clearly defined action to take (the output of policy is one value for action and for exploration we add a noise, normal noise for example, to the action). But in Stochastic Gradient Descent, we have a distribution over actions (the output of policy is mean and variance of a normal distribution) and sample from that distribution to get the action, for exploration. In another term, in stochastic policy gradient, we have a distribution with mean and variance and we draw a sample from that as an action. When we reduce the variance to zero, the policy will be deterministic.</p>

<p>When the action space is discrete, such as q-learning, we get the max over q-values of all actions and select the best action. But in continuous action spaces, you cannot apply q-learning directly, because in continuous spaces finding the greedy policy requires optimization of a_t at every time-step and would be too slow for large networks and continuous action spaces. Based on the proposed equation in the reference paper, here we approximate max Q(s, a) over actions with Q(a, µ(s)).</p>

<p>In DDPG, they used function approximators, neural nets, for both action-value function Q and deterministic policy function µ. In addition, DDPG uses some techniques for stabilizing training, such as updating the target networks using soft updating for both μ and Q. It also uses batch normalization layers, noise for exploration, and a replay buffer to break temporal correlations.</p>

<p>This algorithm is an actor-critic method and the network structure is as follows:</p>

<p><img src="../assets/posts_images/ddpg_post/ddpg_diagram.jpg" alt="DDPG diagram" /></p>

<p>First, the policy network gets the state and outputs the action mean vector. This will be a vector of mean values for different actions. For example, in a self-driving car, there are two continuous actions: steering and acceleration&amp;braking (one continuous value between -x to x, the negative values are for braking and positive values are for acceleration). So we will have two mean for these two actions. To consider exploration, we can use Ornstein-Uhlenbeck or normal noise and add it to the action mean vector in the training phase. In the test phase, we can use the mean vector directly without any added noise. Then this action vector will be concatenated with observation and fed into the Q network. The output of the Q network will be one single value as a state-action value. In DQN, because it had discrete action space, we had multiple state-action values for each action, but here because the action space is continuous, we feed the actions into the Q network and get one single value as the state-action value.</p>

<p>Finally, the sudo code for DDPG is as follows:</p>

<p><img src="../assets/posts_images/ddpg_post/ddpg_algorithm.jpg" alt="DDPG algorithm" /></p>

<p>To understand the algorithm better, it’s good to try to implement it and play with its parameters and test it in different environments. Here is a good implementation in PyTorch that you can start with <a href="https://github.com/higgsfield/RL-Adventure-2/blob/master/5.ddpg.ipynb">this</a>.</p>

<p>I also found the Spinningup implementation of DDPG very clear and understandable too. You can find it <a href="https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ddpg/ddpg.py">here</a></p>

<p>For POMDP problems, it is possible to use LSTMs or any other RNN layers to get a sequence of observations. It needs a different type of replay buffer which I will write another post about that.</p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="kargarisaac/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/fastpages/2020/06/15/rl-series-ddpg.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My posts about Machine Learning</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/kargarisaac" title="kargarisaac"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/kargarisaac" title="kargarisaac"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
