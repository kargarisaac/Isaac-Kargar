<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Model Based Reinforcement Learning (MBRL) | Isaac Kargar</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Model Based Reinforcement Learning (MBRL)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is a summary of MBRL from ICML-2020 tutorial." />
<meta property="og:description" content="This is a summary of MBRL from ICML-2020 tutorial." />
<link rel="canonical" href="https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html" />
<meta property="og:url" content="https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html" />
<meta property="og:site_name" content="Isaac Kargar" />
<meta property="og:image" content="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-26T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"This is a summary of MBRL from ICML-2020 tutorial.","@type":"BlogPosting","headline":"Model Based Reinforcement Learning (MBRL)","url":"https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html","datePublished":"2020-10-26T00:00:00-05:00","dateModified":"2020-10-26T00:00:00-05:00","image":"https://kargarisaac.github.io/blog/images/some_folder/your_image.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://kargarisaac.github.io/blog/feed.xml" title="Isaac Kargar" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Model Based Reinforcement Learning (MBRL) | Isaac Kargar</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Model Based Reinforcement Learning (MBRL)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is a summary of MBRL from ICML-2020 tutorial." />
<meta property="og:description" content="This is a summary of MBRL from ICML-2020 tutorial." />
<link rel="canonical" href="https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html" />
<meta property="og:url" content="https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html" />
<meta property="og:site_name" content="Isaac Kargar" />
<meta property="og:image" content="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-26T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"This is a summary of MBRL from ICML-2020 tutorial.","@type":"BlogPosting","headline":"Model Based Reinforcement Learning (MBRL)","url":"https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html","datePublished":"2020-10-26T00:00:00-05:00","dateModified":"2020-10-26T00:00:00-05:00","image":"https://kargarisaac.github.io/blog/images/some_folder/your_image.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://kargarisaac.github.io/blog/fastpages/jupyter/2020/10/26/mbrl.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://kargarisaac.github.io/blog/feed.xml" title="Isaac Kargar" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Isaac Kargar</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Model Based Reinforcement Learning (MBRL)</h1><p class="page-description">This is a summary of MBRL from ICML-2020 tutorial.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-10-26T00:00:00-05:00" itemprop="datePublished">
        Oct 26, 2020
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      15 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/kargarisaac/blog/tree/master/_notebooks/2020-10-26-mbrl.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/kargarisaac/blog/master?filepath=_notebooks%2F2020-10-26-mbrl.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/kargarisaac/blog/blob/master/_notebooks/2020-10-26-mbrl.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Introduction-and-Motivation">Introduction and Motivation </a></li>
<li class="toc-entry toc-h2"><a href="#Problem-Statement">Problem Statement </a></li>
<li class="toc-entry toc-h2"><a href="#What-is-a-model?">What is a model? </a></li>
<li class="toc-entry toc-h2"><a href="#How-to-use-model?">How to use model? </a></li>
<li class="toc-entry toc-h2"><a href="#How-to-learn-a-model?">How to learn a model? </a>
<ul>
<li class="toc-entry toc-h3"><a href="#state-transition-models">state-transition models </a></li>
<li class="toc-entry toc-h3"><a href="#observation-transition-models">observation-transition models </a></li>
<li class="toc-entry toc-h3"><a href="#latent-state-transition-models">latent state-transition models </a></li>
<li class="toc-entry toc-h3"><a href="#Structured-latent-state-transition-models">Structured latent state-transition models </a></li>
<li class="toc-entry toc-h3"><a href="#Recurrent-value-models">Recurrent value models </a></li>
<li class="toc-entry toc-h3"><a href="#Non-Parametric-models">Non-Parametric models </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Model-based-control-and-how-to-use-a-model?">Model-based control and how to use a model? </a>
<ul>
<li class="toc-entry toc-h3"><a href="#How-models-are-used-in-reinforcement-learning?">How models are used in reinforcement learning? </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Simulating-the-environment">Simulating the environment </a></li>
<li class="toc-entry toc-h4"><a href="#Assisting-the-learning-algorithm">Assisting the learning algorithm </a></li>
<li class="toc-entry toc-h4"><a href="#Strengthening-the-policy">Strengthening the policy </a>
<ul>
<li class="toc-entry toc-h5"><a href="#What-is-the-difference-between-background-and-decision-time-planning?">What is the difference between background and decision-time planning? </a></li>
<li class="toc-entry toc-h5"><a href="#What-is-the-difference-between-discrete-and-continuous-planning?">What is the difference between discrete and continuous planning? </a>
<ul>
<li class="toc-entry toc-h6"><a href="#MCTS-(monte-carlo-tree-search)">MCTS (monte carlo tree search) </a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-10-26-mbrl.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This post is a summary of the model-based RL tutorial at ICML-2020. You can find the videos <a href="https://sites.google.com/view/mbrl-tutorial">here</a>. The pictures are from the slides in the talk.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction-and-Motivation">
<a class="anchor" href="#Introduction-and-Motivation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction and Motivation<a class="anchor-link" href="#Introduction-and-Motivation"> </a>
</h2>
<p>Having access to a model of the world and using it for decision making is a powerful idea. 
There are a lot of applications of MBRL in different areas like robotics (manipulation- what will happen by doing an action), 
self-driving cars (having a model of other agents decisions and future motions and act accordingly),
games (AlphaGo- search over different possibilities), Science ( chemical usecases),
and peration research and energy applications (how to allocate renewable energy in different points in time to meet the demand).</p>
<h2 id="Problem-Statement">
<a class="anchor" href="#Problem-Statement" aria-hidden="true"><span class="octicon octicon-link"></span></a>Problem Statement<a class="anchor-link" href="#Problem-Statement"> </a>
</h2>
<p>In sequential decision making, the agent will interact with the world by doing action $a$ and getting the next state $s$ and reward $r$.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/rl.png" alt="">
    
    
</figure>
</p>
<p>We can write this problem as a Markov Decision Process (MDP) as follows:</p>
<ul>
<li>States $S \epsilon R^{d_S}$</li>
<li>Actions $A \epsilon R^{d_A}$</li>
<li>Reward function $R: S \times A \rightarrow R$</li>
<li>Transition function $T: S \times A \rightarrow S$</li>
<li>Discount $\gamma \epsilon (0,1)$</li>
<li>Policy $\pi: S \rightarrow A$</li>
</ul>
<p>The goal is to find a policy which maximizes the sum of discounted future rewards:
$$
argmax_{\pi} \sum_{t=0}^\infty \gamma^t R(s_t, a_t)
$$
subject to
$$
a_t = \pi(s_t) , s_{t+1}=T(s_t, a_t)
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>How to solve this optimization problem?!</p>
<ul>
<li>Collect data $D= \{ s_t, a_t, r_{t+1}, s_{t+1} \}_{t=0}^T$.</li>
<li>Model-free: learn policy directly from data</li>
</ul>
<p>$ D \rightarrow \pi$ e.g. Q-learning, policy gradient</p>
<ul>
<li>Model-based: learn model, then use it to <strong>learn</strong> or <strong>improve</strong> a policy </li>
</ul>
<p>$ D \rightarrow f \rightarrow \pi$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-a-model?">
<a class="anchor" href="#What-is-a-model?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is a model?<a class="anchor-link" href="#What-is-a-model?"> </a>
</h2>
<p>a model is a representation that explicitly encodes knowledge about the structure of the environment and task.</p>
<p>This model can take a lot of different forms:</p>
<ul>
<li>A transition/dynamic model: $s_{t+1} = f_s(s_t, a_t)$</li>
<li>A model of rewards: $r_{t+1} = f_r(s_t, a_t)$</li>
<li>An inverse transition/dynamics model (which tells you what is the action to take and go from one state to the next state): $a_t = f_s^{-1}(s_t, s_{t+1})$</li>
<li>A model of distance of two states: $d_{ij} = f_d(s_i, s_j)$</li>
<li>A model of future returns: $G_t = Q(s_t, a_t)$ or $G_t = V(s_t)$</li>
</ul>
<p>Typically when someone says MBRL, he/she means the firs two items.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/model.png" alt="">
    
    
</figure>
</p>
<p>Sometimes we know the ground truth dynamics and rewards. Might as well use them! Like game environments or simulators like Mujoco, Carla, and so on.</p>
<p>But we don't have access to the model in all cases, so we need to learn the model. In cases like in robots, complex physical dynamics, and interaction with humans.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-to-use-model?">
<a class="anchor" href="#How-to-use-model?" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to use model?<a class="anchor-link" href="#How-to-use-model?"> </a>
</h2>
<p>In model-free RL agent we have a policy and learning algorithm like the figure below:</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/rl2.png" alt="">
    
    
</figure>
</p>
<p>In model-based RL we can use the model in three different ways:</p>
<ul>
<li>simulating the environment: replacing the environment with model and use it to generate data and use it to update the policy.</li>
<li>Assisting the learning algorithm: modify the learning algorithm to use the model to interpret the data it is getting in a different way. </li>
<li>Strengthening the policy: allow the agent at test time to use the model to try out different actions before it commits to one of them (taking the action in the real world).</li>
</ul>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbrl.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In general, to compare model-free and model-based:</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbrl_vs_mfrl.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-to-learn-a-model?">
<a class="anchor" href="#How-to-learn-a-model?" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to learn a model?<a class="anchor-link" href="#How-to-learn-a-model?"> </a>
</h2>
<p>There are two different dimensions that are useful to pay attention to:</p>
<ul>
<li>
<p>representation of the features for the states that the model is being learned over them</p>
</li>
<li>
<p>representation of the transition between states</p>
</li>
</ul>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In continue we take a look at different transition models.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="state-transition-models">
<a class="anchor" href="#state-transition-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>state-transition models<a class="anchor-link" href="#state-transition-models"> </a>
</h3>
<p>In some cases, we know equations of motion and dynamics but we don't know the exact parameters like mass. We can use system identification to estimate unknown parameters like mass. But these sort of cases require having a lot of domain knowledge about how exactly the system works.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model2.png" alt="">
    
    
</figure>

<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model3.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In some cases that we don't know the dynamics of motion, we can simply use an MLP to get a concatenation of $s_t, a_t$ and output the next state $s_{t+1}$.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model4.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In cases that we have some, not perfect, domain knowledge about the environment, we can use graph neural networks (GNNs) to model the agent (robot). For example in Mujoco we can model a robot (agent) with nodes as its body parts and edges as joint and learn the physics engine.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model5.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="observation-transition-models">
<a class="anchor" href="#observation-transition-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>observation-transition models<a class="anchor-link" href="#observation-transition-models"> </a>
</h3>
<p>In this cases, we don't have access to states (low level states like joint angles), but we have access to images. The MDP for this cases would be like this:</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model6.png" alt="">
    
    
</figure>
</p>
<p>So what can we do with this?</p>
<ul>
<li>Directly predict transitions between observations (observation-transition models)</li>
</ul>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model7.png" alt="">
    
    
</figure>
</p>
<ul>
<li>Reconstruct observation at every timestep: Using sth like LSTMs. Here we need to reconstruct the whole observation in each timestep. The images can be blurry in these cases.</li>
</ul>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model8.png" alt="">
    
    
</figure>
</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model88.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="latent-state-transition-models">
<a class="anchor" href="#latent-state-transition-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>latent state-transition models<a class="anchor-link" href="#latent-state-transition-models"> </a>
</h3>
<p>Another option when we have just access to observation is to instead of making transition between observations we can infere a latent state and then make transitions in that latent space (latent state-transition models) not in the observation space. It would be much faster than reconstructing the observation on every timestep. We take our initial observation or perhaps the last couple of observations and embed them into the latent state and then unroll it in time and do predictions in $z$ instead of $o$.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model9.png" alt="">
    
    
</figure>
</p>
<p>Usually we use the observation and reconstruct it during training but at test time we can unroll it very quickly. we can also reconstruct observation at each timestep we want (not necessarily in all timesteps).</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model10.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Structured-latent-state-transition-models">
<a class="anchor" href="#Structured-latent-state-transition-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Structured latent state-transition models<a class="anchor-link" href="#Structured-latent-state-transition-models"> </a>
</h3>
<p>Another thing that you can do if you have a little bit more domain knowledge is to add a little bit of structure into your latent state. For example, if you know that the scene that you are trying to model consists of objects, then you can try to actually explicitly detect those objects, segment them out and then learn those transitions between objects.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model11.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Recurrent-value-models">
<a class="anchor" href="#Recurrent-value-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Recurrent value models<a class="anchor-link" href="#Recurrent-value-models"> </a>
</h3>
<p>The idea is that when you unroll your latent-state, you additionally predict the value of the state at each point of the furture, in addition to reward. we can train the model without necessarily needing to train using observations, but just training it by predicting the value progressing toward actual observed values when you roll it out in the real environment.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model12.png" alt="">
    
    
</figure>
</p>
<p>why this is useful? because some types of planners actually only need you to predict values rather than predicting states lime MCTS (monte carlo tree search).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Non-Parametric-models">
<a class="anchor" href="#Non-Parametric-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Non-Parametric models<a class="anchor-link" href="#Non-Parametric-models"> </a>
</h3>
<p>So far we talked about parametric ways of learning the model. We can also use non-parametric methods like graphs.</p>
<p>For example the replay buffer that we use in off-policy methods can be seen as an approximation to a type of model where if you have enough data in your replay buffer then you sample from buffer, it basically access the density model over your transitions. You can use extra replay to basically get the same level performances you would get using a model based method which learns a parametric model.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model13.png" alt="">
    
    
</figure>
</p>
<p>The other work we can do using data in buffer is to use data points and learn the transition between them and interpolate to find states between those states in buffer. Somehow learning a distribution and use it to generate new datapoints.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model14.png" alt="">
    
    
</figure>
</p>
<p>Another form of non-parametric transition is a symbolic description which is popular in the planning community not in the deep learning community.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model15.png" alt="">
    
    
</figure>
</p>
<p>The other form of non-parametric models is gaussian processes which gives us strong predictions using very very small amout of data. PILCO is one example of these algorithms.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/learn_model16.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Model-based-control-and-how-to-use-a-model?">
<a class="anchor" href="#Model-based-control-and-how-to-use-a-model?" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model-based control and how to use a model?<a class="anchor-link" href="#Model-based-control-and-how-to-use-a-model?"> </a>
</h2>
<p>We will be using this landscape of various methods and categories that exist, including some representative algorithms:</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc1.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="How-models-are-used-in-reinforcement-learning?">
<a class="anchor" href="#How-models-are-used-in-reinforcement-learning?" aria-hidden="true"><span class="octicon octicon-link"></span></a>How models are used in reinforcement learning?<a class="anchor-link" href="#How-models-are-used-in-reinforcement-learning?"> </a>
</h3>
<p>As we saw earlier, we can use the model in three different ways. In continue, we will see some examples of each case.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc.png" alt="">
    
    
</figure>
</p>
<h4 id="Simulating-the-environment">
<a class="anchor" href="#Simulating-the-environment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Simulating the environment<a class="anchor-link" href="#Simulating-the-environment"> </a>
</h4>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc2.png" alt="">
    
    
</figure>
</p>
<p>One way is to mix the real data with model-generated experience and then apply traditional model-free algorithms like Q-learning, policy gradient, etc. In this cases, the model offfers a larger and augmented training dataset.</p>
<p><strong>Dyna-Q</strong> is an example that uses Q-learning with a learned model. Dyna does the traditional Q-learning updates on real transitions, and also use a model to create fictitious imaginary transitions from the real states and perform exactly the same Q-learning updates on those. So it's basically just a way to augment the experience.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc3.png" alt="">
    
    
</figure>
</p>
<p>This can also be applied in policy learning. We don't need to perform just a single step but multiple steps according to the <strong>model</strong> to generate experience even further away from the real data and do policy parameter updates entirely on this fictitious experiences.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc4.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Assisting-the-learning-algorithm">
<a class="anchor" href="#Assisting-the-learning-algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>Assisting the learning algorithm<a class="anchor-link" href="#Assisting-the-learning-algorithm"> </a>
</h4>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc5.png" alt="">
    
    
</figure>
</p>
<p>One important way that this can be done is to allow end-to-end training through our models. End-to-end training recently has been very successfull in improving and simplifying supervised learning methods in computer vision, NLP, etc.</p>
<p>The question is "can we apply the same type of end-to-end approaches to RL?"</p>
<p>One example is just the policy gradient algorithm. Let's say we want to maximize the sum of discounted future reward of some parametric policy. We can write the objective function with respect to the policy parameters $\theta$</p>
$$
 J(\theta) = \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t)  , \quad a_t = \pi_{\theta}(s_t) , \quad s_{t+1} = T(s_t, a_t)
$$<p>Now we need to apply gradient ascent (for maximization) on policy gradient with respect to policy parameters $\theta  \rightarrow  \nabla_{\theta}J$.</p>
<p>So how can we calculate this $\nabla_{\theta}J$ ?</p>
<p>sampling-based methods have been proposed like REINFORCE to estimate this gradient. But the problem with them is that they cannot have a very high variance and they often require the policy to have some randomness to it to make decisions. This can be unfavorable.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc6.png" alt="">
    
    
</figure>
</p>
<p>Accurate and smooth models, aside from imaginary experiences, offer derivatives:</p>
$$
s_{t+1} = f_s(s_t, a_t) \quad  r_t = f_r(s_t, a_t)
$$$$
\nabla_{s_t}(s_{t+1}), \quad \nabla_{a_t}(s_{t+1}), \quad \nabla_{s_t}(r_t), \quad \nabla_{a_t}(r_t), \quad ...
$$<p>And they are able to answer questions such as: <em>how do small changes in action change next state or reward any of other quantities?</em></p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc7.png" alt="">
    
    
</figure>
</p>
<p>Why is this useful? This is useful because it will allow us to do this type end-to-end differentiation algorithms like <strong>back-propagation</strong>.</p>
<p>Let's rewrite our objective function using models:</p>
$$
 J(\theta) \approx \sum_{t=0}^{H} \gamma^t r_t  , \quad a_t = \pi_{\theta}(s_t) , \quad s_{t+1} = f_s(s_t, a_t), \quad r_t=f_r(s_t,a_t)
$$<p>So how we can use this derivatives to calculate $\nabla_{\theta}J$ ?</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc8.png" alt="">
    
    
</figure>
</p>
<p>The highlighted derivatives are easy to calculate using some libraries like PyTprch or TensorFlow.</p>
<p>By calculating $\nabla_{\theta}J$ in this way:</p>
<p><strong>pros</strong>:</p>
<ul>
<li>The policy gradient that we get is actually a deterministic quantity and there is no variance to it. </li>
<li>It can support potentially much longer term credit assignment</li>
</ul>
<p><strong>cons</strong>:</p>
<ul>
<li>It is prone to local minima</li>
<li>Poor conditioning (vanishing/exploding gradients)</li>
</ul>
<p>Here are two examples to use model-based back-propagation (derivatives) either along real or model-generated ttrajectories to do end to end training:</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc9.png" alt="">
    
    
</figure>
</p>
<ul>
<li>real trajectories are safer, but need to be from the current policy parameters (so itâ€™s less sample-efficient)</li>
<li>model-generated trajectories allow larger policy changes without interacting with the real world, but might suffer more from model inaccuracies</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Strengthening-the-policy">
<a class="anchor" href="#Strengthening-the-policy" aria-hidden="true"><span class="octicon octicon-link"></span></a>Strengthening the policy<a class="anchor-link" href="#Strengthening-the-policy"> </a>
</h4>
<p>So far we talked about the first two ways of using a model in RL. These two ways are in category of <strong>Background Planning</strong>.</p>
<p>There is another category based on the <em>Sutton and Barto (2018)- Reinforcement Learning: An Introduction</em> categorization, called <strong>Decision-Time Planning</strong>, which is a unique option we have available in model-based settings.</p>
<h5 id="What-is-the-difference-between-background-and-decision-time-planning?">
<a class="anchor" href="#What-is-the-difference-between-background-and-decision-time-planning?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is the difference between background and decision-time planning?<a class="anchor-link" href="#What-is-the-difference-between-background-and-decision-time-planning?"> </a>
</h5>
<p>In background planning, we can think of it as answering the question "how do I learn how to act in any possible situation to succeed and reach the goal?"</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc10.png" alt="">
    
    
</figure>
</p>
<ul>
<li>The optimization variables are parameters of a policy or value function or ... and are trained using expectation over all possible situations.</li>
<li>Conceptually we can think of background planning as learning a set of habits that I could just reuse.</li>
<li>We can think of background planning as learning fast type of thinking.</li>
</ul>
<p>In decision-time planning, we want to answer the question "what is the best sequence of actions just for my current situation to succeed or reach the goal?"</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc11.png" alt="">
    
    
</figure>
</p>
<ul>
<li>The optimization parameters are just a sequence of actions or states.</li>
<li>Conceptually we can think of decision-time planning as finding our consciously improvising just for the particular situation that we find ourself in.</li>
<li>We can think of decision-time planning as learning slow type of thinking.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why use one over the other?</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc12.png" alt="">
    
    
</figure>
</p>
<ul>
<li>
<p><em>Act on most recent state of the world</em>: decision-time planning is just concerned about the current state in finding the sequence of actions. You can act based on the most recent state of the world. By contrast, in background planning the habits may be stale and might take a while to get updated as the world's changes.</p>
</li>
<li>
<p><em>Act without any learning</em>: decision-time planning allows us to act without any learning at all. There is no need for policy or value networks before we can start making decisions. It is just an optimization problem as long as you have the model.</p>
</li>
<li>
<p><em>Competent in unfamiliar situations</em>: if you find yourself in situations that are far away from where you were training, your set of habits or policy network might not have competence (the ability to do something successfully or efficiently) there. So you don't have any information to act or are very uncertain or even in the worst case, it will with confidence make decisions that just potentionally make no sense. This is out of distribution and generalization problem. In these cases the decision-time planning would be more beneficial.</p>
</li>
<li>
<p><em>Independent of observation space</em>: another advantage of decision-time planning is that it is also independent of the observation space that you decide on. In background methods we need to consider some encoding or description of the state, joint angles or pixels or graphs, into our policy function. This decisions may play a large role on the total learning performance. When something is not working, you will not really know that is it because of the algorithm, or state space which doesn't contain enough information. In contrast, decision-time planning avoid this confounded, which in practice can be actually quite useful whn you're prototyping new methods.</p>
</li>
</ul>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc13.png" alt="">
    
    
</figure>
</p>
<ul>
<li>
<p><em>Partial observability</em>: decision-time plannings have some issues with it. They assume that you know the full state of the world when you're making the plan. So it's hard to hide information from decision-time planners. It is possible but it is more costly.</p>
</li>
<li>
<p><em>Fast computation at deployment</em>: decision-time planners require more computation. It is not just evaluating a habbit, but it needs more thinking.</p>
</li>
<li>
<p><em>Predictability and coherence</em>: decision-time planners do some actions which are not necessarily predictable or coherent. Because you are consciously thinking about each foot step, you might not have a plan that's exactly the same. So you may have a very chaotic behavior that still succeeds. In contrast, the background planning, because it learns a set of habbits, it can perform a very regular behavior.</p>
</li>
<li>
<p><em>Same for discrete and contiinuous actions</em>: background planning has a very unified treatment of discrete and continuous actions which is conceptually simpler. In decision-time planning, there are different algorithms for discrete and continuous actions. we will see in the following sections more about them.</p>
</li>
</ul>
<p>We can mix and match background and decision-time plannings.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="What-is-the-difference-between-discrete-and-continuous-planning?">
<a class="anchor" href="#What-is-the-difference-between-discrete-and-continuous-planning?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is the difference between discrete and continuous planning?<a class="anchor-link" href="#What-is-the-difference-between-discrete-and-continuous-planning?"> </a>
</h5>
<p>It depends on the problem which you want to solve. So it is not a choice that you can make. For example in controlling a robot, the actions might be the torques for the motors (continuous), or in biomechanical settings it might be muscle excitations (continuous), or in medical problems the treatment that should be applied (discrete).</p>
<p>The distinction between discrete and continuous actions is not significant for background planning methods.</p>
<ul>
<li>You just learn a stochastic policies that sample either from discrete or continuous distributions.</li>
</ul>
$$
a \sim \pi(.|s) \quad \leftarrow Gaussian, categorical, ...
$$<ul>
<li>Backpropagation is still possible via some reparametrization techniques. See <em>Jang et al (2016). Categorical reparametrization with Gumbel-Softmax</em> for an example.</li>
</ul>
<p>In either of these cases (continuous and discrete in background planning methods), because you are optimizing over expectations, your final objective and optimization problem is still smooth wrt the policy parameters.</p>
$$
J(\theta) = E_{\pi}[\sum_t r_t], \quad a_t \sim \pi(.|s_t, \theta)
$$<p>But for decision-time planning, this distinction leads to specialized methods for discrete and continuous actions: discrete search or continuous trajectory optimization.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's see some examples to be able to compare them.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc14.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h6 id="MCTS-(monte-carlo-tree-search)">
<a class="anchor" href="#MCTS-(monte-carlo-tree-search)" aria-hidden="true"><span class="octicon octicon-link"></span></a>MCTS (monte carlo tree search)<a class="anchor-link" href="#MCTS-(monte-carlo-tree-search)"> </a>
</h6>
<p>This algorithm is in discrete actions group and is used in alpha-go and alpha-zero. You keep track of Q-value, which is long term reward, for all states and actions ideally that you want to consider. And also the number of times that the state and action has been previously visited.</p>
<ol>
<li>Initialize $Q_0(s, a) = 0, N_0(s, a)=0, k=0$</li>
</ol>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc15.png" alt="">
    
    
</figure>
</p>
<ol>
<li>Expansion: Starting from the current situation and expand nodes and selecting actions according to a search policy: </li>
</ol>
<p>
$$\pi_k(s) = Q_k(s,a)$$
</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc16.png" alt="">
    
    
</figure>
</p>
<ol>
<li>Evaluation: When a new node is reached, estimate its long-term value using Monte-Carlo rollouts</li>
</ol>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc17.png" alt="">
    
    
</figure>
</p>
<ol>
<li>Backup: propagate the Q-values to parent nodes:</li>
</ol>
$$
Q_{k+1}(s, a) = \frac{Q_k(s,a) N_k(s,a) + R}{N_k(s,a)+1}
$$$$
N_{k+1}(s,a) = N_k(s,a)+1
$$<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc18.png" alt="">
    
    
</figure>
</p>
<ol>
<li>Repeat Steps 2-4 until search budget is exhausted.
$$
k = k + 1
$$</li>
</ol>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/mbc19.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="kargarisaac/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/fastpages/jupyter/2020/10/26/mbrl.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My posts about Machine Learning</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/kargarisaac" title="kargarisaac"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/eshagh-kargar" title="eshagh-kargar"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/kargarisaac" title="kargarisaac"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
