<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Data Engineering - Week 2 | Isaac Kargar</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Data Engineering - Week 2" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Week 2 - Data Engineering Zoomcamp course." />
<meta property="og:description" content="Week 2 - Data Engineering Zoomcamp course." />
<link rel="canonical" href="https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/25/data-engineering-w2.html" />
<meta property="og:url" content="https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/25/data-engineering-w2.html" />
<meta property="og:site_name" content="Isaac Kargar" />
<meta property="og:image" content="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-01-25T00:00:00-06:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/25/data-engineering-w2.html"},"description":"Week 2 - Data Engineering Zoomcamp course.","@type":"BlogPosting","url":"https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/25/data-engineering-w2.html","headline":"Data Engineering - Week 2","dateModified":"2022-01-25T00:00:00-06:00","datePublished":"2022-01-25T00:00:00-06:00","image":"https://kargarisaac.github.io/blog/images/some_folder/your_image.png","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://kargarisaac.github.io/blog/feed.xml" title="Isaac Kargar" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','G-7C8WW0BBJ4','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Data Engineering - Week 2 | Isaac Kargar</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Data Engineering - Week 2" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Week 2 - Data Engineering Zoomcamp course." />
<meta property="og:description" content="Week 2 - Data Engineering Zoomcamp course." />
<link rel="canonical" href="https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/25/data-engineering-w2.html" />
<meta property="og:url" content="https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/25/data-engineering-w2.html" />
<meta property="og:site_name" content="Isaac Kargar" />
<meta property="og:image" content="https://kargarisaac.github.io/blog/images/some_folder/your_image.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-01-25T00:00:00-06:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/25/data-engineering-w2.html"},"description":"Week 2 - Data Engineering Zoomcamp course.","@type":"BlogPosting","url":"https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/25/data-engineering-w2.html","headline":"Data Engineering - Week 2","dateModified":"2022-01-25T00:00:00-06:00","datePublished":"2022-01-25T00:00:00-06:00","image":"https://kargarisaac.github.io/blog/images/some_folder/your_image.png","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://kargarisaac.github.io/blog/feed.xml" title="Isaac Kargar" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','G-7C8WW0BBJ4','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Isaac Kargar</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Data Engineering - Week 2</h1><p class="page-description">Week 2 - Data Engineering Zoomcamp course.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-01-25T00:00:00-06:00" itemprop="datePublished">
        Jan 25, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      24 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/kargarisaac/blog/tree/master/_notebooks/2022-01-25-data-engineering-w2.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/kargarisaac/blog/master?filepath=_notebooks%2F2022-01-25-data-engineering-w2.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/kargarisaac/blog/blob/master/_notebooks/2022-01-25-data-engineering-w2.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#Data-Lake">Data Lake </a></li>
<li class="toc-entry toc-h1"><a href="#Data-Lake-vs-Data-Warehouse">Data Lake vs Data Warehouse </a></li>
<li class="toc-entry toc-h1"><a href="#ETL-vs-ELT">ETL vs ELT </a></li>
<li class="toc-entry toc-h1"><a href="#Workflow-Orchestration">Workflow Orchestration </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Airflow">Airflow </a></li>
<li class="toc-entry toc-h3"><a href="#Google-Cloud-Dataflow">Google Cloud Dataflow </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Airflow">Airflow </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Ingest-Data-to-GCS-and-BigQuery-using-Airflow">Ingest Data to GCS and BigQuery using Airflow </a></li>
<li class="toc-entry toc-h2"><a href="#Ingest-Data-to-Postgres-using-Airflow">Ingest Data to Postgres using Airflow </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-01-25-data-engineering-w2.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Note</strong>: The content of this post is from the course videos, my understandings and searches, and reference documentations.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>
</p>
<center>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/W3Zm6rjOq70" frameborder="0" allowfullscreen=""></iframe>
</center>


</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Data-Lake">
<a class="anchor" href="#Data-Lake" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Lake<a class="anchor-link" href="#Data-Lake"> </a>
</h1>
<p><img src="/blog/images/copied_from_nb/images/data-engineering-w2/1.png" alt=""></p>
<p>A data lake is a collection of technologies that enables querying of data contained in files or blob objects. When used effectively, they enable massive scale and cost-effective analysis of structured and unstructured data assets [<a href="https://lakefs.io/data-lakes/">source</a>].</p>
<p>Data lakes are comprised of four primary components: storage, format, compute, and metadata layers [<a href="https://lakefs.io/data-lakes/">source</a>].</p>
<p><img src="/blog/images/copied_from_nb/images/data-engineering-w2/2.png" alt=""></p>
<p>A data lake is a centralized repository for large amounts of data from a variety of sources. Data can be structured, semi-structured, or unstructured in general.
The goal is to rapidly ingest data and make it available to or accessible to other team members such as data scientists, analysts, and engineers.
The data lake is widely used for machine learning and analytical solutions.
Generally, when you store data in a data lake, you associate it with some form of metadata to facilitate access. Generally, a data lake solution must be secure and scalable.
Additionally, the hardware should be affordable. The reason for this is that you want to store as much data as possible quickly.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Data-Lake-vs-Data-Warehouse">
<a class="anchor" href="#Data-Lake-vs-Data-Warehouse" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Lake vs Data Warehouse<a class="anchor-link" href="#Data-Lake-vs-Data-Warehouse"> </a>
</h1>
<p><img src="/blog/images/copied_from_nb/images/data-engineering-w2/3.png" alt=""></p>
<p>Generally a data lake is an unstructured data and the target users are data scientists or data analysts. It stores huge amount of data, sometimes in the size of petabytes and terabytes. The use cases which are covered by data lake are basically stream processing, machine learning, and real-time analytics.
On the data warehouse side, the data is generally structured. The users are business analysts, the data size is generally small, and the use case consists of batch processing or BI reporting.</p>
<p>To read more, please check <a href="https://lakefs.io/data-lakes/">here</a> and <a href="https://luminousmen.com/post/data-lake-vs-data-warehouse">here</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="ETL-vs-ELT">
<a class="anchor" href="#ETL-vs-ELT" aria-hidden="true"><span class="octicon octicon-link"></span></a>ETL vs ELT<a class="anchor-link" href="#ETL-vs-ELT"> </a>
</h1>
<ul>
<li>Extract Transform and Load vs Extract Load and Transform</li>
<li>ETL is mainly used for a small amount of data whereas ELT is used for large amounts of data</li>
<li>ELT provides data lake support (Schema on read)</li>
<li>ETL provides data warehouse solutions</li>
</ul>
<p><img src="/blog/images/copied_from_nb/images/data-engineering-w2/4.png" alt="">
<em><a href="https://www.guru99.com/etl-vs-elt.html#:~:text=ETL%20stands%20for%20Extract%2C%20Transform,directly%20into%20the%20target%20system.&amp;text=ETL%2C%20ETL%20is%20mainly%20used,for%20large%20amounts%20of%20data.">source</a></em></p>
<p><img src="/blog/images/copied_from_nb/images/data-engineering-w2/5.png" alt="">
<em><a href="https://www.guru99.com/etl-vs-elt.html#:~:text=ETL%20stands%20for%20Extract%2C%20Transform,directly%20into%20the%20target%20system.&amp;text=ETL%2C%20ETL%20is%20mainly%20used,for%20large%20amounts%20of%20data.">source</a></em></p>
<p>Data lake solutions provided by main cloud providers are as follows:</p>
<ul>
<li>GCP - cloud storage</li>
<li>AWS - S3</li>
<li>AZURE - AZURE BLOB</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Workflow-Orchestration">
<a class="anchor" href="#Workflow-Orchestration" aria-hidden="true"><span class="octicon octicon-link"></span></a>Workflow Orchestration<a class="anchor-link" href="#Workflow-Orchestration"> </a>
</h1>
<p>
</p>
<center>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/0yK7LXwYeD0" frameborder="0" allowfullscreen=""></iframe>
</center>


</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We saw a simple data pipeline in week 1. One of the problems in that data pipeline was that we did several important jobs in the same place: downloading data and doing small processing and putting it into postgres. What if after downloading data, some error happens in the code or with the internet? We will lose the downloaded data and should do everything from scratch. That's why we need to do those steps separately.</p>
<p>A data pipeline is a series of steps for data processing. If the data has not yet been loaded into the data platform, it is ingested at the pipeline's start. Then there is a series of steps, each of which produces an output that serves as the input for the subsequent step. This procedure is repeated until the pipeline is completed. In some instances, independent steps may be performed concurrently. [<a href="https://hazelcast.com/glossary/data-pipeline/">source</a>].</p>
<p>A data pipeline is composed of three critical components: a source, a processing step or series of processing steps, and a destination. The destination may be referred to as a sink in some data pipelines. Data pipelines, for example, enable the flow of data from an application to a data warehouse, from a data lake to an analytics database, or to a payment processing system. Additionally, data pipelines can share the same source and sink, allowing the pipeline to focus entirely on data modification. When data is processed between points A and B (or B, C, and D), there is a data pipeline between those points [<a href="https://hazelcast.com/glossary/data-pipeline/">source</a>].</p>
<p><img src="/blog/images/copied_from_nb/images/data-engineering-w2/6.png" alt="">
<em><a href="https://hazelcast.com/glossary/data-pipeline/">source</a></em></p>
<p>In our example, the data pipeline we had in the previous week can be as follows:</p>
<p><img src="/blog/images/copied_from_nb/images/data-engineering-w2/7.png" alt=""></p>
<p>We separated downloading dataset using <code>wget</code> and then ingesting it into postgres. I think we can have even another more step for processing (changing the string to datetime in the downloaded dataset).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>But this week, we will do something more complex. Let's have a look at the data workflow.</p>
<p><img src="/blog/images/copied_from_nb/images/data-engineering-w2/8.png" alt=""></p>
<p>The above figure is called a DAG (Directed Acyclic Graph). We need to be sure that all steps are done sequentially and we can retry some of the steps if some thing happens and then go to the next step. There are some tools called workflow engines tat allow us to define these DAGs and do the data workflow orchestration:</p>
<ul>
<li>LUIGI</li>
<li>APACHE AIRFLOW (we will go for this)</li>
<li>PREFECT</li>
<li>Google Cloud Dataflow</li>
</ul>
<p>Let's get more familiar with the last two ones:</p>
<h3 id="Airflow">
<a class="anchor" href="#Airflow" aria-hidden="true"><span class="octicon octicon-link"></span></a>Airflow<a class="anchor-link" href="#Airflow"> </a>
</h3>
<p>Airflow is a platform to programmatically author, schedule and monitor workflows.
Use Airflow to author workflows as Directed Acyclic Graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.
When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative [<a href="https://airflow.apache.org/docs/apache-airflow/stable/index.html">Airflow docs</a>].</p>
<p><img src="/blog/images/copied_from_nb/images/data-engineering-w2/airflow.gif" alt="">
<em><a href="https://airflow.apache.org/docs/apache-airflow/stable/index.html">Airflow docs</a></em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Google-Cloud-Dataflow">
<a class="anchor" href="#Google-Cloud-Dataflow" aria-hidden="true"><span class="octicon octicon-link"></span></a>Google Cloud Dataflow<a class="anchor-link" href="#Google-Cloud-Dataflow"> </a>
</h3>
<p>Real-time data is generated by websites, mobile applications, IoT devices, and other workloads. All businesses make data collection, processing, and analysis a priority. However, data from these systems is frequently not in a format suitable for analysis or effective use by downstream systems. That is where Dataflow enters the picture! Dataflow is used to process and enrich batch or stream data for analysis, machine learning, and data warehousing applications.</p>
<p>Dataflow is a serverless, high-performance, and cost-effective service for stream and batch processing. It enables portability for processing jobs written in the open source Apache Beam libraries and alleviates operational burden on your data engineering teams by automating infrastructure provisioning and cluster management [<a href="https://cloud.google.com/blog/topics/developers-practitioners/dataflow-backbone-data-analytics">Google cloud docs</a>].</p>
<p><img src="/blog/images/copied_from_nb/images/data-engineering-w2/9.jpeg" alt="">
<em><a href="https://cloud.google.com/blog/topics/developers-practitioners/dataflow-backbone-data-analytics">Google cloud docs</a></em></p>
<p><a href="https://stackshare.io/stackups/airflow-vs-google-cloud-dataflow">Here</a> is a comparison between Airflow and Google cloud dataflow.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Airflow">
<a class="anchor" href="#Airflow" aria-hidden="true"><span class="octicon octicon-link"></span></a>Airflow<a class="anchor-link" href="#Airflow"> </a>
</h1>
<p>
</p>
<center>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/lqDMzReAtrw" frameborder="0" allowfullscreen=""></iframe>
</center>


</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/data-engineering-w2/10.png" alt="">
<em><a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html">Airflow architecture</a></em></p>
<p>Let's review the Airflow architecture. An Airflow installation generally consists of the following components:</p>
<ul>
<li>
<p><strong>Web server</strong>: GUI to inspect, trigger and debug the behaviour of DAGs and tasks. Available at <a href="http://localhost:8080">http://localhost:8080</a>.</p>
</li>
<li>
<p><strong>Scheduler</strong>: Responsible for scheduling jobs. Handles both triggering &amp; scheduled workflows, submits Tasks to the executor to run, monitors all tasks and DAGs, and then triggers the task instances once their dependencies are complete.</p>
</li>
<li>
<p><strong>Worker</strong>: This component executes the tasks given by the scheduler.</p>
</li>
<li>
<p><strong>Metadata database (postgres)</strong>: Backend to the Airflow environment. Used by the scheduler, executor and webserver to store state.</p>
</li>
</ul>
<p>Other components (seen in docker-compose services):</p>
<ul>
<li>
<em>redis</em>: Message broker that forwards messages from scheduler to worker.</li>
<li>
<em>flower</em>: The flower app for monitoring the environment. It is available at <a href="http://localhost:5555">http://localhost:5555</a>.</li>
<li>
<em>airflow-init</em>: initialization service (customized as per this design)</li>
</ul>
<p>Please read more about Airflow architecture <a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html#architecture-overview">here</a> before continuing the blog post.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's install airflow environment using docker.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You may need Python version 3.7+.</p>
<p>You may also need to upgrade your docker-compose version to v2.x+ (as suggested in the course - however airflow documentation suggests v1.29.1 or newer).</p>
<p>Default amount of memory available for Docker on MacOS is often not enough to get Airflow up and running. If enough memory is not allocated, it might lead to airflow webserver continuously restarting. You should at least allocate 4GB memory for the Docker Engine (ideally 8GB). You can check and change the amount of memory in Resources</p>
<p>You can also check if you have enough memory by running this command [<a href="https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html">Airflow docs</a>]:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">docker</span> <span class="n">run</span> <span class="o">--</span><span class="n">rm</span> <span class="s2">"debian:buster-slim"</span> <span class="n">bash</span> <span class="o">-</span><span class="n">c</span> <span class="s1">'numfmt --to iec $(echo $(($(getconf _PHYS_PAGES) * $(getconf PAGE_SIZE))))'</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For me, this is 16 GB:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Unable</span> <span class="n">to</span> <span class="n">find</span> <span class="n">image</span> <span class="s1">'debian:buster-slim'</span> <span class="n">locally</span>
<span class="n">buster</span><span class="o">-</span><span class="n">slim</span><span class="p">:</span> <span class="n">Pulling</span> <span class="kn">from</span> <span class="nn">library</span><span class="o">/</span><span class="n">debian</span>
<span class="mi">6552179</span><span class="n">c3509</span><span class="p">:</span> <span class="n">Pull</span> <span class="n">complete</span> 
<span class="n">Digest</span><span class="p">:</span> <span class="n">sha256</span><span class="p">:</span><span class="n">f6e5cbc7eaaa232ae1db675d83eabfffdabeb9054515c15c2fb510da6bc618a7</span>
<span class="n">Status</span><span class="p">:</span> <span class="n">Downloaded</span> <span class="n">newer</span> <span class="n">image</span> <span class="k">for</span> <span class="n">debian</span><span class="p">:</span><span class="n">buster</span><span class="o">-</span><span class="n">slim</span>
<span class="mi">16</span><span class="n">G</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If enough memory is not allocated, it might lead to airflow-webserver continuously restarting. I used <a href="https://stackoverflow.com/questions/49839028/how-to-upgrade-docker-compose-to-latest-version#:~:text=First%2C%20remove%20the%20old%20version%3A">this</a> answer to update mine. For limiting memory, it is easy to do it in mac and windows like <a href="https://stackoverflow.com/questions/44533319/how-to-assign-more-memory-to-docker-container">here</a> and for linux you can check <a href="https://phoenixnap.com/kb/docker-memory-and-cpu-limit">here</a>.</p>
<p>To deploy Airflow on Docker Compose, you should fetch <code>docker-compose.yaml</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">curl</span> <span class="o">-</span><span class="n">LfO</span> <span class="s1">'https://airflow.apache.org/docs/apache-airflow/2.2.3/docker-compose.yaml'</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This file contains several service definitions:</p>
<ul>
<li>airflow-scheduler - The scheduler monitors all tasks and DAGs, then triggers the task instances once their dependencies are complete. Behind the scenes, the scheduler spins up a subprocess, which monitors and stays in sync with all DAGs in the specified DAG directory. Once per minute, by default, the scheduler collects DAG parsing results and checks whether any active tasks can be triggered [<a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/scheduler.html">ref</a>].</li>
<li>airflow-webserver - The webserver is available at <a href="http://localhost:8080">http://localhost:8080</a>.</li>
<li>airflow-worker - The worker that executes the tasks given by the scheduler.</li>
<li>airflow-init - The initialization service.</li>
<li>flower - The <a href="https://flower.readthedocs.io/en/latest/">flower</a> app is a web based tool for monitoring the environment. It is available at <a href="http://localhost:5555">http://localhost:5555</a>.</li>
<li>postgres - The database.</li>
<li>redis - The <a href="https://redis.io/">redis</a> - broker that forwards messages from scheduler to worker.</li>
</ul>
<p>All these services allow you to run Airflow with <a href="https://airflow.apache.org/docs/apache-airflow/stable/executor/celery.html">CeleryExecutor</a>. For more information, see <a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html">Architecture Overview</a>.</p>
<p>Some directories in the container are mounted, which means that their contents are synchronized between your computer and the container.</p>
<ul>
<li>./dags - you can put your DAG files here.</li>
<li>./logs - contains logs from task execution and scheduler.</li>
<li>./plugins - you can put your custom <a href="https://airflow.apache.org/docs/apache-airflow/stable/plugins.html">plugins</a> here. Airflow has a simple plugin manager built-in that can integrate external features to its core by simply dropping files in your $AIRFLOW_HOME/plugins folder.</li>
</ul>
<p>Here is the architecture of <code>docker-compose.yaml</code> file:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">version</span><span class="p">:</span> <span class="s1">'3'</span>
<span class="n">x</span><span class="o">-</span><span class="n">airflow</span><span class="o">-</span><span class="n">common</span><span class="p">:</span>
  <span class="o">&amp;</span><span class="n">airflow</span><span class="o">-</span><span class="n">common</span>
  <span class="c1"># In order to add custom dependencies or upgrade provider packages you can use your extended image.</span>
  <span class="c1"># Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml</span>
  <span class="c1"># and uncomment the "build" line below, Then run `docker-compose build` to build the images.</span>
  <span class="n">image</span><span class="p">:</span> <span class="err">$</span><span class="p">{</span><span class="n">AIRFLOW_IMAGE_NAME</span><span class="p">:</span><span class="o">-</span><span class="n">apache</span><span class="o">/</span><span class="n">airflow</span><span class="p">:</span><span class="mf">2.2</span><span class="o">.</span><span class="mi">3</span><span class="p">}</span>
  <span class="c1"># build: .</span>
  <span class="n">environment</span><span class="p">:</span>
    <span class="o">&amp;</span><span class="n">airflow</span><span class="o">-</span><span class="n">common</span><span class="o">-</span><span class="n">env</span>
    <span class="n">AIRFLOW__CORE__EXECUTOR</span><span class="p">:</span> <span class="n">CeleryExecutor</span>
    <span class="n">AIRFLOW__CORE__SQL_ALCHEMY_CONN</span><span class="p">:</span> <span class="n">postgresql</span><span class="o">+</span><span class="n">psycopg2</span><span class="p">:</span><span class="o">//</span><span class="n">airflow</span><span class="p">:</span><span class="n">airflow</span><span class="nd">@postgres</span><span class="o">/</span><span class="n">airflow</span>
    <span class="n">AIRFLOW__CELERY__RESULT_BACKEND</span><span class="p">:</span> <span class="n">db</span><span class="o">+</span><span class="n">postgresql</span><span class="p">:</span><span class="o">//</span><span class="n">airflow</span><span class="p">:</span><span class="n">airflow</span><span class="nd">@postgres</span><span class="o">/</span><span class="n">airflow</span>
    <span class="n">AIRFLOW__CELERY__BROKER_URL</span><span class="p">:</span> <span class="n">redis</span><span class="p">:</span><span class="o">//</span><span class="p">:</span><span class="nd">@redis</span><span class="p">:</span><span class="mi">6379</span><span class="o">/</span><span class="mi">0</span>
    <span class="n">AIRFLOW__CORE__FERNET_KEY</span><span class="p">:</span> <span class="s1">''</span>
    <span class="n">AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION</span><span class="p">:</span> <span class="s1">'true'</span>
    <span class="n">AIRFLOW__CORE__LOAD_EXAMPLES</span><span class="p">:</span> <span class="s1">'true'</span>
    <span class="n">AIRFLOW__API__AUTH_BACKEND</span><span class="p">:</span> <span class="s1">'airflow.api.auth.backend.basic_auth'</span>
    <span class="n">_PIP_ADDITIONAL_REQUIREMENTS</span><span class="p">:</span> <span class="err">$</span><span class="p">{</span><span class="n">_PIP_ADDITIONAL_REQUIREMENTS</span><span class="p">:</span><span class="o">-</span><span class="p">}</span>
  <span class="n">volumes</span><span class="p">:</span>
    <span class="o">-</span> <span class="o">./</span><span class="n">dags</span><span class="p">:</span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">airflow</span><span class="o">/</span><span class="n">dags</span>
    <span class="o">-</span> <span class="o">./</span><span class="n">logs</span><span class="p">:</span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">airflow</span><span class="o">/</span><span class="n">logs</span>
    <span class="o">-</span> <span class="o">./</span><span class="n">plugins</span><span class="p">:</span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">airflow</span><span class="o">/</span><span class="n">plugins</span>
  <span class="n">user</span><span class="p">:</span> <span class="s2">"$</span><span class="si">{AIRFLOW_UID:-50000}</span><span class="s2">:0"</span>
  <span class="n">depends_on</span><span class="p">:</span>
    <span class="o">&amp;</span><span class="n">airflow</span><span class="o">-</span><span class="n">common</span><span class="o">-</span><span class="n">depends</span><span class="o">-</span><span class="n">on</span>
    <span class="n">redis</span><span class="p">:</span>
      <span class="n">condition</span><span class="p">:</span> <span class="n">service_healthy</span>
    <span class="n">postgres</span><span class="p">:</span>
      <span class="n">condition</span><span class="p">:</span> <span class="n">service_healthy</span>

<span class="n">services</span><span class="p">:</span>
  <span class="n">postgres</span><span class="p">:</span>
    <span class="o">...</span>

  <span class="n">redis</span><span class="p">:</span>
    <span class="o">...</span>

  <span class="n">airflow</span><span class="o">-</span><span class="n">webserver</span><span class="p">:</span>
    <span class="o">...</span>

  <span class="n">airflow</span><span class="o">-</span><span class="n">scheduler</span><span class="p">:</span>
    <span class="o">...</span>

  <span class="n">airflow</span><span class="o">-</span><span class="n">worker</span><span class="p">:</span>
    <span class="o">...</span>

  <span class="n">airflow</span><span class="o">-</span><span class="n">triggerer</span><span class="p">:</span>
    <span class="o">...</span>

  <span class="n">airflow</span><span class="o">-</span><span class="n">init</span><span class="p">:</span>
    <span class="o">...</span>

  <span class="n">airflow</span><span class="o">-</span><span class="n">cli</span><span class="p">:</span>
    <span class="o">...</span>
    
  <span class="n">flower</span><span class="p">:</span>
    <span class="o">...</span>

<span class="n">volumes</span><span class="p">:</span>
  <span class="n">postgres</span><span class="o">-</span><span class="n">db</span><span class="o">-</span><span class="n">volume</span><span class="p">:</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The above file uses the latest Airflow image (<a href="https://hub.docker.com/r/apache/airflow">apache/airflow</a>). If you need to install a new Python library or system library, you can build your image.</p>
<p>When running Airflow locally, you may wish to use an extended image that includes some additional dependencies - for example, you may wish to add new Python packages or upgrade the airflow providers to a newer version. This is accomplished by including a custom Dockerfile alongside your <code>docker-compose.yaml</code> file. Then, using the <code>docker-compose build</code> command, you can create your image (you need to do it only once). Additionally, you can add the <code>--build</code> flag to your <code>docker-compose</code> commands to automatically rebuild the images when other <code>docker-compose</code> commands are run. To learn more and see additional examples, visit <a href="https://airflow.apache.org/docs/docker-stack/build.html">here</a> [<a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html">Airflow docs</a>].</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Ingest-Data-to-GCS-and-BigQuery-using-Airflow">
<a class="anchor" href="#Ingest-Data-to-GCS-and-BigQuery-using-Airflow" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ingest Data to GCS and BigQuery using Airflow<a class="anchor-link" href="#Ingest-Data-to-GCS-and-BigQuery-using-Airflow"> </a>
</h2>
<p>First, there are some pre-requisites. For the sake of standardization across this tutorial's config, rename your gcp-service-accounts-credentials file to <code>google_credentials.json</code> and store it in your <code>$HOME</code> directory:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">cd</span> <span class="o">~</span> <span class="o">&amp;&amp;</span> <span class="n">mkdir</span> <span class="o">-</span><span class="n">p</span> <span class="o">~/.</span><span class="n">google</span><span class="o">/</span><span class="n">credentials</span><span class="o">/</span>
<span class="n">mv</span> <span class="o">&lt;</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">your</span><span class="o">/</span><span class="n">service</span><span class="o">-</span><span class="n">account</span><span class="o">-</span><span class="n">authkeys</span><span class="o">&gt;.</span><span class="n">json</span> <span class="o">~/.</span><span class="n">google</span><span class="o">/</span><span class="n">credentials</span><span class="o">/</span><span class="n">google_credentials</span><span class="o">.</span><span class="n">json</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In order to use airflow with GCP, we have changed the <code>docker-compose.yaml</code> file in this course as follows:</p>
<ul>
<li>instead of using the official airflow image as the base image, we use a custom docker file to build and start from.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>  <span class="n">build</span><span class="p">:</span>
    <span class="n">context</span><span class="p">:</span> <span class="o">.</span>
    <span class="n">dockerfile</span><span class="p">:</span> <span class="o">./</span><span class="n">Dockerfile</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>disable loading the DAG examples that ship with Airflow. It’s good to get started, but you probably want to set this to False in a production environment</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">AIRFLOW__CORE__LOAD_EXAMPLES</span><span class="p">:</span> <span class="s1">'false'</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>add GCP environment variables (you need to use your own gcp project id and the gcs bucket you created in the previous week)</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">GOOGLE_APPLICATION_CREDENTIALS</span><span class="p">:</span> <span class="o">/.</span><span class="n">google</span><span class="o">/</span><span class="n">credentials</span><span class="o">/</span><span class="n">google_credentials</span><span class="o">.</span><span class="n">json</span>
<span class="n">AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT</span><span class="p">:</span> <span class="s1">'google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json'</span>
<span class="n">GCP_PROJECT_ID</span><span class="p">:</span> <span class="s1">'pivotal-surfer-336713'</span>
<span class="n">GCP_GCS_BUCKET</span><span class="p">:</span> <span class="s2">"dtc_data_lake_pivotal-surfer-336713"</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>add the folder we created at the beginning of the post for google credentials.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">-</span> <span class="o">~/.</span><span class="n">google</span><span class="o">/</span><span class="n">credentials</span><span class="o">/</span><span class="p">:</span><span class="o">/.</span><span class="n">google</span><span class="o">/</span><span class="n">credentials</span><span class="p">:</span><span class="n">ro</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here is the beginning of the file after our modifications:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>  <span class="n">build</span><span class="p">:</span>
    <span class="n">context</span><span class="p">:</span> <span class="o">.</span>
    <span class="n">dockerfile</span><span class="p">:</span> <span class="o">./</span><span class="n">Dockerfile</span>
  <span class="n">environment</span><span class="p">:</span>
    <span class="o">&amp;</span><span class="n">airflow</span><span class="o">-</span><span class="n">common</span><span class="o">-</span><span class="n">env</span>
    <span class="n">AIRFLOW__CORE__EXECUTOR</span><span class="p">:</span> <span class="n">CeleryExecutor</span>
    <span class="n">AIRFLOW__CORE__SQL_ALCHEMY_CONN</span><span class="p">:</span> <span class="n">postgresql</span><span class="o">+</span><span class="n">psycopg2</span><span class="p">:</span><span class="o">//</span><span class="n">airflow</span><span class="p">:</span><span class="n">airflow</span><span class="nd">@postgres</span><span class="o">/</span><span class="n">airflow</span>
    <span class="n">AIRFLOW__CELERY__RESULT_BACKEND</span><span class="p">:</span> <span class="n">db</span><span class="o">+</span><span class="n">postgresql</span><span class="p">:</span><span class="o">//</span><span class="n">airflow</span><span class="p">:</span><span class="n">airflow</span><span class="nd">@postgres</span><span class="o">/</span><span class="n">airflow</span>
    <span class="n">AIRFLOW__CELERY__BROKER_URL</span><span class="p">:</span> <span class="n">redis</span><span class="p">:</span><span class="o">//</span><span class="p">:</span><span class="nd">@redis</span><span class="p">:</span><span class="mi">6379</span><span class="o">/</span><span class="mi">0</span>
    <span class="n">AIRFLOW__CORE__FERNET_KEY</span><span class="p">:</span> <span class="s1">''</span>
    <span class="n">AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION</span><span class="p">:</span> <span class="s1">'true'</span>
    <span class="n">AIRFLOW__CORE__LOAD_EXAMPLES</span><span class="p">:</span> <span class="s1">'false'</span>
    <span class="n">AIRFLOW__API__AUTH_BACKEND</span><span class="p">:</span> <span class="s1">'airflow.api.auth.backend.basic_auth'</span>
    <span class="n">_PIP_ADDITIONAL_REQUIREMENTS</span><span class="p">:</span> <span class="err">$</span><span class="p">{</span><span class="n">_PIP_ADDITIONAL_REQUIREMENTS</span><span class="p">:</span><span class="o">-</span><span class="p">}</span>
    <span class="n">GOOGLE_APPLICATION_CREDENTIALS</span><span class="p">:</span> <span class="o">/.</span><span class="n">google</span><span class="o">/</span><span class="n">credentials</span><span class="o">/</span><span class="n">google_credentials</span><span class="o">.</span><span class="n">json</span>
    <span class="n">AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT</span><span class="p">:</span> <span class="s1">'google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json'</span>
    <span class="n">GCP_PROJECT_ID</span><span class="p">:</span> <span class="s1">'pivotal-surfer-336713'</span>
    <span class="n">GCP_GCS_BUCKET</span><span class="p">:</span> <span class="s2">"dtc_data_lake_pivotal-surfer-336713"</span>

  <span class="n">volumes</span><span class="p">:</span>
    <span class="o">-</span> <span class="o">./</span><span class="n">dags</span><span class="p">:</span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">airflow</span><span class="o">/</span><span class="n">dags</span>
    <span class="o">-</span> <span class="o">./</span><span class="n">logs</span><span class="p">:</span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">airflow</span><span class="o">/</span><span class="n">logs</span>
    <span class="o">-</span> <span class="o">./</span><span class="n">plugins</span><span class="p">:</span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">airflow</span><span class="o">/</span><span class="n">plugins</span>
    <span class="o">-</span> <span class="o">~/.</span><span class="n">google</span><span class="o">/</span><span class="n">credentials</span><span class="o">/</span><span class="p">:</span><span class="o">/.</span><span class="n">google</span><span class="o">/</span><span class="n">credentials</span><span class="p">:</span><span class="n">ro</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Following is the custom Dockerfile whcich is placed inside the <code>airflow</code> folder.
The Dockerfile has the custom packages to be installed. The one we'll need the most is <code>gcloud</code> to connect with the GCS bucket/Data Lake.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># First-time build can take upto 10 mins.</span>

<span class="n">FROM</span> <span class="n">apache</span><span class="o">/</span><span class="n">airflow</span><span class="p">:</span><span class="mf">2.2</span><span class="o">.</span><span class="mi">3</span>

<span class="n">ENV</span> <span class="n">AIRFLOW_HOME</span><span class="o">=/</span><span class="n">opt</span><span class="o">/</span><span class="n">airflow</span>

<span class="n">USER</span> <span class="n">root</span>
<span class="n">RUN</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">update</span> <span class="o">-</span><span class="n">qq</span> <span class="o">&amp;&amp;</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="n">vim</span> <span class="o">-</span><span class="n">qqq</span>
<span class="c1"># git gcc g++ -qqq</span>

<span class="n">COPY</span> <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span> <span class="o">.</span>
<span class="n">RUN</span> <span class="n">pip</span> <span class="n">install</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">cache</span><span class="o">-</span><span class="nb">dir</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>


<span class="c1"># Ref: https://airflow.apache.org/docs/docker-stack/recipes.html</span>

<span class="n">SHELL</span> <span class="p">[</span><span class="s2">"/bin/bash"</span><span class="p">,</span> <span class="s2">"-o"</span><span class="p">,</span> <span class="s2">"pipefail"</span><span class="p">,</span> <span class="s2">"-e"</span><span class="p">,</span> <span class="s2">"-u"</span><span class="p">,</span> <span class="s2">"-x"</span><span class="p">,</span> <span class="s2">"-c"</span><span class="p">]</span>

<span class="n">ARG</span> <span class="n">CLOUD_SDK_VERSION</span><span class="o">=</span><span class="mf">322.0</span><span class="o">.</span><span class="mi">0</span>
<span class="n">ENV</span> <span class="n">GCLOUD_HOME</span><span class="o">=/</span><span class="n">home</span><span class="o">/</span><span class="n">google</span><span class="o">-</span><span class="n">cloud</span><span class="o">-</span><span class="n">sdk</span>

<span class="n">ENV</span> <span class="n">PATH</span><span class="o">=</span><span class="s2">"$</span><span class="si">{GCLOUD_HOME}</span><span class="s2">/bin/:$</span><span class="si">{PATH}</span><span class="s2">"</span>

<span class="n">RUN</span> <span class="n">DOWNLOAD_URL</span><span class="o">=</span><span class="s2">"https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-$</span><span class="si">{CLOUD_SDK_VERSION}</span><span class="s2">-linux-x86_64.tar.gz"</span> \
    <span class="o">&amp;&amp;</span> <span class="n">TMP_DIR</span><span class="o">=</span><span class="s2">"$(mktemp -d)"</span> \
    <span class="o">&amp;&amp;</span> <span class="n">curl</span> <span class="o">-</span><span class="n">fL</span> <span class="s2">"$</span><span class="si">{DOWNLOAD_URL}</span><span class="s2">"</span> <span class="o">--</span><span class="n">output</span> <span class="s2">"$</span><span class="si">{TMP_DIR}</span><span class="s2">/google-cloud-sdk.tar.gz"</span> \
    <span class="o">&amp;&amp;</span> <span class="n">mkdir</span> <span class="o">-</span><span class="n">p</span> <span class="s2">"$</span><span class="si">{GCLOUD_HOME}</span><span class="s2">"</span> \
    <span class="o">&amp;&amp;</span> <span class="n">tar</span> <span class="n">xzf</span> <span class="s2">"$</span><span class="si">{TMP_DIR}</span><span class="s2">/google-cloud-sdk.tar.gz"</span> <span class="o">-</span><span class="n">C</span> <span class="s2">"$</span><span class="si">{GCLOUD_HOME}</span><span class="s2">"</span> <span class="o">--</span><span class="n">strip</span><span class="o">-</span><span class="n">components</span><span class="o">=</span><span class="mi">1</span> \
    <span class="o">&amp;&amp;</span> <span class="s2">"$</span><span class="si">{GCLOUD_HOME}</span><span class="s2">/install.sh"</span> \
       <span class="o">--</span><span class="n">bash</span><span class="o">-</span><span class="n">completion</span><span class="o">=</span><span class="n">false</span> \
       <span class="o">--</span><span class="n">path</span><span class="o">-</span><span class="n">update</span><span class="o">=</span><span class="n">false</span> \
       <span class="o">--</span><span class="n">usage</span><span class="o">-</span><span class="n">reporting</span><span class="o">=</span><span class="n">false</span> \
       <span class="o">--</span><span class="n">quiet</span> \
    <span class="o">&amp;&amp;</span> <span class="n">rm</span> <span class="o">-</span><span class="n">rf</span> <span class="s2">"$</span><span class="si">{TMP_DIR}</span><span class="s2">"</span> \
    <span class="o">&amp;&amp;</span> <span class="n">gcloud</span> <span class="o">--</span><span class="n">version</span>

<span class="n">WORKDIR</span> <span class="err">$</span><span class="n">AIRFLOW_HOME</span>

<span class="n">USER</span> <span class="err">$</span><span class="n">AIRFLOW_UID</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>requirements.txt</code> file in the Dockerfile which contains the required pyton packages is as follows:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">apache</span><span class="o">-</span><span class="n">airflow</span><span class="o">-</span><span class="n">providers</span><span class="o">-</span><span class="n">google</span>
<span class="n">pyarrow</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In case you don't want to see so many services as it is done in the above <code>docker-compose.yaml</code> file, you can use the following one which is placed in the <code>week_2_data_ingestion/airflow/extras</code> folder in the course github repo:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">version</span><span class="p">:</span> <span class="s1">'3.7'</span>
<span class="n">services</span><span class="p">:</span>
    <span class="n">webserver</span><span class="p">:</span>
        <span class="n">container_name</span><span class="p">:</span> <span class="n">airflow</span>
        <span class="n">build</span><span class="p">:</span>
            <span class="n">context</span><span class="p">:</span> <span class="o">..</span>
            <span class="n">dockerfile</span><span class="p">:</span> <span class="o">../</span><span class="n">Dockerfile</span>
        <span class="n">environment</span><span class="p">:</span>
            <span class="o">-</span> <span class="n">PYTHONPATH</span><span class="o">=/</span><span class="n">home</span><span class="o">/</span><span class="n">airflow</span>
            <span class="c1"># airflow connection with SQLAlchemy container</span>
            <span class="o">-</span> <span class="n">AIRFLOW__CORE__SQL_ALCHEMY_CONN</span><span class="o">=</span><span class="n">sqlite</span><span class="p">:</span><span class="o">///</span><span class="err">$</span><span class="n">AIRFLOW_HOME</span><span class="o">/</span><span class="n">airflow</span><span class="o">.</span><span class="n">db</span>
            <span class="o">-</span> <span class="n">AIRFLOW__CORE__EXECUTOR</span><span class="o">=</span><span class="n">LocalExecutor</span>
            <span class="c1"># disable example loading</span>
            <span class="o">-</span> <span class="n">AIRFLOW__CORE__LOAD_EXAMPLES</span><span class="o">=</span><span class="n">FALSE</span>

        <span class="n">volumes</span><span class="p">:</span>
            <span class="o">-</span> <span class="o">./</span><span class="n">dags</span><span class="p">:</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">airflow</span><span class="o">/</span><span class="n">dags</span>
        <span class="c1"># user: "${AIRFLOW_UID:-50000}:0"</span>
        <span class="n">ports</span><span class="p">:</span>
            <span class="o">-</span> <span class="s2">"8080:8080"</span>
        <span class="n">command</span><span class="p">:</span> <span class="o">&gt;</span>  <span class="c1"># airflow db upgrade;</span>
            <span class="n">bash</span> <span class="o">-</span><span class="n">c</span> <span class="s2">"</span>
                <span class="n">airflow</span> <span class="n">scheduler</span> <span class="o">-</span><span class="n">D</span><span class="p">;</span>
                <span class="n">rm</span> <span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">airflow</span><span class="o">/</span><span class="n">airflow</span><span class="o">-</span><span class="n">scheduler</span><span class="o">.*</span><span class="p">;</span>
                <span class="n">airflow</span> <span class="n">webserver</span><span class="s2">"</span>
        <span class="n">healthcheck</span><span class="p">:</span>
            <span class="n">test</span><span class="p">:</span> <span class="p">[</span> <span class="s2">"CMD-SHELL"</span><span class="p">,</span> <span class="s2">"[ -f /home/airflow/airflow-webserver.pid ]"</span> <span class="p">]</span>
            <span class="n">interval</span><span class="p">:</span> <span class="mi">30</span><span class="n">s</span>
            <span class="n">timeout</span><span class="p">:</span> <span class="mi">30</span><span class="n">s</span>
            <span class="n">retries</span><span class="p">:</span> <span class="mi">3</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will not use this file to avoid any confusion.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Before starting Airflow for the first time, You need to prepare your environment, i.e. create the necessary files, directories and initialize the database.</p>
<p>On Linux, the quick-start needs to know your host user id and needs to have group id set to <code>0</code>. Otherwise the files created in <code>dags</code>, <code>logs</code> and <code>plugins</code> will be created with <code>root</code> user. You have to make sure to configure them for the docker-compose: (run it inside the <code>airflow</code> folder where the <code>docker-compose.yaml</code> file is placed) [<a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html">Airflow docs</a>]</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mkdir</span> <span class="o">-</span><span class="n">p</span> <span class="o">./</span><span class="n">dags</span> <span class="o">./</span><span class="n">logs</span> <span class="o">./</span><span class="n">plugins</span>
<span class="n">echo</span> <span class="o">-</span><span class="n">e</span> <span class="s2">"AIRFLOW_UID=$(id -u)"</span> <span class="o">&gt;</span> <span class="o">.</span><span class="n">env</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For other operating systems, you will get warning that <code>AIRFLOW_UID</code> is not set, but you can ignore it. You can also manually create the <code>.env</code> file in the same folder your <code>docker-compose.yaml</code> is placed with this content to get rid of the warning:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">AIRFLOW_UID</span><span class="o">=</span><span class="mi">1000</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then we need to initialize the database. On all operating systems, you need to run database migrations and create the first user account. To do it, run.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">docker</span><span class="o">-</span><span class="n">compose</span> <span class="n">build</span>
<span class="n">docker</span><span class="o">-</span><span class="n">compose</span> <span class="n">up</span> <span class="n">airflow</span><span class="o">-</span><span class="n">init</span>
<span class="n">docker</span><span class="o">-</span><span class="n">compose</span> <span class="n">up</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You may also some error, but you can ignore them as they are for some services in the official docker compose file that we do not use.</p>
<p>You can check which services are up using:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">docker</span><span class="o">-</span><span class="n">compose</span> <span class="n">ps</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For me, the output is as follows:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">airflow</span><span class="o">-</span><span class="n">airflow</span><span class="o">-</span><span class="n">scheduler</span><span class="o">-</span><span class="mi">1</span>   <span class="s2">"/usr/bin/dumb-init …"</span>   <span class="n">airflow</span><span class="o">-</span><span class="n">scheduler</span>   <span class="n">running</span> <span class="p">(</span><span class="n">healthy</span><span class="p">)</span>   <span class="mi">8080</span><span class="o">/</span><span class="n">tcp</span>
<span class="n">airflow</span><span class="o">-</span><span class="n">airflow</span><span class="o">-</span><span class="n">triggerer</span><span class="o">-</span><span class="mi">1</span>   <span class="s2">"/usr/bin/dumb-init …"</span>   <span class="n">airflow</span><span class="o">-</span><span class="n">triggerer</span>   <span class="n">running</span> <span class="p">(</span><span class="n">healthy</span><span class="p">)</span>   <span class="mi">8080</span><span class="o">/</span><span class="n">tcp</span>
<span class="n">airflow</span><span class="o">-</span><span class="n">airflow</span><span class="o">-</span><span class="n">webserver</span><span class="o">-</span><span class="mi">1</span>   <span class="s2">"/usr/bin/dumb-init …"</span>   <span class="n">airflow</span><span class="o">-</span><span class="n">webserver</span>   <span class="n">running</span> <span class="p">(</span><span class="n">healthy</span><span class="p">)</span>   <span class="mf">0.0</span><span class="o">.</span><span class="mf">0.0</span><span class="p">:</span><span class="mi">8080</span><span class="o">-&gt;</span><span class="mi">8080</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">8080</span><span class="o">-&gt;</span><span class="mi">8080</span><span class="o">/</span><span class="n">tcp</span>
<span class="n">airflow</span><span class="o">-</span><span class="n">airflow</span><span class="o">-</span><span class="n">worker</span><span class="o">-</span><span class="mi">1</span>      <span class="s2">"/usr/bin/dumb-init …"</span>   <span class="n">airflow</span><span class="o">-</span><span class="n">worker</span>      <span class="n">running</span> <span class="p">(</span><span class="n">healthy</span><span class="p">)</span>   <span class="mi">8080</span><span class="o">/</span><span class="n">tcp</span>
<span class="n">airflow</span><span class="o">-</span><span class="n">flower</span><span class="o">-</span><span class="mi">1</span>              <span class="s2">"/usr/bin/dumb-init …"</span>   <span class="n">flower</span>              <span class="n">running</span> <span class="p">(</span><span class="n">healthy</span><span class="p">)</span>   <span class="mf">0.0</span><span class="o">.</span><span class="mf">0.0</span><span class="p">:</span><span class="mi">5555</span><span class="o">-&gt;</span><span class="mi">5555</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">5555</span><span class="o">-&gt;</span><span class="mi">5555</span><span class="o">/</span><span class="n">tcp</span>
<span class="n">airflow</span><span class="o">-</span><span class="n">postgres</span><span class="o">-</span><span class="mi">1</span>            <span class="s2">"docker-entrypoint.s…"</span>   <span class="n">postgres</span>            <span class="n">running</span> <span class="p">(</span><span class="n">healthy</span><span class="p">)</span>   <span class="mi">5432</span><span class="o">/</span><span class="n">tcp</span>
<span class="n">airflow</span><span class="o">-</span><span class="n">redis</span><span class="o">-</span><span class="mi">1</span>               <span class="s2">"docker-entrypoint.s…"</span>   <span class="n">redis</span>               <span class="n">running</span> <span class="p">(</span><span class="n">healthy</span><span class="p">)</span>   <span class="mi">6379</span><span class="o">/</span><span class="n">tcp</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are several ways to interact with it:</p>
<ul>
<li>by running <a href="https://airflow.apache.org/docs/apache-airflow/stable/usage-cli.html">CLI commands</a>.</li>
<li>via a browser using the <a href="https://airflow.apache.org/docs/apache-airflow/stable/ui.html">web interface</a>.</li>
<li>using the <a href="https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html">REST API</a>.</li>
</ul>
<p>For the web interface, you can go to this address: <code>http://0.0.0.0:8080/</code></p>
<p>The airflow UI will be like this:</p>
<p><img src="/blog/images/copied_from_nb/images/data-engineering-w2/11.png" alt=""></p>
<p>The account created has the login <code>airflow</code> and the password <code>airflow</code>. After log in you will see two generated dags from the <code>week_2_data_ingestion/airflow/dags</code> folder.

</p>
<center>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/9ksX9REfL8w" frameborder="0" allowfullscreen=""></iframe>
</center>


</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A Workflow has the following components:</p>
<ul>
<li>
<p><code>DAG</code>: Directed acyclic graph, specifies the dependencies between a set of tasks with explicit execution order, and has a beginning as well as an end. (Hence, “acyclic”)</p>
<ul>
<li>
<code>DAG Structure</code>: DAG Definition, Tasks (eg. Operators), Task Dependencies (control flow: <code>&gt;&gt;</code> or <code>&lt;&lt;</code> )</li>
</ul>
</li>
<li>
<p><code>Task</code>: a defined unit of work (aka, operators in Airflow). The Tasks themselves describe what to do, be it fetching data, running analysis, triggering other systems, or more.</p>
<ul>
<li>Common Types: Operators (used in this workshop), Sensors, TaskFlow decorators</li>
<li>Sub-classes of Airflow's BaseOperator</li>
</ul>
</li>
<li>
<p><code>DAG Run</code>: individual execution/run of a DAG</p>
<ul>
<li>scheduled or triggered</li>
</ul>
</li>
<li>
<p><code>Task Instance</code>: an individual run of a single task. Task instances also have an indicative state, which could be “running”, “success”, “failed”, “skipped”, “up for retry”, etc.</p>
<ul>
<li>Ideally, a task should flow from <code>none</code>, to <code>scheduled</code>, to <code>queued</code>, to <code>running</code>, and finally to <code>success</code>.</li>
</ul>
</li>
</ul>
<p>Let's look at how to use Airflow to ingest data into GCP. To do so, we'll need to create a DAG object. One thing to remember is that this Airflow Python script is really just a configuration file that specifies the DAG's structure as code. The tasks defined here will be executed in a context distinct from that of this script. This script cannot be used to cross-communicate between tasks because different tasks run on different workers at different times. Note that we have a more advanced feature called <a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html">XComs</a> that can be used for this purpose [<a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html">Airflow docs</a>].</p>
<p>People mistakenly believe that the DAG definition file is where they can do actual data processing - this is not the case! The goal of the script is to create a DAG object. It must evaluate quickly (seconds, not minutes) because the scheduler will run it on a regular basis to reflect any changes [<a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html">Airflow docs</a>].</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The structure of a DAG file is as follows:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Imports</span>
<span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="o">...</span>

<span class="n">default_args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="o">...</span>
<span class="p">}</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
    <span class="s1">'tutorial'</span><span class="p">,</span>
    <span class="n">default_args</span><span class="o">=</span><span class="n">default_args</span><span class="p">,</span>
    <span class="n">description</span><span class="o">=</span><span class="s1">'A simple tutorial DAG'</span><span class="p">,</span>
    <span class="n">schedule_interval</span><span class="o">=</span><span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">start_date</span><span class="o">=</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2021</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">catchup</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s1">'example'</span><span class="p">],</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>

    <span class="c1"># t1, t2 and t3 are examples of tasks created by instantiating operators</span>
    <span class="n">t1</span> <span class="o">=</span> <span class="n">BashOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s1">'print_date'</span><span class="p">,</span>
        <span class="n">bash_command</span><span class="o">=</span><span class="s1">'date'</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">t2</span> <span class="o">=</span> <span class="o">...</span>

    <span class="n">t3</span> <span class="o">=</span> <span class="o">...</span>

    <span class="c1">## defining task dependencies</span>
    <span class="n">t1</span> <span class="o">&gt;&gt;</span> <span class="p">[</span><span class="n">t2</span><span class="p">,</span> <span class="n">t3</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>An Airflow pipeline is just a Python script that happens to define an Airflow DAG object.</li>
<li>We have the choice to explicitly pass a set of arguments to each task’s constructor (which would become redundant), or (better!) we can define a dictionary of default parameters that we can use when creating tasks.</li>
<li>We’ll need a DAG object to nest our tasks into. Here we pass a string that defines the dag_id, which serves as a unique identifier for your DAG. We also pass the default argument dictionary that we just defined and define a schedule_interval of 1 day for the DAG.</li>
<li>Tasks are generated when instantiating operator objects. An object instantiated from an operator is called a task. The first argument task_id acts as a unique identifier for the task.</li>
<li>Then we need to define dependencies between tasks.</li>
</ul>
<p>You can check more tutorials and examples <a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html">here</a>.</p>
<p>Now let's see out own DAG file for ingesting <code>yellow_tripdata_2021-01.csv</code> dataset into GCP which is place in <code>week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py</code>. First let's check the structure:</p>
<ul>
<li>importing python libraries <ul>
<li>in-built airflow like <code>BashOperator</code> and <code>PythonOperator</code>. There is also <code>DockerOperator</code> to run docker in docker!</li>
<li>
<code>storage</code> from google cloud library to interact with GCS.</li>
<li>BigQuery from google airflow provider to interact with BigQuery and create an external table.</li>
<li>
<code>pyarrow</code> library for converting dataset type to <code>parquet</code> before uploading it to GCS. <code>parquet</code> is used more in production and is faster to upload it and also uses less space on GCS.</li>
</ul>
</li>
<li>setting some variables<ul>
<li>
<code>GCP_PROJECT_ID</code>, <code>GCP_GCS_BUCKET</code> which we set in the <code>docker-compose.yaml</code> file under <code>x-airflow-common</code> section.</li>
<li>info about dataset url</li>
<li>airflow local folder path</li>
<li>name of the desired parquet file</li>
<li>BigQuery dataset which can be found from <code>variables.tf</code> from terraform folder of week 1 (in <code>week_1_basics_n_setup/1_terraform_gcp/terraform/variables.tf</code>). I think the name was <code>BQ_DATASET</code> there, but the value is the same <code>trips_data_all</code>.</li>
</ul>
</li>
<li>Some python functions which will be attached to <code>PythonOperator</code>s like <code>format_to_parquet()</code> and <code>upload_to_gcs()</code>. Their names describe their functionality.</li>
<li>Default arguments which will be used in DAG definition. </li>
<li>Then the DAG declaration with tasks and their dependencies<ul>
<li>
<code>download_dataset_task</code>to download the dataset using a bash command.</li>
<li>
<code>format_to_parquet_task</code> which call the <code>format_to_parquet()</code> function.</li>
<li>
<code>local_to_gcs_task</code> which call the <code>upload_to_gcs()</code> function.</li>
<li>
<code>bigquery_external_table_task</code> to extract schema and create a BigQuery table form the file uploaded to GCS. You can easily run SQL queries on this table.</li>
</ul>
</li>
<li>Then the workflow for direction of tasks: <code>download_dataset_task &gt;&gt; format_to_parquet_task &gt;&gt; local_to_gcs_task &gt;&gt; bigquery_external_table_task</code>
</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">logging</span>

<span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span> <span class="nn">airflow.utils.dates</span> <span class="kn">import</span> <span class="n">days_ago</span>
<span class="kn">from</span> <span class="nn">airflow.operators.bash</span> <span class="kn">import</span> <span class="n">BashOperator</span>
<span class="kn">from</span> <span class="nn">airflow.operators.python</span> <span class="kn">import</span> <span class="n">PythonOperator</span>

<span class="kn">from</span> <span class="nn">google.cloud</span> <span class="kn">import</span> <span class="n">storage</span>
<span class="kn">from</span> <span class="nn">airflow.providers.google.cloud.operators.bigquery</span> <span class="kn">import</span> <span class="n">BigQueryCreateExternalTableOperator</span>
<span class="kn">import</span> <span class="nn">pyarrow.csv</span> <span class="k">as</span> <span class="nn">pv</span>
<span class="kn">import</span> <span class="nn">pyarrow.parquet</span> <span class="k">as</span> <span class="nn">pq</span>

<span class="n">PROJECT_ID</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"GCP_PROJECT_ID"</span><span class="p">)</span>
<span class="n">BUCKET</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"GCP_GCS_BUCKET"</span><span class="p">)</span>

<span class="n">dataset_file</span> <span class="o">=</span> <span class="s2">"yellow_tripdata_2021-01.csv"</span>
<span class="n">dataset_url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"https://s3.amazonaws.com/nyc-tlc/trip+data/</span><span class="si">{</span><span class="n">dataset_file</span><span class="si">}</span><span class="s2">"</span>
<span class="n">path_to_local_home</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"AIRFLOW_HOME"</span><span class="p">,</span> <span class="s2">"/opt/airflow/"</span><span class="p">)</span>
<span class="n">parquet_file</span> <span class="o">=</span> <span class="n">dataset_file</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">'.csv'</span><span class="p">,</span> <span class="s1">'.parquet'</span><span class="p">)</span>
<span class="n">BIGQUERY_DATASET</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"BIGQUERY_DATASET"</span><span class="p">,</span> <span class="s1">'trips_data_all'</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">format_to_parquet</span><span class="p">(</span><span class="n">src_file</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">src_file</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">'.csv'</span><span class="p">):</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">"Can only accept source files in CSV format, for the moment"</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">pv</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">src_file</span><span class="p">)</span>
    <span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">src_file</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">'.csv'</span><span class="p">,</span> <span class="s1">'.parquet'</span><span class="p">))</span>


<span class="c1"># NOTE: takes 20 mins, at an upload speed of 800kbps. Faster if your internet has a better upload speed</span>
<span class="k">def</span> <span class="nf">upload_to_gcs</span><span class="p">(</span><span class="n">bucket</span><span class="p">,</span> <span class="n">object_name</span><span class="p">,</span> <span class="n">local_file</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Ref: https://cloud.google.com/storage/docs/uploading-objects#storage-upload-object-python</span>
<span class="sd">    :param bucket: GCS bucket name</span>
<span class="sd">    :param object_name: target path &amp; file-name</span>
<span class="sd">    :param local_file: source path &amp; file-name</span>
<span class="sd">    :return:</span>
<span class="sd">    """</span>
    <span class="c1"># WORKAROUND to prevent timeout for files &gt; 6 MB on 800 kbps upload speed.</span>
    <span class="c1"># (Ref: https://github.com/googleapis/python-storage/issues/74)</span>
    <span class="n">storage</span><span class="o">.</span><span class="n">blob</span><span class="o">.</span><span class="n">_MAX_MULTIPART_SIZE</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span>  <span class="c1"># 5 MB</span>
    <span class="n">storage</span><span class="o">.</span><span class="n">blob</span><span class="o">.</span><span class="n">_DEFAULT_CHUNKSIZE</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span>  <span class="c1"># 5 MB</span>
    <span class="c1"># End of Workaround</span>

    <span class="n">client</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">Client</span><span class="p">()</span>
    <span class="n">bucket</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">bucket</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>

    <span class="n">blob</span> <span class="o">=</span> <span class="n">bucket</span><span class="o">.</span><span class="n">blob</span><span class="p">(</span><span class="n">object_name</span><span class="p">)</span>
    <span class="n">blob</span><span class="o">.</span><span class="n">upload_from_filename</span><span class="p">(</span><span class="n">local_file</span><span class="p">)</span>


<span class="n">default_args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"owner"</span><span class="p">:</span> <span class="s2">"airflow"</span><span class="p">,</span>
    <span class="s2">"start_date"</span><span class="p">:</span> <span class="n">days_ago</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="s2">"depends_on_past"</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s2">"retries"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># NOTE: DAG declaration - using a Context Manager (an implicit way)</span>
<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
    <span class="n">dag_id</span><span class="o">=</span><span class="s2">"data_ingestion_gcs_dag"</span><span class="p">,</span>
    <span class="n">schedule_interval</span><span class="o">=</span><span class="s2">"@daily"</span><span class="p">,</span>
    <span class="n">default_args</span><span class="o">=</span><span class="n">default_args</span><span class="p">,</span>
    <span class="n">catchup</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">max_active_runs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s1">'dtc-de'</span><span class="p">],</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>

    <span class="n">download_dataset_task</span> <span class="o">=</span> <span class="n">BashOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">"download_dataset_task"</span><span class="p">,</span>
        <span class="n">bash_command</span><span class="o">=</span><span class="sa">f</span><span class="s2">"curl -sS </span><span class="si">{</span><span class="n">dataset_url</span><span class="si">}</span><span class="s2"> &gt; </span><span class="si">{</span><span class="n">path_to_local_home</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">dataset_file</span><span class="si">}</span><span class="s2">"</span>
    <span class="p">)</span>

    <span class="n">format_to_parquet_task</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">"format_to_parquet_task"</span><span class="p">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">format_to_parquet</span><span class="p">,</span>
        <span class="n">op_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">"src_file"</span><span class="p">:</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">path_to_local_home</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">dataset_file</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>

    <span class="c1"># TODO: Homework - research and try XCOM to communicate output values between 2 tasks/operators</span>
    <span class="n">local_to_gcs_task</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">"local_to_gcs_task"</span><span class="p">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">upload_to_gcs</span><span class="p">,</span>
        <span class="n">op_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">"bucket"</span><span class="p">:</span> <span class="n">BUCKET</span><span class="p">,</span>
            <span class="s2">"object_name"</span><span class="p">:</span> <span class="sa">f</span><span class="s2">"raw/</span><span class="si">{</span><span class="n">parquet_file</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span>
            <span class="s2">"local_file"</span><span class="p">:</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">path_to_local_home</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">parquet_file</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>

    <span class="n">bigquery_external_table_task</span> <span class="o">=</span> <span class="n">BigQueryCreateExternalTableOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">"bigquery_external_table_task"</span><span class="p">,</span>
        <span class="n">table_resource</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">"tableReference"</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">"projectId"</span><span class="p">:</span> <span class="n">PROJECT_ID</span><span class="p">,</span>
                <span class="s2">"datasetId"</span><span class="p">:</span> <span class="n">BIGQUERY_DATASET</span><span class="p">,</span>
                <span class="s2">"tableId"</span><span class="p">:</span> <span class="s2">"external_table"</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="s2">"externalDataConfiguration"</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">"sourceFormat"</span><span class="p">:</span> <span class="s2">"PARQUET"</span><span class="p">,</span>
                <span class="s2">"sourceUris"</span><span class="p">:</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"gs://</span><span class="si">{</span><span class="n">BUCKET</span><span class="si">}</span><span class="s2">/raw/</span><span class="si">{</span><span class="n">parquet_file</span><span class="si">}</span><span class="s2">"</span><span class="p">],</span>
            <span class="p">},</span>
        <span class="p">},</span>
    <span class="p">)</span>

    <span class="n">download_dataset_task</span> <span class="o">&gt;&gt;</span> <span class="n">format_to_parquet_task</span> <span class="o">&gt;&gt;</span> <span class="n">local_to_gcs_task</span> <span class="o">&gt;&gt;</span> <span class="n">bigquery_external_table_task</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's run it. First go to <code>localhost:8080</code> and use <code>airflow</code> and <code>airflow</code> as username and password to log in. Then switch on the <code>data_ingestion_gcs_dag</code> and click on that to open and be able to see the tree. You can also swith to graph using the toolbar on top of the page.</p>
<p><img src="/blog/images/copied_from_nb/images/data-engineering-w2/12.png" alt=""></p>
<p><img src="/blog/images/copied_from_nb/images/data-engineering-w2/13.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then click on the play button on top-right part of the page and select <code>Trigger DAG</code>. Note that after running <code>docker-compose ps</code> everything should be in the <code>healthy</code> mode.</p>
<p>In case any of the tasks fails, you can check the logs as follows:</p>
<p><img src="/blog/images/copied_from_nb/images/data-engineering-w2/2.gif" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If the tasks are completed successfully, then you will see the uploaded <code>parquet</code> file in GCS bucket and also the table in BigQuery.</p>
<p><strong>Note</strong>: All the <code>PythonOperator</code> codes (functions called by that) are executed in airflow-workers (which is a container) and files (datasets) are saved there and not in your local machine. If you use <code>DockerOperator</code>, you are actually running a docker inside another docker (airflow-worker).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Ingest-Data-to-Postgres-using-Airflow">
<a class="anchor" href="#Ingest-Data-to-Postgres-using-Airflow" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ingest Data to Postgres using Airflow<a class="anchor-link" href="#Ingest-Data-to-Postgres-using-Airflow"> </a>
</h2>
<p>In order to see another step-by-step tutorial on ingesting data to local postgres using airflow, you can check the following video.

</p>
<center>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/s2U8MWJH5xA" frameborder="0" allowfullscreen=""></iframe>
</center>


</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Before, we uploaded data to GCS using the <code>upload_to_gcs</code> function. But here, a s we want to ingest data into postgres (which we will run using another docker-compose), we need to use another function. All the steps are the same (check the above video) and we just check the DAG file and ingestion script here which is a modified version of what we used in week 1.</p>
<p>Let's see the <code>week_2_data_ingestion/airflow/dags_local/data_ingestion_local.py</code> file first:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span>

<span class="kn">from</span> <span class="nn">airflow.operators.bash</span> <span class="kn">import</span> <span class="n">BashOperator</span>
<span class="kn">from</span> <span class="nn">airflow.operators.python</span> <span class="kn">import</span> <span class="n">PythonOperator</span>

<span class="kn">from</span> <span class="nn">ingest_script</span> <span class="kn">import</span> <span class="n">ingest_callable</span>


<span class="n">AIRFLOW_HOME</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"AIRFLOW_HOME"</span><span class="p">,</span> <span class="s2">"/opt/airflow/"</span><span class="p">)</span>


<span class="n">PG_HOST</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">'PG_HOST'</span><span class="p">)</span>
<span class="n">PG_USER</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">'PG_USER'</span><span class="p">)</span>
<span class="n">PG_PASSWORD</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">'PG_PASSWORD'</span><span class="p">)</span>
<span class="n">PG_PORT</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">'PG_PORT'</span><span class="p">)</span>
<span class="n">PG_DATABASE</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">'PG_DATABASE'</span><span class="p">)</span>


<span class="n">local_workflow</span> <span class="o">=</span> <span class="n">DAG</span><span class="p">(</span>
    <span class="s2">"LocalIngestionDag"</span><span class="p">,</span>
    <span class="n">schedule_interval</span><span class="o">=</span><span class="s2">"0 6 2 * *"</span><span class="p">,</span>
    <span class="n">start_date</span><span class="o">=</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2021</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>


<span class="n">URL_PREFIX</span> <span class="o">=</span> <span class="s1">'https://s3.amazonaws.com/nyc-tlc/trip+data'</span> 
<span class="n">URL_TEMPLATE</span> <span class="o">=</span> <span class="n">URL_PREFIX</span> <span class="o">+</span> <span class="s1">'/yellow_tripdata_{{ execution_date.strftime(</span><span class="se">\'</span><span class="s1">%Y-%m</span><span class="se">\'</span><span class="s1">) }}.csv'</span>
<span class="n">OUTPUT_FILE_TEMPLATE</span> <span class="o">=</span> <span class="n">AIRFLOW_HOME</span> <span class="o">+</span> <span class="s1">'/output_{{ execution_date.strftime(</span><span class="se">\'</span><span class="s1">%Y-%m</span><span class="se">\'</span><span class="s1">) }}.csv'</span>
<span class="n">TABLE_NAME_TEMPLATE</span> <span class="o">=</span> <span class="s1">'yellow_taxi_{{ execution_date.strftime(</span><span class="se">\'</span><span class="s1">%Y_%m</span><span class="se">\'</span><span class="s1">) }}'</span>

<span class="k">with</span> <span class="n">local_workflow</span><span class="p">:</span>
    <span class="n">wget_task</span> <span class="o">=</span> <span class="n">BashOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s1">'wget'</span><span class="p">,</span>
        <span class="n">bash_command</span><span class="o">=</span><span class="sa">f</span><span class="s1">'curl -sSL </span><span class="si">{</span><span class="n">URL_TEMPLATE</span><span class="si">}</span><span class="s1"> &gt; </span><span class="si">{</span><span class="n">OUTPUT_FILE_TEMPLATE</span><span class="si">}</span><span class="s1">'</span>
    <span class="p">)</span>

    <span class="n">ingest_task</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">"ingest"</span><span class="p">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">ingest_callable</span><span class="p">,</span>
        <span class="n">op_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">user</span><span class="o">=</span><span class="n">PG_USER</span><span class="p">,</span>
            <span class="n">password</span><span class="o">=</span><span class="n">PG_PASSWORD</span><span class="p">,</span>
            <span class="n">host</span><span class="o">=</span><span class="n">PG_HOST</span><span class="p">,</span>
            <span class="n">port</span><span class="o">=</span><span class="n">PG_PORT</span><span class="p">,</span>
            <span class="n">db</span><span class="o">=</span><span class="n">PG_DATABASE</span><span class="p">,</span>
            <span class="n">table_name</span><span class="o">=</span><span class="n">TABLE_NAME_TEMPLATE</span><span class="p">,</span>
            <span class="n">csv_file</span><span class="o">=</span><span class="n">OUTPUT_FILE_TEMPLATE</span>
        <span class="p">),</span>
    <span class="p">)</span>

    <span class="n">wget_task</span> <span class="o">&gt;&gt;</span> <span class="n">ingest_task</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And the ingestion function which is imported in the above script is as follows:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sqlalchemy</span> <span class="kn">import</span> <span class="n">create_engine</span>


<span class="k">def</span> <span class="nf">ingest_callable</span><span class="p">(</span><span class="n">user</span><span class="p">,</span> <span class="n">password</span><span class="p">,</span> <span class="n">host</span><span class="p">,</span> <span class="n">port</span><span class="p">,</span> <span class="n">db</span><span class="p">,</span> <span class="n">table_name</span><span class="p">,</span> <span class="n">csv_file</span><span class="p">,</span> <span class="n">execution_date</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">table_name</span><span class="p">,</span> <span class="n">csv_file</span><span class="p">,</span> <span class="n">execution_date</span><span class="p">)</span>

    <span class="n">engine</span> <span class="o">=</span> <span class="n">create_engine</span><span class="p">(</span><span class="sa">f</span><span class="s1">'postgresql://</span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s1">:</span><span class="si">{</span><span class="n">password</span><span class="si">}</span><span class="s1">@</span><span class="si">{</span><span class="n">host</span><span class="si">}</span><span class="s1">:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">db</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="n">engine</span><span class="o">.</span><span class="n">connect</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'connection established successfully, instering data...'</span><span class="p">)</span>

    <span class="n">t_start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
    <span class="n">df_iter</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csv_file</span><span class="p">,</span> <span class="n">iterator</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">chunksize</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>

    <span class="n">df</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">df_iter</span><span class="p">)</span>

    <span class="n">df</span><span class="o">.</span><span class="n">tpep_pickup_datetime</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">tpep_pickup_datetime</span><span class="p">)</span>
    <span class="n">df</span><span class="o">.</span><span class="n">tpep_dropoff_datetime</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">tpep_dropoff_datetime</span><span class="p">)</span>

    <span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to_sql</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span> <span class="n">con</span><span class="o">=</span><span class="n">engine</span><span class="p">,</span> <span class="n">if_exists</span><span class="o">=</span><span class="s1">'replace'</span><span class="p">)</span>

    <span class="n">df</span><span class="o">.</span><span class="n">to_sql</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span> <span class="n">con</span><span class="o">=</span><span class="n">engine</span><span class="p">,</span> <span class="n">if_exists</span><span class="o">=</span><span class="s1">'append'</span><span class="p">)</span>

    <span class="n">t_end</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'inserted the first chunk, took </span><span class="si">%.3f</span><span class="s1"> second'</span> <span class="o">%</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">))</span>

    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span> 
        <span class="n">t_start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">df</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">df_iter</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"completed"</span><span class="p">)</span>
            <span class="k">break</span>

        <span class="n">df</span><span class="o">.</span><span class="n">tpep_pickup_datetime</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">tpep_pickup_datetime</span><span class="p">)</span>
        <span class="n">df</span><span class="o">.</span><span class="n">tpep_dropoff_datetime</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">tpep_dropoff_datetime</span><span class="p">)</span>

        <span class="n">df</span><span class="o">.</span><span class="n">to_sql</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span> <span class="n">con</span><span class="o">=</span><span class="n">engine</span><span class="p">,</span> <span class="n">if_exists</span><span class="o">=</span><span class="s1">'append'</span><span class="p">)</span>

        <span class="n">t_end</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">'inserted another chunk, took </span><span class="si">%.3f</span><span class="s1"> second'</span> <span class="o">%</span> <span class="p">(</span><span class="n">t_end</span> <span class="o">-</span> <span class="n">t_start</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we have two <code>docker-compose.yaml</code> files: one for this week which runs airflow stuff and one for week 1 for postgres and pgadmin. Let's see how we can connect them.</p>
<p>When we run the <code>docker-compose.yaml</code> file for airflow, it creates a network called <code>airflow_default</code> which we will use as an external network to connect the docker-compose for postgres and pgadmin to [<a href="https://stackoverflow.com/questions/38088279/communication-between-multiple-docker-compose-projects">ref</a>].</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># docker-compose.yaml of week 1</span>
<span class="n">services</span><span class="p">:</span>
    <span class="n">pgdatabase</span><span class="p">:</span>
        <span class="n">image</span><span class="p">:</span> <span class="n">postgres</span><span class="p">:</span><span class="mi">13</span>
        <span class="n">environment</span><span class="p">:</span>
            <span class="o">-</span> <span class="n">POSTGRES_USER</span><span class="o">=</span><span class="n">root</span>
            <span class="o">-</span> <span class="n">POSTGRES_PASSWORD</span><span class="o">=</span><span class="n">root</span>
            <span class="o">-</span> <span class="n">POSTGRES_DB</span><span class="o">=</span><span class="n">ny_taxi</span>
        <span class="n">volumes</span><span class="p">:</span>
            <span class="o">-</span> <span class="s2">"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw"</span>
        <span class="n">ports</span><span class="p">:</span>
            <span class="o">-</span> <span class="s2">"5432:5432"</span>
        <span class="n">networks</span><span class="p">:</span>
            <span class="o">-</span> <span class="n">airflow</span>
    <span class="n">pgadmin</span><span class="p">:</span>
        <span class="n">image</span><span class="p">:</span> <span class="n">dpage</span><span class="o">/</span><span class="n">pgadmin4</span>
        <span class="n">environment</span><span class="p">:</span>
            <span class="o">-</span> <span class="n">PGADMIN_DEFAULT_EMAIL</span><span class="o">=</span><span class="n">admin</span><span class="nd">@admin</span><span class="o">.</span><span class="n">com</span>
            <span class="o">-</span> <span class="n">PGADMIN_DEFAULT_PASSWORD</span><span class="o">=</span><span class="n">root</span>
        <span class="n">ports</span><span class="p">:</span>
            <span class="o">-</span> <span class="s2">"8080:80"</span>

<span class="n">networks</span><span class="p">:</span>
  <span class="n">airflow</span><span class="p">:</span>
    <span class="n">external</span><span class="p">:</span>
      <span class="n">name</span><span class="p">:</span> <span class="n">airflow_default</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will have two postgres databases, one for airflow  that stores its own metadata and one for uplading our dataset into.</p>
<p>First run <code>docker-compose up</code> in week 1 folder, and then test if you can connect using <code>pgcli -h localhost -p 5432 -U root -d ny_taxi</code> command.</p>
<p>Then we can go to the airflow worker container and see if we can connect to the postgres database that we ran above:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">docker</span> <span class="n">exec</span> <span class="o">-</span><span class="n">it</span> <span class="o">&lt;</span><span class="n">container</span> <span class="nb">id</span><span class="o">&gt;</span> <span class="n">bash</span>
<span class="c1"># then type python to open python </span>
<span class="o">&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sqlalchemy</span> <span class="kn">import</span> <span class="n">create_engine</span>
<span class="o">&gt;&gt;</span> <span class="n">engine</span> <span class="o">=</span> <span class="n">create_engine</span><span class="p">(</span><span class="s1">'postgresql://root:root@pgdatabase:5432/ny_taxi'</span><span class="p">)</span>
<span class="o">&gt;&gt;</span> <span class="n">engine</span><span class="o">.</span><span class="n">connect</span><span class="p">()</span>
<span class="c1"># no error here</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you don't see any error by following the above procedure, it shows that you can connect to the postgres from airflow worker container. So calling the ingestion script from the <code>PythonOperator</code> in the DAG file should work.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="kargarisaac/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/data%20engineering/jupyter/2022/01/25/data-engineering-w2.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My posts about Machine Learning</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/kargarisaac" target="_blank" title="kargarisaac"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/eshagh-kargar" target="_blank" title="eshagh-kargar"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/kargarisaac" target="_blank" title="kargarisaac"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
